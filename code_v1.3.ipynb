{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_excel(\"example_datasets/examples-datasets-mar25.xlsx\")\n",
    "only_chatgpt = pd.read_excel(\"example_datasets/knowledge-datasets-mar25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan informasi dataset\n",
    "print(\"Dataset Student_ChatGPT:\")\n",
    "student_chatgpt.info()\n",
    "print(\"\\nDataset Only_ChatGPT:\")\n",
    "only_chatgpt.info()\n",
    "\n",
    "# Tampilkan beberapa baris awal dataset\n",
    "print(\"\\nContoh Data Student_ChatGPT:\")\n",
    "student_chatgpt.head()\n",
    "\n",
    "print(\"\\nContoh Data Only_ChatGPT:\")\n",
    "only_chatgpt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing teks:\n",
    "    - Mengubah teks menjadi huruf kecil\n",
    "    - Membersihkan whitespace berlebih\n",
    "    - Menjaga teks sebagai paragraf utuh\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks input.\n",
    "        \n",
    "    Returns:\n",
    "        str: Teks yang telah diproses atau None jika tidak valid.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Bersihkan teks\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Ganti multiple whitespace dengan satu spasi\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data Student_ChatGPT\n",
    "std_par = []\n",
    "gpt_par_1 = []  # ChatGPT dari dataset Student_ChatGPT\n",
    "\n",
    "# Proses teks Student\n",
    "for text in student_chatgpt['Pelajar']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        std_par.append(processed)\n",
    "\n",
    "# Proses teks ChatGPT (dari Student_ChatGPT)\n",
    "for text in student_chatgpt['GPT']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        gpt_par_1.append(processed)\n",
    "\n",
    "# Preprocessing data Only_ChatGPT\n",
    "gpt_par_2 = []  # ChatGPT dari dataset Only_ChatGPT\n",
    "\n",
    "# Proses teks ChatGPT (dari Only_ChatGPT)\n",
    "for text in only_chatgpt['GPT']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        gpt_par_2.append(processed)\n",
    "\n",
    "# Tampilkan jumlah data hasil preprocessing\n",
    "print(f\"Total paragraf Student: {len(std_par)}\")\n",
    "print(f\"Total paragraf ChatGPT (Student_ChatGPT): {len(gpt_par_1)}\")\n",
    "print(f\"Total paragraf ChatGPT (Only_ChatGPT): {len(gpt_par_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(texts, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi data\n",
    "print(\"Tokenisasi kalimat Student...\")\n",
    "student_tokens = tokenize_text(std_par)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_tokens = tokenize_text(gpt_par_1 )\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_tokens_2 = tokenize_text(gpt_par_2)\n",
    "\n",
    "# Tampilkan hasil tokenisasi (contoh: Student)\n",
    "print(\"\\nContoh hasil tokenisasi:\")\n",
    "print(student_tokens['input_ids'][:1])  # Input token ID\n",
    "print(student_tokens['attention_mask'][:1])  # Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memeriksa distribusi panjang token untuk memastikan max_length cukup\n",
    "student_lengths = [sum(mask) for mask in student_tokens['attention_mask'].numpy()]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(student_lengths, bins=30)\n",
    "plt.title('Distribusi Panjang Token (Student)')\n",
    "plt.xlabel('Jumlah Token Aktif')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.show()\n",
    "print(f\"Rata-rata panjang token: {np.mean(student_lengths):.2f}\")\n",
    "print(f\"Persentase terpotong: {sum(l == 128 for l in student_lengths) / len(student_lengths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendekode token untuk memastikan tokenisasi berfungsi dengan baik\n",
    "sample_text = std_par[0]\n",
    "sample_tokens = tokenizer.encode(sample_text)\n",
    "print(f\"Teks asli: {sample_text}\")\n",
    "print(f\"Token ID: {sample_tokens}\")\n",
    "print(f\"Token dekode: {tokenizer.decode(sample_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika tokenisasi memakan waktu lama, pertimbangkan untuk menyimpannya\n",
    "tokenized_data = {\n",
    "    'student': student_tokens,\n",
    "    'chatgpt ': chatgpt_tokens ,\n",
    "    'chatgpt_2': chatgpt_tokens_2\n",
    "}\n",
    "\n",
    "# Menyimpan input_ids dan attention_mask sebagai numpy arrays\n",
    "tokenized_numpy = {\n",
    "    'student': {\n",
    "        'input_ids': student_tokens['input_ids'].numpy(),\n",
    "        'attention_mask': student_tokens['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt ': {\n",
    "        'input_ids': chatgpt_tokens ['input_ids'].numpy(),\n",
    "        'attention_mask': chatgpt_tokens ['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt_2': {\n",
    "        'input_ids': chatgpt_tokens_2['input_ids'].numpy(),\n",
    "        'attention_mask': chatgpt_tokens_2['attention_mask'].numpy()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('tokenized_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_numpy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "#Buat Model Bi-Encoder Student_ChatGPT\n",
    "\n",
    "#Buat Model Bi-Encoder Only_ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat Model Bi-Encoder Student_ChatGPT\n",
    "def create_bi_encoder(bert_model):\n",
    "    # Input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # BERT layer\n",
    "    bert_outputs = bert_model([input_ids, attention_mask])\n",
    "    pooled_output = bert_outputs[1]  # Use the pooled output for sentence embedding\n",
    "    \n",
    "    # Dense layers for embedding\n",
    "    embedding = tf.keras.layers.Dense(512, activation=\"tanh\")(pooled_output)\n",
    "    embedding = tf.keras.layers.Dropout(0.1)(embedding)\n",
    "    embedding = tf.keras.layers.Dense(512, activation=None)(embedding)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    embedding = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.nn.l2_normalize(x, axis=1)\n",
    "    )(embedding)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, attention_mask],\n",
    "        outputs=embedding,\n",
    "        name=\"bi_encoder\"\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create Bi-Encoder for Student_ChatGPT\n",
    "bi_encoder_student_chatgpt = create_bi_encoder(bert_model)\n",
    "\n",
    "# Buat Model Bi-Encoder Only_ChatGPT\n",
    "bi_encoder_only_chatgpt = create_bi_encoder(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive loss function\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1.0\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create training pairs\n",
    "def create_training_pairs(texts1, texts2, is_positive=True):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Create pairs within the same dataset\n",
    "    for i in range(len(texts1)):\n",
    "        for j in range(i+1, len(texts1)):\n",
    "            pairs.append((texts1[i], texts1[j]))\n",
    "            labels.append(1)  # Positive pair\n",
    "    \n",
    "    # Create pairs between datasets\n",
    "    if texts2 is not None:\n",
    "        for i in range(len(texts1)):\n",
    "            for j in range(len(texts2)):\n",
    "                pairs.append((texts1[i], texts2[j]))\n",
    "                labels.append(1 if is_positive else 0)  # Positive or negative pair\n",
    "    \n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training pairs for Student_ChatGPT\n",
    "# Positive pairs: Student-Student, ChatGPT-ChatGPT\n",
    "student_student_pairs, student_student_labels = create_training_pairs(std_par, None)\n",
    "chatgpt_chatgpt_pairs, chatgpt_chatgpt_labels = create_training_pairs(gpt_par_1, None)\n",
    "\n",
    "# Negative pairs: Student-ChatGPT\n",
    "student_chatgpt_pairs, student_chatgpt_labels = create_training_pairs(std_par, gpt_par_1, is_positive=False)\n",
    "\n",
    "# Tampilkan jumlah pasangan data\n",
    "print(f\"Jumlah pasangan Student-Student (positif): {len(student_student_pairs)}\")\n",
    "print(f\"Jumlah pasangan ChatGPT-ChatGPT (positif): {len(chatgpt_chatgpt_pairs)}\")\n",
    "print(f\"Jumlah pasangan Student-ChatGPT (negatif): {len(student_chatgpt_pairs)}\")\n",
    "\n",
    "# Combine all pairs for Student_ChatGPT model\n",
    "all_pairs_student_chatgpt = student_student_pairs + chatgpt_chatgpt_pairs + student_chatgpt_pairs\n",
    "all_labels_student_chatgpt = student_student_labels + chatgpt_chatgpt_labels + student_chatgpt_labels\n",
    "\n",
    "\n",
    "print(f\"Total jumlah pasangan data untuk model Student_ChatGPT: {len(all_pairs_student_chatgpt)}\")\n",
    "print(f\"Distribusi label: Positif = {sum(all_labels_student_chatgpt)}, Negatif = {len(all_labels_student_chatgpt) - sum(all_labels_student_chatgpt)}\")\n",
    "\n",
    "# Training Datasets Student_ChatGPT\n",
    "# Tokenize pairs\n",
    "input_ids_1 = []\n",
    "attention_mask_1 = []\n",
    "input_ids_2 = []\n",
    "attention_mask_2 = []\n",
    "\n",
    "for pair in all_pairs_student_chatgpt:\n",
    "    tokens1 = tokenize_text([pair[0]])\n",
    "    tokens2 = tokenize_text([pair[1]])\n",
    "    \n",
    "    input_ids_1.append(tokens1['input_ids'][0])\n",
    "    attention_mask_1.append(tokens1['attention_mask'][0])\n",
    "    input_ids_2.append(tokens2['input_ids'][0])\n",
    "    attention_mask_2.append(tokens2['attention_mask'][0])\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_1 = tf.stack(input_ids_1)\n",
    "attention_mask_1 = tf.stack(attention_mask_1)\n",
    "input_ids_2 = tf.stack(input_ids_2)\n",
    "attention_mask_2 = tf.stack(attention_mask_2)\n",
    "labels = tf.convert_to_tensor(all_labels_student_chatgpt, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training pairs for Student_ChatGPT\n",
    "# Positive pairs: Student-Student, ChatGPT-ChatGPT\n",
    "student_student_pairs, student_student_labels = create_training_pairs(std_par, None)\n",
    "chatgpt_chatgpt_pairs, chatgpt_chatgpt_labels = create_training_pairs(gpt_par_1, None)\n",
    "\n",
    "# Negative pairs: Student-ChatGPT\n",
    "student_chatgpt_pairs, student_chatgpt_labels = create_training_pairs(std_par, gpt_par_1, is_positive=False)\n",
    "\n",
    "# Tampilkan jumlah pasangan data\n",
    "print(f\"Jumlah pasangan Student-Student (positif): {len(student_student_pairs)}\")\n",
    "print(f\"Jumlah pasangan ChatGPT-ChatGPT (positif): {len(chatgpt_chatgpt_pairs)}\")\n",
    "print(f\"Jumlah pasangan Student-ChatGPT (negatif): {len(student_chatgpt_pairs)}\")\n",
    "\n",
    "# Combine all pairs for Student_ChatGPT model\n",
    "all_pairs_student_chatgpt = student_student_pairs + chatgpt_chatgpt_pairs + student_chatgpt_pairs\n",
    "all_labels_student_chatgpt = student_student_labels + chatgpt_chatgpt_labels + student_chatgpt_labels\n",
    "\n",
    "print(f\"Total jumlah pasangan data untuk model Student_ChatGPT: {len(all_pairs_student_chatgpt)}\")\n",
    "print(f\"Distribusi label: Positif = {sum(all_labels_student_chatgpt)}, Negatif = {len(all_labels_student_chatgpt) - sum(all_labels_student_chatgpt)}\")\n",
    "\n",
    "# Training Datasets Student_ChatGPT\n",
    "# Tokenize pairs\n",
    "input_ids_1 = []\n",
    "attention_mask_1 = []\n",
    "input_ids_2 = []\n",
    "attention_mask_2 = []\n",
    "\n",
    "# Variabel untuk menghitung pasangan yang terpotong\n",
    "truncated_pairs_count = 0\n",
    "\n",
    "for pair in all_pairs_student_chatgpt:\n",
    "    tokens1 = tokenize_text([pair[0]])\n",
    "    tokens2 = tokenize_text([pair[1]])\n",
    "    \n",
    "    # Periksa apakah ada token yang terpotong (mencapai max_length)\n",
    "    if sum(tokens1['attention_mask'][0]) == 512 or sum(tokens2['attention_mask'][0]) == 512:\n",
    "        truncated_pairs_count += 1\n",
    "    \n",
    "    input_ids_1.append(tokens1['input_ids'][0])\n",
    "    attention_mask_1.append(tokens1['attention_mask'][0])\n",
    "    input_ids_2.append(tokens2['input_ids'][0])\n",
    "    attention_mask_2.append(tokens2['attention_mask'][0])\n",
    "\n",
    "print(f\"Jumlah pasangan yang terpotong (mencapai max_length 512): {truncated_pairs_count}\")\n",
    "print(f\"Persentase pasangan yang terpotong: {truncated_pairs_count / len(all_pairs_student_chatgpt) * 100:.2f}%\")\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_1 = tf.stack(input_ids_1)\n",
    "attention_mask_1 = tf.stack(attention_mask_1)\n",
    "input_ids_2 = tf.stack(input_ids_2)\n",
    "attention_mask_2 = tf.stack(attention_mask_2)\n",
    "labels = tf.convert_to_tensor(all_labels_student_chatgpt, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Siamese network for Student_ChatGPT\n",
    "input_ids_a = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids_a\")\n",
    "attention_mask_a = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask_a\")\n",
    "input_ids_b = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids_b\")\n",
    "attention_mask_b = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask_b\")\n",
    "\n",
    "# Get embeddings\n",
    "embedding_a = bi_encoder_student_chatgpt([input_ids_a, attention_mask_a])\n",
    "embedding_b = bi_encoder_student_chatgpt([input_ids_b, attention_mask_b])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = tf.reduce_sum(embedding_a * embedding_b, axis=1)\n",
    "\n",
    "# Create model\n",
    "siamese_model_student_chatgpt = tf.keras.Model(\n",
    "    inputs=[input_ids_a, attention_mask_a, input_ids_b, attention_mask_b],\n",
    "    outputs=similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "siamese_model_student_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=contrastive_loss,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history_student_chatgpt = siamese_model_student_chatgpt.fit(\n",
    "    [input_ids_1, attention_mask_1, input_ids_2, attention_mask_2],\n",
    "    labels,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive loss digunakan untuk melatih Bi-Encoder dengan tujuan:\n",
    "# - Pasangan kalimat yang mirip memiliki nilai similarity tinggi (loss rendah).\n",
    "# - Pasangan kalimat yang tidak mirip memiliki nilai similarity rendah (loss tinggi).\n",
    "\n",
    "# Datasets Student_ChatGPT: variable -> std_par & gpt_par_1\n",
    "# Pasangan Positif:\n",
    "# - Student - Student Dalam satu data, antar kalimat\n",
    "# - Student - Student Beda data, Antar kalimat\n",
    "# - ChatGPT - ChatGPT Dalam satu data, antar kalimat\n",
    "# - ChatGPT - ChatGPT Beda data, Antar kalimat\n",
    "\n",
    "# Pasangan Negatif:\n",
    "# - Student - ChatGPT Beda data, Antar kalimat\n",
    "\n",
    "#Training Datasets Student_ChatGPT (Gunakan Model Fine-Tuning IndoBERT di atas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training pairs for Only_ChatGPT\n",
    "# Positive pairs: ChatGPT-ChatGPT\n",
    "chatgpt2_chatgpt2_pairs, chatgpt2_chatgpt2_labels = create_training_pairs(gpt_par_2, None)\n",
    "\n",
    "# Tampilkan jumlah pasangan data\n",
    "print(f\"Jumlah pasangan ChatGPT-ChatGPT (Only_ChatGPT, positif): {len(chatgpt2_chatgpt2_pairs)}\")\n",
    "print(f\"Total jumlah pasangan data untuk model Only_ChatGPT: {len(chatgpt2_chatgpt2_pairs)}\")\n",
    "print(f\"Distribusi label: Positif = {sum(chatgpt2_chatgpt2_labels)}, Negatif = {len(chatgpt2_chatgpt2_labels) - sum(chatgpt2_chatgpt2_labels)}\")\n",
    "\n",
    "# Tokenize pairs\n",
    "input_ids_1_only = []\n",
    "attention_mask_1_only = []\n",
    "input_ids_2_only = []\n",
    "attention_mask_2_only = []\n",
    "\n",
    "# Variabel untuk menghitung pasangan yang terpotong\n",
    "truncated_pairs_count_only = 0\n",
    "\n",
    "for pair in chatgpt2_chatgpt2_pairs:\n",
    "    tokens1 = tokenize_text([pair[0]])\n",
    "    tokens2 = tokenize_text([pair[1]])\n",
    "    \n",
    "    # Periksa apakah ada token yang terpotong (mencapai max_length)\n",
    "    if sum(tokens1['attention_mask'][0]) == 512 or sum(tokens2['attention_mask'][0]) == 512:\n",
    "        truncated_pairs_count_only += 1\n",
    "    \n",
    "    input_ids_1_only.append(tokens1['input_ids'][0])\n",
    "    attention_mask_1_only.append(tokens1['attention_mask'][0])\n",
    "    input_ids_2_only.append(tokens2['input_ids'][0])\n",
    "    attention_mask_2_only.append(tokens2['attention_mask'][0])\n",
    "\n",
    "print(f\"Jumlah pasangan yang terpotong (mencapai max_length 512): {truncated_pairs_count_only}\")\n",
    "print(f\"Persentase pasangan yang terpotong: {truncated_pairs_count_only / len(chatgpt2_chatgpt2_pairs) * 100:.2f}%\")\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_1_only = tf.stack(input_ids_1_only)\n",
    "attention_mask_1_only = tf.stack(attention_mask_1_only)\n",
    "input_ids_2_only = tf.stack(input_ids_2_only)\n",
    "attention_mask_2_only = tf.stack(attention_mask_2_only)\n",
    "labels_only = tf.convert_to_tensor(chatgpt2_chatgpt2_labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Siamese network for Only_ChatGPT\n",
    "input_ids_a_only = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids_a_only\")\n",
    "attention_mask_a_only = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask_a_only\")\n",
    "input_ids_b_only = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids_b_only\")\n",
    "attention_mask_b_only = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask_b_only\")\n",
    "\n",
    "# Get embeddings\n",
    "embedding_a_only = bi_encoder_only_chatgpt([input_ids_a_only, attention_mask_a_only])\n",
    "embedding_b_only = bi_encoder_only_chatgpt([input_ids_b_only, attention_mask_b_only])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_only = tf.reduce_sum(embedding_a_only * embedding_b_only, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "siamese_model_only_chatgpt = tf.keras.Model(\n",
    "    inputs=[input_ids_a_only, attention_mask_a_only, input_ids_b_only, attention_mask_b_only],\n",
    "    outputs=similarity_only\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "siamese_model_only_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=contrastive_loss,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history_only_chatgpt = siamese_model_only_chatgpt.fit(\n",
    "    [input_ids_1_only, attention_mask_1_only, input_ids_2_only, attention_mask_2_only],\n",
    "    labels_only,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets Only_ChatGPT: variable -> gpt_par_2\n",
    "# Pasangan Positif:\n",
    "# - ChatGPT - ChatGPT Dalam satu data, antar kalimat\n",
    "# - ChatGPT - ChatGPT Beda data, Antar kalimat\n",
    "\n",
    "\n",
    "#Training Datasets Only_ChatGPT (Self-Supervised, Gunakan Model Fine-Tuning IndoBERT di atas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Student_ChatGPT model history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_student_chatgpt.history['loss'], label='Train Loss')\n",
    "plt.plot(history_student_chatgpt.history['val_loss'], label='Val Loss')\n",
    "plt.title('Student_ChatGPT Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Only_ChatGPT model history\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_only_chatgpt.history['loss'], label='Train Loss')\n",
    "plt.plot(history_only_chatgpt.history['val_loss'], label='Val Loss')\n",
    "plt.title('Only_ChatGPT Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings untuk kalimat Student, ChatGPT_1 dan ChatGPT_2\n",
    "\n",
    "# Fungsi untuk menghitung similarity score\n",
    "\n",
    "# Hitung similarity scores X-Student (Model Student_ChatGPT), X-ChatGPT1 (Model Student_ChatGPT), X-ChatGPT2 (Model Only_ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for Student, ChatGPT_1 and ChatGPT_2\n",
    "def generate_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        tokens = tokenize_text(batch_texts)\n",
    "        batch_embeddings = model([tokens['input_ids'], tokens['attention_mask']])\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return tf.concat(embeddings, axis=0)\n",
    "\n",
    "# Generate embeddings\n",
    "student_embeddings = generate_embeddings(std_par, bi_encoder_student_chatgpt, tokenizer)\n",
    "chatgpt1_embeddings = generate_embeddings(gpt_par_1, bi_encoder_student_chatgpt, tokenizer)\n",
    "chatgpt2_embeddings = generate_embeddings(gpt_par_2, bi_encoder_only_chatgpt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute similarity scores\n",
    "def compute_similarity(embedding, reference_embeddings):\n",
    "    # Compute cosine similarity between embedding and all reference embeddings\n",
    "    similarities = tf.matmul(embedding, tf.transpose(reference_embeddings))\n",
    "    # Return the maximum similarity score\n",
    "    return tf.reduce_max(similarities, axis=1)\n",
    "\n",
    "# Compute similarity scores for all texts\n",
    "student_similarity_scores = []\n",
    "chatgpt1_similarity_scores = []\n",
    "chatgpt2_similarity_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each text, compute similarity with student, chatgpt1, and chatgpt2 references\n",
    "for i in range(len(std_par)):\n",
    "    embedding = tf.reshape(student_embeddings[i], [1, -1])\n",
    "    \n",
    "    # Similarity with student texts\n",
    "    student_sim = compute_similarity(embedding, student_embeddings)\n",
    "    # Similarity with chatgpt1 texts\n",
    "    chatgpt1_sim = compute_similarity(embedding, chatgpt1_embeddings)\n",
    "    # Similarity with chatgpt2 texts\n",
    "    chatgpt2_sim = compute_similarity(embedding, chatgpt2_embeddings)\n",
    "    \n",
    "    student_similarity_scores.append([student_sim.numpy()[0], chatgpt1_sim.numpy()[0], chatgpt2_sim.numpy()[0]])\n",
    "\n",
    "for i in range(len(gpt_par_1)):\n",
    "    embedding = tf.reshape(chatgpt1_embeddings[i], [1, -1])\n",
    "    \n",
    "    # Similarity with student texts\n",
    "    student_sim = compute_similarity(embedding, student_embeddings)\n",
    "    # Similarity with chatgpt1 texts\n",
    "    chatgpt1_sim = compute_similarity(embedding, chatgpt1_embeddings)\n",
    "    # Similarity with chatgpt2 texts\n",
    "    chatgpt2_sim = compute_similarity(embedding, chatgpt2_embeddings)\n",
    "    \n",
    "    chatgpt1_similarity_scores.append([student_sim.numpy()[0], chatgpt1_sim.numpy()[0], chatgpt2_sim.numpy()[0]])\n",
    "\n",
    "for i in range(len(gpt_par_2)):\n",
    "    embedding = tf.reshape(chatgpt2_embeddings[i], [1, -1])\n",
    "    \n",
    "    # Similarity with student texts\n",
    "    student_sim = compute_similarity(embedding, student_embeddings)\n",
    "    # Similarity with chatgpt1 texts\n",
    "    chatgpt1_sim = compute_similarity(embedding, chatgpt1_embeddings)\n",
    "    # Similarity with chatgpt2 texts\n",
    "    chatgpt2_sim = compute_similarity(embedding, chatgpt2_embeddings)\n",
    "    \n",
    "    chatgpt2_similarity_scores.append([student_sim.numpy()[0], chatgpt1_sim.numpy()[0], chatgpt2_sim.numpy()[0]])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "student_similarity_scores = np.array(student_similarity_scores)\n",
    "chatgpt1_similarity_scores = np.array(chatgpt1_similarity_scores)\n",
    "chatgpt2_similarity_scores = np.array(chatgpt2_similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model Bi-Encoder untuk student_chatgpt\n",
    "\n",
    "# Simpan model Bi-Encoder untuk only_chatgpt\n",
    "\n",
    "# Simpan tokenizer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bi-Encoder model for student_chatgpt\n",
    "bi_encoder_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt')\n",
    "\n",
    "# Save Bi-Encoder model for only_chatgpt\n",
    "bi_encoder_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt')\n",
    "\n",
    "# Save tokenizer configuration\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk ekstraksi fitur stylometric\n",
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur stylometric dari teks:\n",
    "    - Panjang kata rata-rata\n",
    "    - Rasio kata unik\n",
    "    - Rasio tanda baca\n",
    "    \n",
    "     Args:\n",
    "        text (str): Input teks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Fitur stylometric.\n",
    "    \"\"\"\n",
    "    # Basic features\n",
    "    n_chars = len(text)\n",
    "    n_words = len(text.split())\n",
    "    avg_word_length = n_chars / n_words if n_words > 0 else 0\n",
    "    unique_word_ratio = len(set(text.split())) / n_words if n_words > 0 else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    punctuation_ratio = len(re.findall(r'[.,!?;:]', text)) / n_chars if n_chars > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'sentence_length': n_words\n",
    "    }\n",
    "\n",
    "# Ekstraksi fitur untuk semua dataset\n",
    "print(\"Ekstraksi fitur stylometric untuk Student...\")\n",
    "student_features = [extract_stylometric_features(text) for text in std_par]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_features_1 = [extract_stylometric_features(text) for text in gpt_par_1]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_features_2 = [extract_stylometric_features(text) for text in gpt_par_2]\n",
    "\n",
    "# Konversi fitur ke DataFrame\n",
    "student_features_df = pd.DataFrame(student_features)\n",
    "chatgpt_features_1_df = pd.DataFrame(chatgpt_features_1)\n",
    "chatgpt_features_2_df = pd.DataFrame(chatgpt_features_2)\n",
    "\n",
    "# Tampilkan beberapa fitur hasil ekstraksi\n",
    "print(\"\\nFitur Stylometric Student:\")\n",
    "display(student_features_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Student_ChatGPT):\")\n",
    "display(chatgpt_features_1_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Only_ChatGPT):\")\n",
    "display(chatgpt_features_2_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua fitur untuk normalisasi\n",
    "all_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)\n",
    "\n",
    "# Normalisasi fitur menggunakan StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(all_features)\n",
    "\n",
    "# Pisahkan kembali fitur yang telah dinormalisasi\n",
    "n_student = len(student_features_df)\n",
    "n_chatgpt_1 = len(chatgpt_features_1_df)\n",
    "\n",
    "student_features_normalized = normalized_features[:n_student]\n",
    "chatgpt_features_1_normalized = normalized_features[n_student:n_student + n_chatgpt_1]\n",
    "chatgpt_features_2_normalized = normalized_features[n_student + n_chatgpt_1:]\n",
    "\n",
    "print(\"Fitur Student setelah normalisasi:\")\n",
    "print(student_features_normalized[:5])\n",
    "\n",
    "# Simpan scaler untuk inference nanti\n",
    "with open('scaler_stylometric.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahkan label pada dataset untuk visualisasi\n",
    "student_features_df['label'] = 'Esai Siswa'\n",
    "chatgpt_features_1_df['label'] = 'Esai ChatGPT'\n",
    "chatgpt_features_2_df['label'] = 'Pengetahuan ChatGPT'\n",
    "\n",
    "# Gabungkan dataset\n",
    "combined_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(['avg_word_length', 'unique_word_ratio', 'punctuation_ratio', 'sentence_length']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(x='label', y=feature, data=combined_features)\n",
    "    plt.title(f'Distribusi {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.violinplot(x='label', y='avg_word_length', data=combined_features, inner='box')\n",
    "plt.title('Distribusi Panjang Kata Rata-Rata')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(combined_features, hue='label', vars=['avg_word_length', 'unique_word_ratio', 'punctuation_ratio', 'sentence_length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layers untuk tiga jenis fitur\n",
    "bert_embedding_input = tf.keras.layers.Input(\n",
    "    shape=(512,), #Shape Embeddings\n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding\"\n",
    ")\n",
    "stylometric_input = tf.keras.layers.Input(\n",
    "    shape=(4,), #Shape Jumlah fitur Stylometric\n",
    "    dtype=tf.float32, \n",
    "    name=\"stylometric_features\"\n",
    ")\n",
    "similarity_score_input = tf.keras.layers.Input(\n",
    "    shape=(3,), #Shape Score similarity from Student, ChatGPT1, ChatGPT2\n",
    "    dtype=tf.float32, \n",
    "    name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "# Dense layer untuk masing-masing input\n",
    "bert_dense = tf.keras.layers.Dense(64, activation=\"relu\")(bert_embedding_input)\n",
    "style_dense = tf.keras.layers.Dense(16, activation=\"relu\")(stylometric_input)\n",
    "sim_dense = tf.keras.layers.Dense(8, activation=\"relu\")(similarity_score_input)\n",
    "\n",
    "# Gabungkan semua fitur\n",
    "combined = tf.keras.layers.Concatenate()([bert_dense, style_dense, sim_dense])\n",
    "\n",
    "# Dense layers untuk klasifikasi\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(combined)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Buat model classifier\n",
    "classifier = tf.keras.Model(\n",
    "    inputs=[bert_embedding_input, stylometric_input, similarity_score_input],\n",
    "    outputs=output,\n",
    "    name=\"single_sentence_classifier\"\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label untuk kalimat\n",
    "student_labels = np.zeros(len(student_embeddings))  # Label 0 for Student\n",
    "chatgpt_labels = np.ones(len(chatgpt1_embeddings))  # Label 1 for ChatGPT\n",
    "\n",
    "# Combine embedding data\n",
    "all_embeddings = np.vstack([student_embeddings.numpy(), chatgpt1_embeddings.numpy()])\n",
    "\n",
    "# Combine stylometric features\n",
    "all_stylometric_features = np.vstack([student_features_normalized, chatgpt_features_1_normalized])\n",
    "\n",
    "# Pastikan semua data memiliki ukuran yang sama\n",
    "n_student = len(student_embeddings)\n",
    "n_chatgpt = len(chatgpt1_embeddings)\n",
    "\n",
    "# Create similarity scores array\n",
    "all_similarity_scores = np.vstack([student_similarity_scores, chatgpt1_similarity_scores])\n",
    "\n",
    "all_labels = np.hstack([student_labels, chatgpt_labels])\n",
    "\n",
    "# Split data into training and validation\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(all_labels)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "\n",
    "# Prepare inputs for training and validation\n",
    "train_inputs = {\n",
    "    \"bert_embedding\": all_embeddings[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "val_inputs = {\n",
    "    \"bert_embedding\": all_embeddings[val_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[val_idx],\n",
    "    \"similarity_score\": all_similarity_scores[val_idx]\n",
    "}\n",
    "\n",
    "train_labels_split = all_labels[train_idx]\n",
    "val_labels_split = all_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifier\n",
    "print(\"Training Single Sentence Classifier...\")\n",
    "history = classifier.fit(\n",
    "    train_inputs,\n",
    "    train_labels_split,\n",
    "    validation_data=(val_inputs, val_labels_split),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Prediksi pada validation set\n",
    "val_predictions = classifier.predict(val_inputs)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(val_labels_split, val_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels_split, val_predictions_binary, target_names=['Student', 'ChatGPT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat direktori jika belum ada\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model classifier\n",
    "classifier.save('saved_models/single_sentence_classifier.h5')\n",
    "\n",
    "# Simpan konfigurasi tokenizer\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "# Simpan scaler untuk fitur stylometric\n",
    "with open(\"scaler_stylometric.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model dan konfigurasi berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load all saved models and configurations\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('saved_models/tokenizer')\n",
    "    \n",
    "    # Load bi-encoder models\n",
    "    bi_encoder_student_chatgpt = tf.keras.models.load_model('saved_models/bi_encoder_student_chatgpt')\n",
    "    bi_encoder_only_chatgpt = tf.keras.models.load_model('saved_models/bi_encoder_only_chatgpt')\n",
    "    \n",
    "    # Load classifier\n",
    "    classifier = tf.keras.models.load_model('saved_models/single_sentence_classifier.h5')\n",
    "    \n",
    "    # Load stylometric scaler\n",
    "    with open('scaler_stylometric.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    return tokenizer, bi_encoder_student_chatgpt, bi_encoder_only_chatgpt, classifier, scaler\n",
    "\n",
    "def predict_text_source(text, tokenizer, bi_encoder_student_chatgpt, bi_encoder_only_chatgpt, classifier, scaler, \n",
    "                        reference_student_embeddings, reference_chatgpt1_embeddings, reference_chatgpt2_embeddings):\n",
    "    \"\"\"\n",
    "    Predict if a text is written by a student or ChatGPT\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        tokenizer: BERT tokenizer\n",
    "        bi_encoder_student_chatgpt: Bi-encoder model for student-chatgpt\n",
    "        bi_encoder_only_chatgpt: Bi-encoder model for only-chatgpt\n",
    "        classifier: Final classifier model\n",
    "        scaler: Stylometric features scaler\n",
    "        reference_embeddings: Reference embeddings for similarity calculation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Prediction results including probability and classification\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_text(text)\n",
    "    if not processed_text:\n",
    "        return {\"error\": \"Invalid input text\"}\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = tokenize_text([processed_text])\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embedding_student_chatgpt = bi_encoder_student_chatgpt([tokens['input_ids'], tokens['attention_mask']])\n",
    "    embedding_only_chatgpt = bi_encoder_only_chatgpt([tokens['input_ids'], tokens['attention_mask']])\n",
    "    \n",
    "    # Extract stylometric features\n",
    "    style_features = extract_stylometric_features(processed_text)\n",
    "    style_features_df = pd.DataFrame([style_features])\n",
    "    style_features_normalized = scaler.transform(style_features_df)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    embedding_reshaped = tf.reshape(embedding_student_chatgpt, [1, -1])\n",
    "    \n",
    "    # Similarity with student texts\n",
    "    student_sim = compute_similarity(embedding_reshaped, reference_student_embeddings)\n",
    "    # Similarity with chatgpt1 texts\n",
    "    chatgpt1_sim = compute_similarity(embedding_reshaped, reference_chatgpt1_embeddings)\n",
    "    # Similarity with chatgpt2 texts\n",
    "    chatgpt2_sim = compute_similarity(embedding_reshaped, reference_chatgpt2_embeddings)\n",
    "    \n",
    "    similarity_scores = np.array([[student_sim.numpy()[0], chatgpt1_sim.numpy()[0], chatgpt2_sim.numpy()[0]]])\n",
    "    \n",
    "    # Prepare inputs for classifier\n",
    "    inputs = {\n",
    "        \"bert_embedding\": embedding_student_chatgpt.numpy(),\n",
    "        \"stylometric_features\": style_features_normalized,\n",
    "        \"similarity_score\": similarity_scores\n",
    "    }\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = classifier.predict(inputs)\n",
    "    probability = float(prediction[0][0])\n",
    "    \n",
    "    # Determine classification\n",
    "    if probability > 0.5:\n",
    "        classification = \"ChatGPT\"\n",
    "    else:\n",
    "        classification = \"Student\"\n",
    "    \n",
    "    return {\n",
    "        \"probability\": probability,\n",
    "        \"classification\": classification,\n",
    "        \"stylometric_features\": style_features,\n",
    "        \"similarity_scores\": {\n",
    "            \"student_similarity\": float(student_sim.numpy()[0]),\n",
    "            \"chatgpt1_similarity\": float(chatgpt1_sim.numpy()[0]),\n",
    "            \"chatgpt2_similarity\": float(chatgpt2_sim.numpy()[0])\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "def demo_prediction():\n",
    "    # Load models\n",
    "    tokenizer, bi_encoder_student_chatgpt, bi_encoder_only_chatgpt, classifier, scaler = load_models()\n",
    "    \n",
    "    # Load reference embeddings (these would be saved during training)\n",
    "    # For demonstration, we'll generate them again\n",
    "    reference_student_embeddings = generate_embeddings(std_par[:10], bi_encoder_student_chatgpt, tokenizer)\n",
    "    reference_chatgpt1_embeddings = generate_embeddings(gpt_par_1[:10], bi_encoder_student_chatgpt, tokenizer)\n",
    "    reference_chatgpt2_embeddings = generate_embeddings(gpt_par_2[:10], bi_encoder_only_chatgpt, tokenizer)\n",
    "    \n",
    "    # Example text\n",
    "    example_student_text = std_par[0]\n",
    "    example_chatgpt_text = gpt_par_1[0]\n",
    "    \n",
    "    # Make predictions\n",
    "    student_result = predict_text_source(\n",
    "        example_student_text, \n",
    "        tokenizer, \n",
    "        bi_encoder_student_chatgpt, \n",
    "        bi_encoder_only_chatgpt, \n",
    "        classifier, \n",
    "        scaler,\n",
    "        reference_student_embeddings,\n",
    "        reference_chatgpt1_embeddings,\n",
    "        reference_chatgpt2_embeddings\n",
    "    )\n",
    "    \n",
    "    chatgpt_result = predict_text_source(\n",
    "        example_chatgpt_text, \n",
    "        tokenizer, \n",
    "        bi_encoder_student_chatgpt, \n",
    "        bi_encoder_only_chatgpt, \n",
    "        classifier, \n",
    "        scaler,\n",
    "        reference_student_embeddings,\n",
    "        reference_chatgpt1_embeddings,\n",
    "        reference_chatgpt2_embeddings\n",
    "    )\n",
    "    \n",
    "    print(\"Student Text Prediction:\")\n",
    "    print(f\"Classification: {student_result['classification']}\")\n",
    "    print(f\"Probability: {student_result['probability']:.4f}\")\n",
    "    print(\"\\nChatGPT Text Prediction:\")\n",
    "    print(f\"Classification: {chatgpt_result['classification']}\")\n",
    "    print(f\"Probability: {chatgpt_result['probability']:.4f}\")\n",
    "\n",
    "# Uncomment to run demo\n",
    "# demo_prediction()\n",
    "\n",
    "# Create a simple web interface for the model\n",
    "def create_web_interface():\n",
    "    import gradio as gr\n",
    "    \n",
    "    # Load models\n",
    "    tokenizer, bi_encoder_student_chatgpt, bi_encoder_only_chatgpt, classifier, scaler = load_models()\n",
    "    \n",
    "    # Load reference embeddings\n",
    "    reference_student_embeddings = generate_embeddings(std_par[:10], bi_encoder_student_chatgpt, tokenizer)\n",
    "    reference_chatgpt1_embeddings = generate_embeddings(gpt_par_1[:10], bi_encoder_student_chatgpt, tokenizer)\n",
    "    reference_chatgpt2_embeddings = generate_embeddings(gpt_par_2[:10], bi_encoder_only_chatgpt, tokenizer)\n",
    "    \n",
    "    def predict(text):\n",
    "        result = predict_text_source(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            bi_encoder_student_chatgpt, \n",
    "            bi_encoder_only_chatgpt, \n",
    "            classifier, \n",
    "            scaler,\n",
    "            reference_student_embeddings,\n",
    "            reference_chatgpt1_embeddings,\n",
    "            reference_chatgpt2_embeddings\n",
    "        )\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            return result[\"error\"]\n",
    "        \n",
    "        return f\"Classification: {result['classification']}\\nProbability: {result['probability']:.4f}\"\n",
    "    \n",
    "    interface = gr.Interface(\n",
    "        fn=predict,\n",
    "        inputs=gr.Textbox(lines=10, placeholder=\"Enter text to analyze...\"),\n",
    "        outputs=\"text\",\n",
    "        title=\"Student vs ChatGPT Text Classifier\",\n",
    "        description=\"This model analyzes text to determine if it was written by a student or generated by ChatGPT.\"\n",
    "    )\n",
    "    \n",
    "    interface.launch()\n",
    "\n",
    "# Uncomment to launch web interface\n",
    "# create_web_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
