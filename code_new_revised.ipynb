{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumentasi Model Deteksi & Klasifikasi Esai Siswa dan ChatGPT\n",
    "\n",
    "Notebook ini bertujuan untuk membangun sistem klasifikasi esai siswa dan ChatPGT menggunakan dua dataset: Student_ChatGPT dan Only_ChatGPT. Sistem ini mencakup arsitektur Bi-Encoder dan classifier untuk membedakan teks dari Student dan ChatGPT. Dokumentasi ini memandu Anda melalui proses pembentukan sistem secara bertahap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library\n",
    "\n",
    "Pada langkah ini, semua library yang diperlukan untuk pemrosesan teks, pembelajaran mesin, dan visualisasi data diimpor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Set seed untuk reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Dua dataset utama diimpor:\n",
    "1. Dataset Student_ChatGPT: Berisi interaksi antara Student dan ChatGPT dengan format:\n",
    "- Kolom 1: Teks dari Student.\n",
    "- Kolom 2: Respon dari ChatGPT.\n",
    "2. Dataset Only_ChatGPT: Berisi teks yang dihasilkan oleh ChatGPT terkait pengetahuan satu mata pelajaran selama satu tahun (2 semester).\n",
    "\n",
    "Dataset ini digunakan untuk membangun model Bi-Encoder dan classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_csv(\"student_chatgpt.csv\")\n",
    "only_chatgpt = pd.read_csv(\"only_chatgpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan informasi dataset\n",
    "print(\"Dataset Student_ChatGPT:\")\n",
    "print(student_chatgpt.info())\n",
    "print(\"\\nDataset Only_ChatGPT:\")\n",
    "print(only_chatgpt.info())\n",
    "\n",
    "# Tampilkan beberapa baris awal dataset\n",
    "print(\"\\nContoh Data Student_ChatGPT:\")\n",
    "display(student_chatgpt.head())\n",
    "\n",
    "print(\"\\nContoh Data Only_ChatGPT:\")\n",
    "display(only_chatgpt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data\n",
    "\n",
    "Langkah ini memproses data mentah menjadi format yang siap digunakan oleh model:\n",
    "1. Preprocessing Teks:\n",
    "- Semua teks diubah menjadi huruf kecil.\n",
    "- Teks dipisahkan menjadi kalimat menggunakan tokenisasi kalimat.\n",
    "2. Distribusi Data:\n",
    "- Total jumlah kalimat dari masing-masing sumber (Student, ChatGPT dari Student_ChatGPT, dan Only_ChatGPT) dihitung dan ditampilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk preprocessing teks\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing teks:\n",
    "    - Mengubah teks menjadi huruf kecil.\n",
    "    - Membagi teks menjadi kalimat (tokenisasi kalimat).\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks input.\n",
    "        \n",
    "    Returns:\n",
    "        list: Daftar kalimat yang telah diproses.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return []\n",
    "    text = text.lower().strip()\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Preprocessing data Student_ChatGPT\n",
    "student_sentences = []\n",
    "chatgpt_sentences_1 = []  # ChatGPT dari dataset Student_ChatGPT\n",
    "\n",
    "# Proses teks Student\n",
    "for text in student_chatgpt['student']:\n",
    "    student_sentences.extend(preprocess_text(text))\n",
    "\n",
    "# Proses teks ChatGPT (dari Student_ChatGPT)\n",
    "for text in student_chatgpt['chatgpt']:\n",
    "    chatgpt_sentences_1.extend(preprocess_text(text))\n",
    "\n",
    "# Preprocessing data Only_ChatGPT\n",
    "chatgpt_sentences_2 = []  # ChatGPT dari dataset Only_ChatGPT\n",
    "\n",
    "# Proses teks ChatGPT (dari Only_ChatGPT)\n",
    "for text in only_chatgpt['chatgpt']:\n",
    "    chatgpt_sentences_2.extend(preprocess_text(text))\n",
    "\n",
    "# Tampilkan jumlah data hasil preprocessing\n",
    "print(f\"Total kalimat Student: {len(student_sentences)}\")\n",
    "print(f\"Total kalimat ChatGPT (Student_ChatGPT): {len(chatgpt_sentences_1)}\")\n",
    "print(f\"Total kalimat ChatGPT (Only_ChatGPT): {len(chatgpt_sentences_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ekstraksi Fitur Stylometric\n",
    "\n",
    "Fitur stylometric diekstraksi untuk memberikan representasi tambahan tentang pola teks. Fitur ini meliputi:\n",
    "- Panjang Kata Rata-Rata: Rata-rata panjang kata dalam kalimat.\n",
    "- Rasio Kata Unik: Proporsi kata unik terhadap total kata.\n",
    "- Rasio Tanda Baca: Proporsi tanda baca terhadap total karakter.\n",
    "- Rasio Huruf Besar: Proporsi huruf besar terhadap total karakter.\n",
    "- Rasio Kata Benda dan Kata Kerja: Proporsi kata benda dan kata kerja berdasarkan POS tagging.\n",
    "\n",
    "Fitur ini digunakan sebagai input tambahan untuk model classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk ekstraksi fitur stylometric\n",
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur stylometric dari teks:\n",
    "    - Panjang kata rata-rata\n",
    "    - Rasio kata unik\n",
    "    - Rasio tanda baca\n",
    "    - Rasio huruf besar\n",
    "    - Rasio kata benda dan kata kerja\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input teks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Fitur stylometric.\n",
    "    \"\"\"\n",
    "    # Basic features\n",
    "    n_chars = len(text)\n",
    "    n_words = len(text.split())\n",
    "    avg_word_length = n_chars / n_words if n_words > 0 else 0\n",
    "    unique_word_ratio = len(set(text.split())) / n_words if n_words > 0 else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    punctuation_ratio = len(re.findall(r'[.,!?;:]', text)) / n_chars if n_chars > 0 else 0\n",
    "    uppercase_ratio = sum(1 for c in text if c.isupper()) / n_chars if n_chars > 0 else 0\n",
    "    \n",
    "    # POS tagging features\n",
    "    pos_tags = pos_tag(word_tokenize(text))\n",
    "    pos_counts = Counter(tag for _, tag in pos_tags)\n",
    "    noun_ratio = pos_counts.get('NN', 0) / len(pos_tags) if len(pos_tags) > 0 else 0\n",
    "    verb_ratio = sum(pos_counts.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']) / len(pos_tags) if len(pos_tags) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'uppercase_ratio': uppercase_ratio,\n",
    "        'noun_ratio': noun_ratio,\n",
    "        'verb_ratio': verb_ratio,\n",
    "        'sentence_length': n_words\n",
    "    }\n",
    "\n",
    "# Ekstraksi fitur untuk semua dataset\n",
    "print(\"Ekstraksi fitur stylometric untuk Student...\")\n",
    "student_features = [extract_stylometric_features(text) for text in student_sentences]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_features_1 = [extract_stylometric_features(text) for text in chatgpt_sentences_1]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_features_2 = [extract_stylometric_features(text) for text in chatgpt_sentences_2]\n",
    "\n",
    "# Konversi fitur ke DataFrame\n",
    "student_features_df = pd.DataFrame(student_features)\n",
    "chatgpt_features_1_df = pd.DataFrame(chatgpt_features_1)\n",
    "chatgpt_features_2_df = pd.DataFrame(chatgpt_features_2)\n",
    "\n",
    "# Tampilkan beberapa fitur hasil ekstraksi\n",
    "print(\"\\nFitur Stylometric Student:\")\n",
    "display(student_features_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Student_ChatGPT):\")\n",
    "display(chatgpt_features_1_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Only_ChatGPT):\")\n",
    "display(chatgpt_features_2_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalisasi Fitur Stylometric\n",
    "\n",
    "Semua fitur stylometric dinormalisasi menggunakan StandardScaler untuk memastikan skala yang konsisten. Langkah ini penting untuk meningkatkan performa model saat memproses input dari berbagai skala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua fitur untuk normalisasi\n",
    "all_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)\n",
    "\n",
    "# Normalisasi fitur menggunakan StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(all_features)\n",
    "\n",
    "# Pisahkan kembali fitur yang telah dinormalisasi\n",
    "n_student = len(student_features_df)\n",
    "n_chatgpt_1 = len(chatgpt_features_1_df)\n",
    "\n",
    "student_features_normalized = normalized_features[:n_student]\n",
    "chatgpt_features_1_normalized = normalized_features[n_student:n_student + n_chatgpt_1]\n",
    "chatgpt_features_2_normalized = normalized_features[n_student + n_chatgpt_1:]\n",
    "\n",
    "print(\"Fitur Student setelah normalisasi:\")\n",
    "print(student_features_normalized[:5])\n",
    "\n",
    "# Simpan scaler untuk inference nanti\n",
    "with open('scaler_stylometric.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisasi Fitur Stylometric\n",
    "\n",
    "Distribusi fitur stylometric divisualisasikan untuk mengidentifikasi pola perbedaan antara teks dari Student dan ChatGPT. Visualisasi ini membantu memahami karakteristik unik masing-masing sumber teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahkan label pada dataset untuk visualisasi\n",
    "student_features_df['label'] = 'Student'\n",
    "chatgpt_features_1_df['label'] = 'ChatGPT_Student'\n",
    "chatgpt_features_2_df['label'] = 'ChatGPT_Only'\n",
    "\n",
    "# Gabungkan dataset\n",
    "combined_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)\n",
    "\n",
    "# Plot distribusi salah satu fitur (contoh: avg_word_length)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='label', y='avg_word_length', data=combined_features)\n",
    "plt.title('Distribusi Panjang Kata Rata-Rata')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Panjang Kata Rata-Rata')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inisialisasi BERT Tokenizer\n",
    "\n",
    "Teks dari dataset di-tokenisasi menggunakan tokenizer pretrained dari IndoBERT:\n",
    "- `input_ids`: Token ID untuk setiap kata dalam teks.\n",
    "- `attention_mask`: Masking untuk menandai kata yang relevan dan padding.\n",
    "\n",
    "Tokenisasi diperlukan agar teks dapat diproses oleh model BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(texts, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi data\n",
    "print(\"Tokenisasi kalimat Student...\")\n",
    "student_tokens = tokenize_text(student_sentences)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_tokens_1 = tokenize_text(chatgpt_sentences_1)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_tokens_2 = tokenize_text(chatgpt_sentences_2)\n",
    "\n",
    "# Tampilkan hasil tokenisasi (contoh: Student)\n",
    "print(\"\\nContoh hasil tokenisasi:\")\n",
    "print(student_tokens['input_ids'][:1])  # Input token ID\n",
    "print(student_tokens['attention_mask'][:1])  # Attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Membuat Model BERT Bi-Encoder\n",
    "\n",
    "Dua model Bi-Encoder dibuat:\n",
    "- Bi-Encoder untuk Only_ChatGPT: Digunakan untuk memahami teks dari dataset Only_ChatGPT.\n",
    "- Bi-Encoder untuk Student_ChatGPT: Digunakan untuk memahami interaksi antara Student dan ChatGPT.\n",
    "\n",
    "Arsitektur Bi-Encoder melibatkan:\n",
    "- IndoBERT sebagai backbone untuk menghasilkan embeddings.\n",
    "- Dense Layers untuk fine-tuning.\n",
    "- L2 Normalization untuk menghasilkan embeddings yang seragam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Fungsi untuk membuat Bi-Encoder\n",
    "def build_bi_encoder(bert_model):\n",
    "    \"\"\"\n",
    "    Membuat model Bi-Encoder dengan IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        bert_model (TFBertModel): Model dasar IndoBERT.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Model Bi-Encoder.\n",
    "    \"\"\"\n",
    "    # Input layer untuk token ID dan attention mask\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Extract CLS token embeddings dari IndoBERT\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0][:, 0, :]  # [CLS] token\n",
    "    \n",
    "    # Dense layer untuk fine-tuning\n",
    "    dense1_chatgpt = tf.keras.layers.Dense(256, activation=\"relu\")(bert_output)\n",
    "    dropout1_chatgpt = tf.keras.layers.Dropout(0.1)(dense1_chatgpt)\n",
    "    dense2_chatgpt = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1_chatgpt)\n",
    "    dropout2_chatgpt = tf.keras.layers.Dropout(0.1)(dense2_chatgpt)\n",
    "    dense3_chatgpt = tf.keras.layers.Dense(128)(dropout2_chatgpt)\n",
    "    \n",
    "    # Normalisasi output (L2 normalization)\n",
    "    normalized_output = tf.nn.l2_normalize(dense3_chatgpt, axis=1)\n",
    "    \n",
    "    # Model Bi-Encoder\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=normalized_output)\n",
    "\n",
    "# Buat dua Bi-Encoder\n",
    "bi_encoder_only_chatgpt = build_bi_encoder(bert_model)\n",
    "bi_encoder_student_chatgpt = build_bi_encoder(bert_model)\n",
    "\n",
    "# Tampilkan arsitektur\n",
    "print(\"Bi-Encoder untuk Only_ChatGPT:\")\n",
    "bi_encoder_only_chatgpt.summary()\n",
    "\n",
    "print(\"\\nBi-Encoder untuk Student_ChatGPT:\")\n",
    "bi_encoder_student_chatgpt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Constrastive Loss\n",
    "\n",
    "Contrastive loss digunakan untuk melatih Bi-Encoder dengan tujuan:\n",
    "- Pasangan kalimat yang mirip memiliki nilai similarity tinggi (loss rendah).\n",
    "- Pasangan kalimat yang tidak mirip memiliki nilai similarity rendah (loss tinggi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk contrastive loss\n",
    "def contrastive_loss(margin=1.0):\n",
    "    \"\"\"\n",
    "    Implementasi contrastive loss.\n",
    "    \n",
    "    Args:\n",
    "        margin (float): Margin untuk pasangan yang tidak mirip.\n",
    "        \n",
    "    Returns:\n",
    "        function: Fungsi loss.\n",
    "    \"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: 1 untuk pasangan mirip, 0 untuk tidak mirip.\n",
    "            y_pred: Cosine similarity antara dua embeddings.\n",
    "        \"\"\"\n",
    "        distance = 1 - y_pred  # Konversi similarity ke distance\n",
    "        positive_loss = y_true * tf.square(distance)\n",
    "        negative_loss = (1 - y_true) * tf.square(tf.maximum(margin - distance, 0))\n",
    "        return tf.reduce_mean(positive_loss + negative_loss)\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Bi-Encoder\n",
    "\n",
    "Kedua Bi-Encoder dilatih secara terpisah:\n",
    "1. Only_ChatGPT:\n",
    "- Pasangan positif: Kalimat yang saling berdekatan dalam satu bab.\n",
    "2. Student_ChatGPT:\n",
    "- Pasangan positif: Student dan ChatGPT untuk soal yang sama.\n",
    "- Pasangan negatif: Student dengan Student atau ChatGPT dengan ChatGPT.\n",
    "\n",
    "Hasil pelatihan divisualisasikan untuk memantau performa loss selama training dan validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat pasangan positif\n",
    "def create_positive_pairs(sentences):\n",
    "    \"\"\"\n",
    "    Membuat pasangan positif dari daftar kalimat, diambil secara berurutan.\n",
    "    \"\"\"\n",
    "    input_1 = sentences[:-1]\n",
    "    input_2 = sentences[1:]\n",
    "    labels = tf.ones(len(input_1))  # Label 1 untuk pasangan positif\n",
    "    return input_1, input_2, labels\n",
    "\n",
    "# Fungsi untuk membuat pasangan negatif\n",
    "def create_negative_pairs(sentences):\n",
    "    \"\"\"\n",
    "    Membuat pasangan negatif dari daftar kalimat, dengan memilih pasangan acak.\n",
    "    \"\"\"\n",
    "    input_1 = sentences[:-1]\n",
    "    input_2 = np.random.permutation(sentences[1:])  # Acak untuk pasangan negatif\n",
    "    labels = tf.zeros(len(input_1))  # Label 0 untuk pasangan negatif\n",
    "    return input_1, input_2, labels\n",
    "\n",
    "# Persiapan data untuk Only_ChatGPT\n",
    "print(\"Mempersiapkan data untuk Only_ChatGPT...\")\n",
    "only_chatgpt_positive_1, only_chatgpt_positive_2, only_chatgpt_labels = create_positive_pairs(chatgpt_sentences_2)\n",
    "\n",
    "# Persiapan data untuk Student_ChatGPT\n",
    "print(\"Mempersiapkan data untuk Student_ChatGPT...\")\n",
    "student_positive_1, student_positive_2, student_positive_labels = create_positive_pairs(student_sentences)\n",
    "chatgpt_positive_1, chatgpt_positive_2, chatgpt_positive_labels = create_positive_pairs(chatgpt_sentences_1)\n",
    "\n",
    "# Gabungkan pasangan positif dan negatif\n",
    "student_input_1 = np.concatenate([student_positive_1, student_positive_1])\n",
    "student_input_2 = np.concatenate([student_positive_2, np.random.permutation(student_positive_2)])\n",
    "student_labels = np.concatenate([student_positive_labels, np.zeros(len(student_positive_labels))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Training Student - ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Bi-Encoder untuk Student_ChatGPT\n",
    "bi_encoder_student_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=contrastive_loss(margin=1.0)\n",
    ")\n",
    "\n",
    "# Training untuk Student_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Student_ChatGPT...\")\n",
    "history_student_chatgpt = bi_encoder_student_chatgpt.fit(\n",
    "    x={\n",
    "        \"input_ids\": tokenize_text(student_input_1)['input_ids'],\n",
    "        \"attention_mask\": tokenize_text(student_input_1)['attention_mask']\n",
    "    },\n",
    "    y=student_labels,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Training Only ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Bi-Encoder untuk Only_ChatGPT\n",
    "bi_encoder_only_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=contrastive_loss(margin=1.0)\n",
    ")\n",
    "\n",
    "# Training untuk Only_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Only_ChatGPT...\")\n",
    "history_only_chatgpt = bi_encoder_only_chatgpt.fit(\n",
    "    x={\n",
    "        \"input_ids\": tokenize_text(only_chatgpt_positive_1)['input_ids'],\n",
    "        \"attention_mask\": tokenize_text(only_chatgpt_positive_1)['attention_mask']\n",
    "    },\n",
    "    y=only_chatgpt_labels,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history untuk Student_ChatGPT\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_student_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_student_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training History - Student_ChatGPT')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training history untuk Only_ChatGPT\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_only_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_only_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training History - Only_ChatGPT')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluasi Bi-Encoder\n",
    "\n",
    "Bi-Encoder dievaluasi dengan menghitung cosine similarity antara embeddings yang dihasilkan:\n",
    "- Student vs ChatGPT (Student_ChatGPT).\n",
    "- ChatGPT (Student_ChatGPT) vs ChatGPT (Only_ChatGPT).\n",
    "\n",
    "Similarity score digunakan untuk memahami seberapa dekat embeddings dari berbagai sumber teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung similarity score\n",
    "def calculate_similarity(embeddings_1, embeddings_2):\n",
    "    \"\"\"\n",
    "    Menghitung cosine similarity antara dua set embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_1 (tf.Tensor): Embeddings pertama.\n",
    "        embeddings_2 (tf.Tensor): Embeddings kedua.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Matrix similarity.\n",
    "    \"\"\"\n",
    "    # Normalisasi embeddings\n",
    "    norm_embeddings_1 = tf.nn.l2_normalize(embeddings_1, axis=1)\n",
    "    norm_embeddings_2 = tf.nn.l2_normalize(embeddings_2, axis=1)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    similarity_matrix = tf.matmul(norm_embeddings_1, norm_embeddings_2, transpose_b=True)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Generate embeddings untuk kalimat Student dan ChatGPT\n",
    "print(\"Menghasilkan embeddings untuk Student...\")\n",
    "student_embeddings = bi_encoder_student_chatgpt.predict({\n",
    "    \"input_ids\": student_tokens['input_ids'],\n",
    "    \"attention_mask\": student_tokens['attention_mask']\n",
    "})\n",
    "\n",
    "print(\"Menghasilkan embeddings untuk ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_1_embeddings = bi_encoder_student_chatgpt.predict({\n",
    "    \"input_ids\": chatgpt_tokens_1['input_ids'],\n",
    "    \"attention_mask\": chatgpt_tokens_1['attention_mask']\n",
    "})\n",
    "\n",
    "print(\"Menghasilkan embeddings untuk ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_2_embeddings = bi_encoder_only_chatgpt.predict({\n",
    "    \"input_ids\": chatgpt_tokens_2['input_ids'],\n",
    "    \"attention_mask\": chatgpt_tokens_2['attention_mask']\n",
    "})\n",
    "\n",
    "# Hitung similarity scores\n",
    "print(\"Menghitung similarity scores...\")\n",
    "similarity_student_chatgpt1 = calculate_similarity(student_embeddings, chatgpt_1_embeddings)\n",
    "similarity_chatgpt1_chatgpt2 = calculate_similarity(chatgpt_1_embeddings, chatgpt_2_embeddings)\n",
    "\n",
    "# Tampilkan beberapa contoh similarity scores\n",
    "print(\"\\nContoh similarity scores antara Student dan ChatGPT (Student_ChatGPT):\")\n",
    "print(similarity_student_chatgpt1[:5, :5].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Simpan Model Bi-Encoder\n",
    "\n",
    "Model Bi-Encoder yang telah dilatih disimpan untuk penggunaan di masa depan:\n",
    "- Bi-Encoder Student_ChatGPT.\n",
    "- Bi-Encoder Only_ChatGPT.\n",
    "- Tokenizer juga disimpan untuk konsistensi preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model Bi-Encoder untuk student_chatgpt\n",
    "print(\"Menyimpan model Bi-Encoder student_chatgpt...\")\n",
    "history_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model Bi-Encoder untuk only_chatgpt\n",
    "print(\"Menyimpan model Bi-Encoder only_chatgpt...\")\n",
    "history_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Simpan tokenizer configuration\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "print(\"Model dan tokenizer berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pencarian Top-50 Similarity\n",
    "\n",
    "Fungsi pencarian digunakan untuk menemukan 50 kalimat terdekat berdasarkan embeddings:\n",
    "- Query berupa sebuah kalimat.\n",
    "- Cosine similarity digunakan untuk mencari kalimat terdekat dalam dataset.\n",
    "\n",
    "Hasil berupa daftar 50 kalimat yang paling mirip dengan query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mencari top-k kalimat\n",
    "def get_top_k_similar(query_embedding, target_embeddings, target_sentences, k=50):\n",
    "    \"\"\"\n",
    "    Mencari k kalimat paling mirip berdasarkan cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query_embedding (tf.Tensor): Embedding dari query.\n",
    "        target_embeddings (tf.Tensor): Embeddings target.\n",
    "        target_sentences (list): Daftar kalimat target.\n",
    "        k (int): Jumlah kalimat terdekat yang ingin dicari.\n",
    "        \n",
    "    Returns:\n",
    "        list: Daftar kalimat paling mirip.\n",
    "    \"\"\"\n",
    "    # Normalisasi embeddings\n",
    "    query_normalized = tf.nn.l2_normalize(query_embedding, axis=1)\n",
    "    target_normalized = tf.nn.l2_normalize(target_embeddings, axis=1)\n",
    "    \n",
    "    # Hitung similarity\n",
    "    similarity_scores = tf.matmul(query_normalized, target_normalized, transpose_b=True)\n",
    "    \n",
    "    # Ambil top-k kalimat\n",
    "    top_k_indices = tf.math.top_k(similarity_scores, k=k).indices.numpy()\n",
    "    top_k_sentences = [target_sentences[i] for i in top_k_indices[0]]\n",
    "    return top_k_sentences\n",
    "\n",
    "# Contoh penggunaan untuk mencari top-50 di Student_ChatGPT\n",
    "query_sentence = \"Explain the Pythagorean theorem.\"\n",
    "query_embedding = bi_encoder_student_chatgpt.predict(tokenize_text([query_sentence]))\n",
    "top_50_sentences = get_top_k_similar(query_embedding, student_embeddings, student_sentences)\n",
    "\n",
    "print(\"\\nTop-50 kalimat terdekat:\")\n",
    "for i, sentence in enumerate(top_50_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Membuat Model BERT Single Sentence Classifier\n",
    "\n",
    "Model classifier dibuat untuk membedakan teks berdasarkan tiga input:\n",
    "1. Embeddings BERT: Representasi teks dari Bi-Encoder.\n",
    "2. Fitur Stylometric: Representasi linguistik tambahan.\n",
    "3. Similarity Score: Skor kesamaan embeddings.\n",
    "\n",
    "Arsitektur classifier:\n",
    "- Tiga input terpisah dihubungkan melalui dense layers.\n",
    "- Output berupa probabilitas apakah teks berasal dari Student atau ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layers untuk tiga jenis fitur\n",
    "bert_embedding_input = tf.keras.layers.Input(\n",
    "    shape=(128,), \n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding\"\n",
    ")\n",
    "stylometric_input = tf.keras.layers.Input(\n",
    "    shape=(7,), \n",
    "    dtype=tf.float32, \n",
    "    name=\"stylometric_features\"\n",
    ")\n",
    "similarity_score_input = tf.keras.layers.Input(\n",
    "    shape=(1,), \n",
    "    dtype=tf.float32, \n",
    "    name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "# Dense layer untuk masing-masing input\n",
    "bert_dense = tf.keras.layers.Dense(64, activation=\"relu\")(bert_embedding_input)\n",
    "style_dense = tf.keras.layers.Dense(16, activation=\"relu\")(stylometric_input)\n",
    "sim_dense = tf.keras.layers.Dense(8, activation=\"relu\")(similarity_score_input)\n",
    "\n",
    "# Gabungkan semua fitur\n",
    "combined = tf.keras.layers.Concatenate()([bert_dense, style_dense, sim_dense])\n",
    "\n",
    "# Dense layers untuk klasifikasi\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(combined)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Buat model classifier\n",
    "classifier = tf.keras.Model(\n",
    "    inputs=[bert_embedding_input, stylometric_input, similarity_score_input],\n",
    "    outputs=output,\n",
    "    name=\"single_sentence_classifier\"\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Membuat Data Training\n",
    "\n",
    "Data untuk training dan validation dibuat dengan menggabungkan:\n",
    "- Embeddings dari Bi-Encoder.\n",
    "- Fitur stylometric yang telah dinormalisasi.\n",
    "- Similarity scores.\n",
    "- Label: 0 untuk Student dan 1 untuk ChatGPT.\n",
    "\n",
    "Dataset kemudian dibagi menjadi training dan validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label untuk kalimat\n",
    "student_labels = np.zeros(len(student_embeddings))  # Label 0 untuk Student\n",
    "chatgpt_labels = np.ones(len(chatgpt_1_embeddings))  # Label 1 untuk ChatGPT\n",
    "\n",
    "# Gabungkan data embeddings dan label\n",
    "all_embeddings = np.vstack([student_embeddings, chatgpt_1_embeddings])\n",
    "all_labels = np.hstack([student_labels, chatgpt_labels])\n",
    "\n",
    "# Gabungkan fitur stylometric\n",
    "all_stylometric_features = np.vstack([student_features_normalized, chatgpt_features_1_normalized])\n",
    "\n",
    "# Gabungkan similarity scores\n",
    "all_similarity_scores = np.hstack([\n",
    "    np.diag(similarity_student_chatgpt1),  # Similarity Student vs ChatGPT\n",
    "    np.diag(similarity_chatgpt1_chatgpt2)  # Similarity ChatGPT vs ChatGPT\n",
    "]).reshape(-1, 1)\n",
    "\n",
    "# Split data menjadi training dan validation\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(all_labels)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "\n",
    "# Persiapkan input untuk training dan validation\n",
    "train_inputs = {\n",
    "    \"bert_embedding\": all_embeddings[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "val_inputs = {\n",
    "    \"bert_embedding\": all_embeddings[val_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[val_idx],\n",
    "    \"similarity_score\": all_similarity_scores[val_idx]\n",
    "}\n",
    "\n",
    "train_labels_split = all_labels[train_idx]\n",
    "val_labels_split = all_labels[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Train Model Classifier\n",
    "\n",
    "Model classifier dilatih menggunakan dataset yang telah disiapkan. `Callback EarlyStopping` digunakan untuk menghentikan pelatihan jika performa tidak meningkat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifier\n",
    "print(\"Training Single Sentence Classifier...\")\n",
    "history = classifier.fit(\n",
    "    train_inputs,\n",
    "    train_labels_split,\n",
    "    validation_data=(val_inputs, val_labels_split),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Evaluasi Model\n",
    "\n",
    "Model dievaluasi pada dataset validation:\n",
    "- **Confusion Matrix**: Menampilkan jumlah prediksi benar dan salah untuk masing-masing label.\n",
    "- **Classification Report**: Menampilkan metrik seperti precision, recall, F1-score, dan accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Prediksi pada validation set\n",
    "val_predictions = classifier.predict(val_inputs)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(val_labels_split, val_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels_split, val_predictions_binary, target_names=['Student', 'ChatGPT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Simpan Model Classifier\n",
    "\n",
    "Model classifier yang telah dilatih disimpan bersama dengan konfigurasi tokenizer dan scaler untuk inference di masa depan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat direktori jika belum ada\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model classifier\n",
    "classifier.save('saved_models/single_sentence_classifier.h5')\n",
    "\n",
    "# Simpan konfigurasi tokenizer\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "# Simpan scaler untuk fitur stylometric\n",
    "with open(\"scaler_stylometric.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model dan konfigurasi berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Prediction Inference\n",
    "\n",
    "Fungsi inference dibuat untuk memprediksi apakah sebuah teks berasal dari Student atau ChatGPT:\n",
    "- Teks di-preprocess dan di-tokenisasi.\n",
    "- Embeddings dihasilkan oleh Bi-Encoder.\n",
    "- Fitur stylometric diekstraksi.\n",
    "- Model classifier menghasilkan prediksi probabilitas.\n",
    "\n",
    "Contoh hasil prediksi menampilkan label dan confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_source(text):\n",
    "    \"\"\"\n",
    "    Prediksi apakah teks berasal dari Student atau ChatGPT.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input teks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Hasil prediksi dan confidence score.\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    sentences = preprocess_text(text)\n",
    "    if not sentences:\n",
    "        return {\"error\": \"Input teks kosong atau tidak valid.\"}\n",
    "    \n",
    "    # Tokenisasi teks\n",
    "    encodings = tokenize_text(sentences)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = bi_encoder.predict({\n",
    "        \"input_ids\": encodings['input_ids'],\n",
    "        \"attention_mask\": encodings['attention_mask']\n",
    "    })\n",
    "    \n",
    "    # Extract stylometric features\n",
    "    style_features = np.array([extract_stylometric_features(sent) for sent in sentences])\n",
    "    style_features_normalized = scaler.transform(style_features)\n",
    "    \n",
    "    # Dummy similarity score (karena tidak ada pasangan untuk inference)\n",
    "    similarity_scores = np.zeros((len(sentences), 1))\n",
    "    \n",
    "    # Buat input untuk classifier\n",
    "    inputs = {\n",
    "        \"bert_embedding\": embeddings,\n",
    "        \"stylometric_features\": style_features_normalized,\n",
    "        \"similarity_score\": similarity_scores\n",
    "    }\n",
    "    \n",
    "    # Prediksi menggunakan classifier\n",
    "    predictions = classifier.predict(inputs)\n",
    "    return {\"predictions\": predictions.tolist(), \"avg_confidence\": float(np.mean(predictions))}\n",
    "\n",
    "# Contoh prediksi\n",
    "test_text = \"This is an example sentence written by a student.\"\n",
    "result = predict_text_source(test_text)\n",
    "print(\"\\nHasil Prediksi:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
