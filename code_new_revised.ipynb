{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set seed untuk reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Dataset structure should be like this,\n",
    "\n",
    "Dataset Student_ChatGPT:\n",
    "\n",
    "col 1 - Student (comes from 1 to 2 problem topics)\n",
    "\n",
    "col 2 - ChatGPT (same as Student, comes from 1 to 2 problem topics)\n",
    "\n",
    "---\n",
    "\n",
    "Dataset Only_ChatGPT\n",
    "\n",
    "col 1- ChatGPT (Knowledge of the material for 1 year or 2 semesters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_csv(\"student_chatgpt.csv\")\n",
    "only_chatgpt = pd.read_csv(\"only_chatgpt.csv\")\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Student-ChatGPT Dataset Info:\")\n",
    "print(f\"Number of rows: {len(student_chatgpt)}\")\n",
    "print(\"\\nSample of student_chatgpt data:\")\n",
    "print(student_chatgpt.head(2))\n",
    "\n",
    "print(\"\\nOnly-ChatGPT Dataset Info:\")\n",
    "print(f\"Number of rows: {len(only_chatgpt)}\")\n",
    "print(\"\\nSample of only_chatgpt data:\")\n",
    "print(only_chatgpt.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase and segment into sentences\"\"\"\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    return sentences\n",
    "\n",
    "# Process student essays\n",
    "student_sentences = []\n",
    "student_labels = []\n",
    "for text in student_chatgpt['student']:\n",
    "    sentences = preprocess_text(text)\n",
    "    student_sentences.extend(sentences)\n",
    "    student_labels.extend([0] * len(sentences))  # 0 for student\n",
    "\n",
    "# Process ChatGPT responses from student_chatgpt dataset\n",
    "chatgpt_sentences_1 = []\n",
    "chatgpt_labels_1 = []\n",
    "for text in student_chatgpt['chatgpt']:\n",
    "    sentences = preprocess_text(text)\n",
    "    chatgpt_sentences_1.extend(sentences)\n",
    "    chatgpt_labels_1.extend([1] * len(sentences))  # 1 for ChatGPT\n",
    "\n",
    "# Process only_chatgpt responses\n",
    "chatgpt_sentences_2 = []\n",
    "chatgpt_labels_2 = []\n",
    "for text in only_chatgpt['chatgpt']:\n",
    "    sentences = preprocess_text(text)\n",
    "    chatgpt_sentences_2.extend(sentences)\n",
    "    chatgpt_labels_2.extend([1] * len(sentences))\n",
    "\n",
    "print(f\"Number of student sentences: {len(student_sentences)}\")\n",
    "print(f\"Number of ChatGPT sentences (from student_chatgpt): {len(chatgpt_sentences_1)}\")\n",
    "print(f\"Number of ChatGPT sentences (from only_chatgpt): {len(chatgpt_sentences_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inisialisasi BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer for IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(texts, max_length=512):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Membuat Model BERT Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model dasar IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Arsitektur untuk Bi-Encoder ChatGPT\n",
    "input_ids_chatgpt = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask_chatgpt = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Dapatkan embeddings dari BERT\n",
    "bert_outputs_chatgpt = bert_model(input_ids_chatgpt, attention_mask=attention_mask_chatgpt)[0]\n",
    "cls_token_chatgpt = bert_outputs_chatgpt[:, 0, :]  # Ambil token [CLS]\n",
    "\n",
    "# Layer-layer tambahan untuk fine-tuning\n",
    "dense1_chatgpt = tf.keras.layers.Dense(256, activation=\"relu\")(cls_token_chatgpt)\n",
    "dropout1_chatgpt = tf.keras.layers.Dropout(0.1)(dense1_chatgpt)\n",
    "dense2_chatgpt = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1_chatgpt)\n",
    "dropout2_chatgpt = tf.keras.layers.Dropout(0.1)(dense2_chatgpt)\n",
    "output_chatgpt = tf.keras.layers.Dense(128)(dropout2_chatgpt)\n",
    "\n",
    "# Normalisasi output\n",
    "normalized_output_chatgpt = tf.nn.l2_normalize(output_chatgpt, axis=1)\n",
    "\n",
    "# Buat model Bi-Encoder untuk ChatGPT\n",
    "bi_encoder_chatgpt = tf.keras.Model(\n",
    "    inputs=[input_ids_chatgpt, attention_mask_chatgpt],\n",
    "    outputs=normalized_output_chatgpt,\n",
    "    name=\"bi_encoder_chatgpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arsitektur untuk Bi-Encoder Student (struktur yang sama, variable berbeda)\n",
    "input_ids_student = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask_student = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "bert_outputs_student = bert_model(input_ids_student, attention_mask=attention_mask_student)[0]\n",
    "cls_token_student = bert_outputs_student[:, 0, :]\n",
    "\n",
    "dense1_student = tf.keras.layers.Dense(256, activation=\"relu\")(cls_token_student)\n",
    "dropout1_student = tf.keras.layers.Dropout(0.1)(dense1_student)\n",
    "dense2_student = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1_student)\n",
    "dropout2_student = tf.keras.layers.Dropout(0.1)(dense2_student)\n",
    "output_student = tf.keras.layers.Dense(128)(dropout2_student)\n",
    "\n",
    "normalized_output_student = tf.nn.l2_normalize(output_student, axis=1)\n",
    "\n",
    "bi_encoder_student = tf.keras.Model(\n",
    "    inputs=[input_ids_student, attention_mask_student],\n",
    "    outputs=normalized_output_student,\n",
    "    name=\"bi_encoder_student\"\n",
    ")\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Bi-Encoder\n",
    "\n",
    "Problem resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi contrastive loss\n",
    "def contrastive_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # y_true: 1 untuk similar pairs, 0 untuk dissimilar pairs\n",
    "        # y_pred: cosine similarity antara dua embeddings\n",
    "        \n",
    "        # Konversi similarity ke distance\n",
    "        distance = 1 - y_pred\n",
    "        \n",
    "        # Loss untuk similar pairs\n",
    "        positive_loss = y_true * tf.square(distance)\n",
    "        \n",
    "        # Loss untuk dissimilar pairs\n",
    "        negative_loss = (1 - y_true) * tf.square(tf.maximum(margin - distance, 0))\n",
    "        \n",
    "        return tf.reduce_mean(positive_loss + negative_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Training bi_encoder_chatgpt dengan data only_chatgpt\n",
    "\n",
    "# Persiapkan data untuk training\n",
    "print(\"Mempersiapkan data training untuk ChatGPT Bi-Encoder...\")\n",
    "\n",
    "# Tokenisasi data ChatGPT\n",
    "chatgpt2_train_encodings = tokenize_text(chatgpt_sentences_2)\n",
    "\n",
    "# Buat positive pairs dari kalimat yang berdekatan\n",
    "batch_size = 32\n",
    "train_steps = len(chatgpt_sentences_2) // batch_size\n",
    "\n",
    "# Compile model dengan contrastive loss\n",
    "bi_encoder_chatgpt.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=contrastive_loss(margin=1.0)\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"Training Bi-Encoder ChatGPT...\")\n",
    "bi_encoder_chatgpt.fit(\n",
    "    {\n",
    "        \"input_ids\": chatgpt2_train_encodings['input_ids'],\n",
    "        \"attention_mask\": chatgpt2_train_encodings['attention_mask']\n",
    "    },\n",
    "    # Target matrix: diagonal adalah 1 (similar pairs), sisanya 0\n",
    "    tf.eye(len(chatgpt_sentences_2)),\n",
    "    epochs=3,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Training bi_encoder_student dengan data student_chatgpt\n",
    "\n",
    "# Persiapkan data training\n",
    "print(\"Mempersiapkan data training untuk Student Bi-Encoder...\")\n",
    "\n",
    "# Tokenisasi data Student dan ChatGPT\n",
    "student_train_encodings = tokenize_text(student_sentences)\n",
    "chatgpt1_train_encodings = tokenize_text(chatgpt_sentences_1)\n",
    "\n",
    "# Combine data untuk training\n",
    "combined_input_ids = tf.concat([\n",
    "    student_train_encodings['input_ids'],\n",
    "    chatgpt1_train_encodings['input_ids']\n",
    "], axis=0)\n",
    "\n",
    "combined_attention_mask = tf.concat([\n",
    "    student_train_encodings['attention_mask'],\n",
    "    chatgpt1_train_encodings['attention_mask']\n",
    "], axis=0)\n",
    "\n",
    "# Buat similarity matrix\n",
    "total_samples = len(student_sentences) + len(chatgpt_sentences_1)\n",
    "similarity_matrix = tf.zeros((total_samples, total_samples))\n",
    "\n",
    "# Set similarity 1 untuk pasangan dari sumber yang sama\n",
    "student_size = len(student_sentences)\n",
    "similarity_matrix = tf.tensor_scatter_nd_update(\n",
    "    similarity_matrix,\n",
    "    tf.where(tf.eye(student_size) > 0),\n",
    "    tf.ones(student_size)\n",
    ")\n",
    "\n",
    "chatgpt_start = student_size\n",
    "chatgpt_size = len(chatgpt_sentences_1)\n",
    "similarity_matrix = tf.tensor_scatter_nd_update(\n",
    "    similarity_matrix,\n",
    "    tf.where(tf.eye(chatgpt_size) > 0) + chatgpt_start,\n",
    "    tf.ones(chatgpt_size)\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "bi_encoder_student.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=contrastive_loss(margin=1.0)\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"Training Bi-Encoder Student...\")\n",
    "bi_encoder_student.fit(\n",
    "    {\n",
    "        \"input_ids\": combined_input_ids,\n",
    "        \"attention_mask\": combined_attention_mask\n",
    "    },\n",
    "    similarity_matrix,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"ChatGPT Bi-Encoder training data size: {len(chatgpt_sentences_2)} sentences\")\n",
    "print(f\"Student Bi-Encoder training data size: {len(student_sentences) + len(chatgpt_sentences_1)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluasi Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siapkan data test untuk evaluasi\n",
    "# Gunakan sebagian kecil data yang belum digunakan dalam training\n",
    "test_student = student_sentences[-100:]  # Ambil 100 kalimat terakhir untuk testing\n",
    "test_chatgpt_1 = chatgpt_sentences_1[-100:]  # Dari student_chatgpt dataset\n",
    "test_chatgpt_2 = chatgpt_sentences_2[-100:]  # Dari only_chatgpt dataset\n",
    "\n",
    "# Tokenisasi data test\n",
    "test_student_encodings = tokenize_text(test_student)\n",
    "test_chatgpt1_encodings = tokenize_text(test_chatgpt_1)\n",
    "test_chatgpt2_encodings = tokenize_text(test_chatgpt_2)\n",
    "\n",
    "# Generate embeddings untuk data test\n",
    "print(\"Generating test embeddings...\")\n",
    "test_student_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": test_student_encodings['input_ids'],\n",
    "    \"attention_mask\": test_student_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "test_chatgpt1_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": test_chatgpt1_encodings['input_ids'],\n",
    "    \"attention_mask\": test_chatgpt1_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "test_chatgpt2_embeddings = bi_encoder_chatgpt.predict({\n",
    "    \"input_ids\": test_chatgpt2_encodings['input_ids'],\n",
    "    \"attention_mask\": test_chatgpt2_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "# Hitung similarity scores\n",
    "def calculate_similarity_metrics(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Menghitung similarity scores antara dua set embeddings\n",
    "    Returns: mean similarity, min similarity, max similarity\n",
    "    \"\"\"\n",
    "    # Normalisasi embeddings\n",
    "    normalized_emb1 = tf.nn.l2_normalize(embeddings1, axis=1)\n",
    "    normalized_emb2 = tf.nn.l2_normalize(embeddings2, axis=1)\n",
    "    \n",
    "    # Hitung similarity matrix\n",
    "    similarity_matrix = tf.matmul(normalized_emb1, normalized_emb2, transpose_b=True)\n",
    "    \n",
    "    # Ambil metrics\n",
    "    mean_sim = tf.reduce_mean(similarity_matrix)\n",
    "    max_sim = tf.reduce_max(similarity_matrix)\n",
    "    min_sim = tf.reduce_min(similarity_matrix)\n",
    "    \n",
    "    return mean_sim.numpy(), min_sim.numpy(), max_sim.numpy()\n",
    "\n",
    "# Evaluasi similarity antara different types of text\n",
    "print(\"\\nEvaluasi Similarity Scores:\")\n",
    "\n",
    "# Student vs Student\n",
    "mean_sim, min_sim, max_sim = calculate_similarity_metrics(\n",
    "    test_student_embeddings[:50], test_student_embeddings[50:])\n",
    "print(\"\\nStudent vs Student Similarity:\")\n",
    "print(f\"Mean: {mean_sim:.3f}, Min: {min_sim:.3f}, Max: {max_sim:.3f}\")\n",
    "\n",
    "# Student vs ChatGPT (from student_chatgpt)\n",
    "mean_sim, min_sim, max_sim = calculate_similarity_metrics(\n",
    "    test_student_embeddings, test_chatgpt1_embeddings)\n",
    "print(\"\\nStudent vs ChatGPT (student_chatgpt) Similarity:\")\n",
    "print(f\"Mean: {mean_sim:.3f}, Min: {min_sim:.3f}, Max: {max_sim:.3f}\")\n",
    "\n",
    "# ChatGPT vs ChatGPT (across datasets)\n",
    "mean_sim, min_sim, max_sim = calculate_similarity_metrics(\n",
    "    test_chatgpt1_embeddings, test_chatgpt2_embeddings)\n",
    "print(\"\\nChatGPT vs ChatGPT (across datasets) Similarity:\")\n",
    "print(f\"Mean: {mean_sim:.3f}, Min: {min_sim:.3f}, Max: {max_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Simpan Model Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model Bi-Encoder untuk student_chatgpt\n",
    "print(\"Menyimpan model Bi-Encoder student_chatgpt...\")\n",
    "bi_encoder_student.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model Bi-Encoder untuk only_chatgpt\n",
    "print(\"Menyimpan model Bi-Encoder only_chatgpt...\")\n",
    "bi_encoder_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Simpan tokenizer configuration\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "print(\"Model dan tokenizer berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating embeddings for all sentences...\")\n",
    "\n",
    "# Generate embeddings untuk student sentences\n",
    "print(\"\\nGenerating student embeddings...\")\n",
    "student_encodings = tokenize_text(student_sentences)\n",
    "student_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": student_encodings['input_ids'],\n",
    "    \"attention_mask\": student_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "# Generate embeddings untuk ChatGPT dari student_chatgpt\n",
    "print(\"Generating ChatGPT (student_chatgpt) embeddings...\")\n",
    "chatgpt1_encodings = tokenize_text(chatgpt_sentences_1)\n",
    "chatgpt1_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": chatgpt1_encodings['input_ids'],\n",
    "    \"attention_mask\": chatgpt1_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "# Generate embeddings untuk ChatGPT dari only_chatgpt\n",
    "print(\"Generating ChatGPT (only_chatgpt) embeddings...\")\n",
    "chatgpt2_encodings = tokenize_text(chatgpt_sentences_2)\n",
    "chatgpt2_embeddings = bi_encoder_chatgpt.predict({\n",
    "    \"input_ids\": chatgpt2_encodings['input_ids'],\n",
    "    \"attention_mask\": chatgpt2_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "# Simpan embeddings untuk penggunaan selanjutnya\n",
    "print(\"\\nMenyimpan embeddings...\")\n",
    "np.save('saved_models/student_embeddings.npy', student_embeddings)\n",
    "np.save('saved_models/chatgpt1_embeddings.npy', chatgpt1_embeddings)\n",
    "np.save('saved_models/chatgpt2_embeddings.npy', chatgpt2_embeddings)\n",
    "\n",
    "print(\"\\nSummary of generated embeddings:\")\n",
    "print(f\"Student embeddings shape: {student_embeddings.shape}\")\n",
    "print(f\"ChatGPT (student_chatgpt) embeddings shape: {chatgpt1_embeddings.shape}\")\n",
    "print(f\"ChatGPT (only_chatgpt) embeddings shape: {chatgpt2_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mencari top 50 kalimat yang mirip untuk setiap kalimat...\")\n",
    "\n",
    "def get_top_k_similar(query_embedding, target_embeddings, k=50):\n",
    "    \"\"\"\n",
    "    Mencari k kalimat yang paling mirip berdasarkan cosine similarity\n",
    "    Args:\n",
    "        query_embedding: embedding dari kalimat input (1 x embedding_size)\n",
    "        target_embeddings: kumpulan embedding target (n x embedding_size)\n",
    "        k: jumlah kalimat mirip yang diinginkan\n",
    "    Returns:\n",
    "        indices: indeks dari k kalimat yang paling mirip\n",
    "        scores: nilai similarity untuk setiap kalimat\n",
    "    \"\"\"\n",
    "    # Normalisasi embeddings\n",
    "    query_normalized = tf.nn.l2_normalize(query_embedding, axis=1)\n",
    "    target_normalized = tf.nn.l2_normalize(target_embeddings, axis=1)\n",
    "    \n",
    "    # Hitung similarity\n",
    "    similarity_scores = tf.matmul(query_normalized, target_normalized, transpose_b=True)\n",
    "    \n",
    "    # Ambil top k\n",
    "    scores, indices = tf.nn.top_k(similarity_scores[0], k=k)\n",
    "    \n",
    "    return indices.numpy(), scores.numpy()\n",
    "\n",
    "# List untuk menyimpan pasangan kalimat mirip dan labelnya\n",
    "similar_pairs = []\n",
    "similar_labels = []\n",
    "\n",
    "# Proses setiap kalimat student\n",
    "for idx, student_embedding in enumerate(student_embeddings):\n",
    "    print(f\"\\rMemproses kalimat ke-{idx+1}/{len(student_embeddings)}\", end=\"\")\n",
    "    \n",
    "    # Reshape embedding untuk matmul\n",
    "    query_emb = tf.reshape(student_embedding, (1, -1))\n",
    "    \n",
    "    # 1. Cari 25 kalimat mirip dari data student\n",
    "    student_indices, student_scores = get_top_k_similar(\n",
    "        query_emb, student_embeddings, k=50\n",
    "    )\n",
    "    \n",
    "    # 2. Cari 25 kalimat mirip dari data ChatGPT (gabungan)\n",
    "    # Gabungkan embeddings ChatGPT dari kedua dataset\n",
    "    combined_chatgpt_embeddings = np.vstack([chatgpt1_embeddings, chatgpt2_embeddings])\n",
    "    chatgpt_indices, chatgpt_scores = get_top_k_similar(\n",
    "        query_emb, combined_chatgpt_embeddings, k=50\n",
    "    )\n",
    "    \n",
    "    # Tambahkan ke daftar pasangan\n",
    "    # Pasangan dengan kalimat student\n",
    "    for i, score in zip(student_indices, student_scores):\n",
    "        if i != idx:  # Hindari pasangan dengan diri sendiri\n",
    "            similar_pairs.append({\n",
    "                'input_sentence': student_sentences[idx],\n",
    "                'similar_sentence': student_sentences[i],\n",
    "                'similarity_score': score,\n",
    "                'label': 0  # 0 untuk pasangan student-student\n",
    "            })\n",
    "    \n",
    "    # Pasangan dengan kalimat ChatGPT\n",
    "    for i, score in zip(chatgpt_indices, chatgpt_scores):\n",
    "        # Tentukan sumber kalimat ChatGPT (dataset 1 atau 2)\n",
    "        if i < len(chatgpt_sentences_1):\n",
    "            similar_sentence = chatgpt_sentences_1[i]\n",
    "        else:\n",
    "            similar_sentence = chatgpt_sentences_2[i - len(chatgpt_sentences_1)]\n",
    "            \n",
    "        similar_pairs.append({\n",
    "            'input_sentence': student_sentences[idx],\n",
    "            'similar_sentence': similar_sentence,\n",
    "            'similarity_score': score,\n",
    "            'label': 1  # 1 untuk pasangan student-chatgpt\n",
    "        })\n",
    "\n",
    "print(\"\\n\\nStatistik hasil pencarian kalimat mirip:\")\n",
    "print(f\"Total pasangan kalimat yang dihasilkan: {len(similar_pairs)}\")\n",
    "print(f\"Jumlah pasangan student-student: {sum(1 for pair in similar_pairs if pair['label'] == 0)}\")\n",
    "print(f\"Jumlah pasangan student-chatgpt: {sum(1 for pair in similar_pairs if pair['label'] == 1)}\")\n",
    "\n",
    "# Simpan hasil untuk digunakan di Cross-Encoder\n",
    "import pickle\n",
    "with open('saved_models/similar_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(similar_pairs, f)\n",
    "\n",
    "print(\"\\nHasil pencarian kalimat mirip telah disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Membuat Model BERT Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer untuk Cross-Encoder\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# BERT embeddings\n",
    "bert_outputs = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "cls_token = bert_outputs[:, 0, :]  # Representasi token CLS\n",
    "\n",
    "# Dense layers untuk klasifikasi\n",
    "dense1 = tf.keras.layers.Dense(256, activation=\"relu\")(cls_token)\n",
    "dropout1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "dense2 = tf.keras.layers.Dense(64, activation=\"relu\")(dropout1)\n",
    "dropout2 = tf.keras.layers.Dropout(0.1)(dense2)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout2)\n",
    "\n",
    "# Model Cross-Encoder\n",
    "cross_encoder = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Kompilasi Cross-Encoder\n",
    "cross_encoder.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", \"precision\", \"recall\", \"AUC\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Membuat Pasangan Data untuk Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cross-encoder training data\n",
    "cross_encoder_inputs = []\n",
    "for pair in similar_pairs:\n",
    "    encoding = tokenizer(\n",
    "        pair['input_sentence'],\n",
    "        pair['similar_sentence'],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    cross_encoder_inputs.append({\n",
    "        \"input_ids\": encoding['input_ids'][0],\n",
    "        \"attention_mask\": encoding['attention_mask'][0]\n",
    "    })\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids = tf.stack([x[\"input_ids\"] for x in cross_encoder_inputs])\n",
    "attention_masks = tf.stack([x[\"attention_mask\"] for x in cross_encoder_inputs])\n",
    "labels = tf.convert_to_tensor(similar_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.fit(\n",
    "    {\"input_ids\": input_ids, \"attention_mask\": attention_masks},\n",
    "    labels,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = similar_pairs[-1000:]  # Take last 1000 pairs for testing\n",
    "test_encodings = []\n",
    "test_labels = []\n",
    "\n",
    "for pair in test_pairs:\n",
    "    encoding = tokenizer(\n",
    "        pair['input_sentence'],\n",
    "        pair['similar_sentence'],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    test_encodings.append({\n",
    "        \"input_ids\": encoding['input_ids'][0],\n",
    "        \"attention_mask\": encoding['attention_mask'][0]\n",
    "    })\n",
    "    test_labels.append(pair['label'])\n",
    "\n",
    "test_input_ids = tf.stack([x[\"input_ids\"] for x in test_encodings])\n",
    "test_attention_masks = tf.stack([x[\"attention_mask\"] for x in test_encodings])\n",
    "test_labels = tf.convert_to_tensor(test_labels)\n",
    "\n",
    "results = cross_encoder.evaluate(\n",
    "    {\"input_ids\": test_input_ids, \"attention_mask\": test_attention_masks},\n",
    "    test_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Simpan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "bi_encoder_student.save(\"bi_encoder_student_model.h5\")\n",
    "bi_encoder_chatgpt.save(\"bi_encoder_chatgpt_model.h5\")\n",
    "cross_encoder.save(\"cross_encoder_model.h5\")\n",
    "\n",
    "print(\"All models have been saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
