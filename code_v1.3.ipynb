{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_excel(\"example_datasets/examples-datasets-mar25.xlsx\")\n",
    "only_chatgpt = pd.read_excel(\"example_datasets/knowledge-datasets-mar25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan informasi dataset\n",
    "print(\"Dataset Student_ChatGPT:\")\n",
    "student_chatgpt.info()\n",
    "print(\"\\nDataset Only_ChatGPT:\")\n",
    "only_chatgpt.info()\n",
    "\n",
    "# Tampilkan beberapa baris awal dataset\n",
    "print(\"\\nContoh Data Student_ChatGPT:\")\n",
    "student_chatgpt.head()\n",
    "\n",
    "print(\"\\nContoh Data Only_ChatGPT:\")\n",
    "only_chatgpt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing teks:\n",
    "    - Mengubah teks menjadi huruf kecil\n",
    "    - Membersihkan whitespace berlebih\n",
    "    - Menjaga teks sebagai paragraf utuh\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks input.\n",
    "        \n",
    "    Returns:\n",
    "        str: Teks yang telah diproses atau None jika tidak valid.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Bersihkan teks\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Ganti multiple whitespace dengan satu spasi\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data Student_ChatGPT\n",
    "std_par = []\n",
    "gpt_par_1 = []  # ChatGPT dari dataset Student_ChatGPT\n",
    "\n",
    "# Proses teks Student\n",
    "for text in student_chatgpt['Pelajar']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        std_par.append(processed)\n",
    "\n",
    "# Proses teks ChatGPT (dari Student_ChatGPT)\n",
    "for text in student_chatgpt['GPT']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        gpt_par_1.append(processed)\n",
    "\n",
    "# Preprocessing data Only_ChatGPT\n",
    "gpt_par_2 = []  # ChatGPT dari dataset Only_ChatGPT\n",
    "\n",
    "# Proses teks ChatGPT (dari Only_ChatGPT)\n",
    "for text in only_chatgpt['GPT']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        gpt_par_2.append(processed)\n",
    "\n",
    "# Tampilkan jumlah data hasil preprocessing\n",
    "print(f\"Total paragraf Student: {len(std_par)}\")\n",
    "print(f\"Total paragraf ChatGPT (Student_ChatGPT): {len(gpt_par_1)}\")\n",
    "print(f\"Total paragraf ChatGPT (Only_ChatGPT): {len(gpt_par_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(texts, max_length=256):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi data\n",
    "print(\"Tokenisasi kalimat Student...\")\n",
    "std_tokens = tokenize_text(std_par)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\")\n",
    "gpt_tokens_1 = tokenize_text(gpt_par_1)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\")\n",
    "gpt_tokens_2 = tokenize_text(gpt_par_2)\n",
    "\n",
    "# Tampilkan hasil tokenisasi (contoh: Student)\n",
    "print(\"\\nContoh hasil tokenisasi:\")\n",
    "print(std_tokens['input_ids'][:1])  # Input token ID\n",
    "print(std_tokens['attention_mask'][:1])  # Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memeriksa distribusi panjang token untuk memastikan max_length cukup\n",
    "student_lengths = [sum(mask) for mask in std_tokens['attention_mask'].numpy()]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(student_lengths, bins=30)\n",
    "plt.title('Distribusi Panjang Token (Student)')\n",
    "plt.xlabel('Jumlah Token Aktif')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.show()\n",
    "print(f\"Rata-rata panjang token: {np.mean(student_lengths):.2f}\")\n",
    "print(f\"Persentase terpotong: {sum(l == 128 for l in student_lengths) / len(student_lengths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendekode token untuk memastikan tokenisasi berfungsi dengan baik\n",
    "sample_text = std_par[0]\n",
    "sample_tokens = tokenizer.encode(sample_text)\n",
    "print(f\"Teks asli: {sample_text}\")\n",
    "print(f\"Token ID: {sample_tokens}\")\n",
    "print(f\"Token dekode: {tokenizer.decode(sample_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika tokenisasi memakan waktu lama, pertimbangkan untuk menyimpannya\n",
    "tokenized_data = {\n",
    "    'student': std_tokens,\n",
    "    'chatgpt_1 ': gpt_tokens_1 ,\n",
    "    'chatgpt_2': gpt_tokens_2\n",
    "}\n",
    "\n",
    "# Menyimpan input_ids dan attention_mask sebagai numpy arrays\n",
    "tokenized_numpy = {\n",
    "    'student': {\n",
    "        'input_ids': std_tokens['input_ids'].numpy(),\n",
    "        'attention_mask': std_tokens['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt ': {\n",
    "        'input_ids': gpt_tokens_1 ['input_ids'].numpy(),\n",
    "        'attention_mask': gpt_tokens_1 ['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt_2': {\n",
    "        'input_ids': gpt_tokens_2['input_ids'].numpy(),\n",
    "        'attention_mask': gpt_tokens_2['attention_mask'].numpy()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('tokenized_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_numpy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "#Buat Model fine tuning Bi-Encoder Student_ChatGPT\n",
    "\n",
    "#Buat Model fine tuning Bi-Encoder Only_ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Freeze BERT layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Fungsi untuk membuat Bi-Encoder\n",
    "def build_bi_encoder(bert_model):\n",
    "    \"\"\"\n",
    "    Membuat model Bi-Encoder dengan IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        bert_model (TFBertModel): Model dasar IndoBERT.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Model Bi-Encoder.\n",
    "    \"\"\"\n",
    "    # Input layer untuk token ID dan attention mask\n",
    "    input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Extract CLS token embeddings dari IndoBERT\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0][:, 0, :]  # [CLS] token\n",
    "    \n",
    "    # Dense layer untuk fine-tuning\n",
    "    dense1 = tf.keras.layers.Dense(256, activation=\"relu\")(bert_output)\n",
    "    dropout1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "    dense2 = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1)\n",
    "    dropout2 = tf.keras.layers.Dropout(0.1)(dense2)\n",
    "    dense3 = tf.keras.layers.Dense(128)(dropout2)\n",
    "    \n",
    "    # Normalisasi output (L2 normalization)\n",
    "    normalized_output = tf.nn.l2_normalize(dense3, axis=1)\n",
    "    \n",
    "    # Model Bi-Encoder\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=normalized_output)\n",
    "\n",
    "# Buat dua Bi-Encoder\n",
    "bi_encoder_student_chatgpt = build_bi_encoder(bert_model)\n",
    "bi_encoder_only_chatgpt = build_bi_encoder(bert_model)\n",
    "\n",
    "# Tampilkan arsitektur\n",
    "print(\"Bi-Encoder untuk Student_ChatGPT:\")\n",
    "bi_encoder_student_chatgpt.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Datasets Student_ChatGPT (Gunakan Model Fine-Tuning IndoBERT di atas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat pasangan data untuk contrastive learning\n",
    "def create_contrastive_pairs(student_tokens, chatgpt_tokens, n_pairs=5000):\n",
    "    \"\"\"\n",
    "    Membuat pasangan data untuk contrastive learning.\n",
    "    \n",
    "    Args:\n",
    "        student_tokens: Token dari teks student.\n",
    "        chatgpt_tokens: Token dari teks ChatGPT.\n",
    "        n_pairs: Jumlah pasangan yang akan dibuat.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Pasangan anchor, positive, negative, dan label.\n",
    "    \"\"\"\n",
    "    # Jumlah data\n",
    "    n_student = student_tokens['input_ids'].shape[0]\n",
    "    n_chatgpt = chatgpt_tokens['input_ids'].shape[0]\n",
    "    \n",
    "    # Inisialisasi array untuk pasangan data\n",
    "    anchor_input_ids = []\n",
    "    anchor_attention_mask = []\n",
    "    positive_input_ids = []\n",
    "    positive_attention_mask = []\n",
    "    negative_input_ids = []\n",
    "    negative_attention_mask = []\n",
    "    labels = []\n",
    "    \n",
    "    # Membuat pasangan positif (student-student)\n",
    "    for _ in range(n_pairs // 4):\n",
    "        idx1, idx2 = np.random.choice(n_student, 2, replace=False)\n",
    "        \n",
    "        anchor_input_ids.append(student_tokens['input_ids'][idx1])\n",
    "        anchor_attention_mask.append(student_tokens['attention_mask'][idx1])\n",
    "        \n",
    "        positive_input_ids.append(student_tokens['input_ids'][idx2])\n",
    "        positive_attention_mask.append(student_tokens['attention_mask'][idx2])\n",
    "        \n",
    "        # Negative dari ChatGPT\n",
    "        neg_idx = np.random.choice(n_chatgpt)\n",
    "        negative_input_ids.append(chatgpt_tokens['input_ids'][neg_idx])\n",
    "        negative_attention_mask.append(chatgpt_tokens['attention_mask'][neg_idx])\n",
    "        \n",
    "        labels.append(1)  # 1 untuk pasangan positif\n",
    "    \n",
    "    # Membuat pasangan positif (chatgpt-chatgpt)\n",
    "    for _ in range(n_pairs // 4):\n",
    "        idx1, idx2 = np.random.choice(n_chatgpt, 2, replace=False)\n",
    "        \n",
    "        anchor_input_ids.append(chatgpt_tokens['input_ids'][idx1])\n",
    "        anchor_attention_mask.append(chatgpt_tokens['attention_mask'][idx1])\n",
    "        \n",
    "        positive_input_ids.append(chatgpt_tokens['input_ids'][idx2])\n",
    "        positive_attention_mask.append(chatgpt_tokens['attention_mask'][idx2])\n",
    "        \n",
    "        # Negative dari Student\n",
    "        neg_idx = np.random.choice(n_student)\n",
    "        negative_input_ids.append(student_tokens['input_ids'][neg_idx])\n",
    "        negative_attention_mask.append(student_tokens['attention_mask'][neg_idx])\n",
    "        \n",
    "        labels.append(1)  # 1 untuk pasangan positif\n",
    "    \n",
    "    # Membuat pasangan negatif (student-chatgpt)\n",
    "    for _ in range(n_pairs // 2):\n",
    "        student_idx = np.random.choice(n_student)\n",
    "        chatgpt_idx = np.random.choice(n_chatgpt)\n",
    "        \n",
    "        anchor_input_ids.append(student_tokens['input_ids'][student_idx])\n",
    "        anchor_attention_mask.append(student_tokens['attention_mask'][student_idx])\n",
    "        \n",
    "        negative_input_ids.append(chatgpt_tokens['input_ids'][chatgpt_idx])\n",
    "        negative_attention_mask.append(chatgpt_tokens['attention_mask'][chatgpt_idx])\n",
    "        \n",
    "        # Positive dari Student (berbeda dengan anchor)\n",
    "        pos_idx = np.random.choice([i for i in range(n_student) if i != student_idx])\n",
    "        positive_input_ids.append(student_tokens['input_ids'][pos_idx])\n",
    "        positive_attention_mask.append(student_tokens['attention_mask'][pos_idx])\n",
    "        \n",
    "        labels.append(0)  # 0 untuk pasangan negatif\n",
    "    \n",
    "    # Konversi ke tensor\n",
    "    return {\n",
    "        'anchor': {\n",
    "            'input_ids': tf.convert_to_tensor(anchor_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(anchor_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'positive': {\n",
    "            'input_ids': tf.convert_to_tensor(positive_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(positive_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': tf.convert_to_tensor(negative_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(negative_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'labels': tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    }\n",
    "\n",
    "# Buat pasangan data untuk model Student_ChatGPT\n",
    "student_chatgpt_pairs = create_contrastive_pairs(std_tokens, gpt_tokens_1, n_pairs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi contrastive loss\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Contrastive loss untuk triplet (anchor, positive, negative).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Label (1 untuk pasangan positif, 0 untuk pasangan negatif).\n",
    "        y_pred: Jarak antara anchor-positive dan anchor-negative.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Nilai loss.\n",
    "    \"\"\"\n",
    "    margin = 0.5\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model untuk training dengan triplet loss\n",
    "def build_triplet_model(bi_encoder):\n",
    "    \"\"\"\n",
    "    Membangun model untuk training dengan triplet loss.\n",
    "    \n",
    "    Args:\n",
    "        bi_encoder: Model bi-encoder yang akan dilatih.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Model untuk training dengan triplet loss.\n",
    "    \"\"\"\n",
    "    # Input untuk anchor, positive, dan negative\n",
    "    anchor_input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"anchor_input_ids\")\n",
    "    anchor_attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"anchor_attention_mask\")\n",
    "    \n",
    "    positive_input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"positive_input_ids\")\n",
    "    positive_attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"positive_attention_mask\")\n",
    "    \n",
    "    negative_input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"negative_input_ids\")\n",
    "    negative_attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"negative_attention_mask\")\n",
    "    \n",
    "    # Embedding untuk anchor, positive, dan negative\n",
    "    anchor_embedding = bi_encoder([anchor_input_ids, anchor_attention_mask])\n",
    "    positive_embedding = bi_encoder([positive_input_ids, positive_attention_mask])\n",
    "    negative_embedding = bi_encoder([negative_input_ids, negative_attention_mask])\n",
    "    \n",
    "    # Hitung cosine similarity\n",
    "    pos_similarity = tf.reduce_sum(anchor_embedding * positive_embedding, axis=1)\n",
    "    neg_similarity = tf.reduce_sum(anchor_embedding * negative_embedding, axis=1)\n",
    "    \n",
    "    # Output model adalah perbedaan similarity\n",
    "    output = tf.stack([pos_similarity, neg_similarity], axis=1)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[\n",
    "            anchor_input_ids, anchor_attention_mask,\n",
    "            positive_input_ids, positive_attention_mask,\n",
    "            negative_input_ids, negative_attention_mask\n",
    "        ],\n",
    "        outputs=output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model triplet untuk Student_ChatGPT\n",
    "triplet_model_student_chatgpt = build_triplet_model(bi_encoder_student_chatgpt)\n",
    "\n",
    "# Custom loss function untuk triplet\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Triplet loss: mendorong similarity positif lebih tinggi dari similarity negatif.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tidak digunakan dalam triplet loss.\n",
    "        y_pred: Stack dari [positive_similarity, negative_similarity].\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Nilai loss.\n",
    "    \"\"\"\n",
    "    pos_sim = y_pred[:, 0]\n",
    "    neg_sim = y_pred[:, 1]\n",
    "    margin = 0.5\n",
    "    \n",
    "    # Triplet loss: max(0, margin - (pos_sim - neg_sim))\n",
    "    loss = tf.maximum(0., margin - (pos_sim - neg_sim))\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "triplet_model_student_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=triplet_loss\n",
    ")\n",
    "\n",
    "# Training model Student_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Student_ChatGPT...\")\n",
    "history_student_chatgpt = triplet_model_student_chatgpt.fit(\n",
    "    x=[\n",
    "        student_chatgpt_pairs['anchor']['input_ids'],\n",
    "        student_chatgpt_pairs['anchor']['attention_mask'],\n",
    "        student_chatgpt_pairs['positive']['input_ids'],\n",
    "        student_chatgpt_pairs['positive']['attention_mask'],\n",
    "        student_chatgpt_pairs['negative']['input_ids'],\n",
    "        student_chatgpt_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    y=student_chatgpt_pairs['labels'],  # Tidak digunakan dalam triplet loss\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Datasets Only_ChatGPT (Self-Supervised, Gunakan Model Fine-Tuning IndoBERT di atas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat pasangan data untuk model Only_ChatGPT (self-supervised)\n",
    "def create_self_supervised_pairs(tokens, n_pairs=5000):\n",
    "    \"\"\"\n",
    "    Membuat pasangan data untuk self-supervised learning.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Token dari teks.\n",
    "        n_pairs: Jumlah pasangan yang akan dibuat.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Pasangan anchor, positive, dan label.\n",
    "    \"\"\"\n",
    "    # Jumlah data\n",
    "    n_samples = tokens['input_ids'].shape[0]\n",
    "    \n",
    "    # Inisialisasi array untuk pasangan data\n",
    "    anchor_input_ids = []\n",
    "    anchor_attention_mask = []\n",
    "    positive_input_ids = []\n",
    "    positive_attention_mask = []\n",
    "    negative_input_ids = []\n",
    "    negative_attention_mask = []\n",
    "    \n",
    "    # Membuat pasangan positif (similar texts)\n",
    "    for _ in range(n_pairs):\n",
    "        # Pilih dua indeks berbeda secara acak\n",
    "        idx1, idx2, idx3 = np.random.choice(n_samples, 3, replace=False)\n",
    "        \n",
    "        anchor_input_ids.append(tokens['input_ids'][idx1])\n",
    "        anchor_attention_mask.append(tokens['attention_mask'][idx1])\n",
    "        \n",
    "        positive_input_ids.append(tokens['input_ids'][idx2])\n",
    "        positive_attention_mask.append(tokens['attention_mask'][idx2])\n",
    "        \n",
    "        negative_input_ids.append(tokens['input_ids'][idx3])\n",
    "        negative_attention_mask.append(tokens['attention_mask'][idx3])\n",
    "    \n",
    "    # Konversi ke tensor\n",
    "    return {\n",
    "        'anchor': {\n",
    "            'input_ids': tf.convert_to_tensor(anchor_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(anchor_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'positive': {\n",
    "            'input_ids': tf.convert_to_tensor(positive_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(positive_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': tf.convert_to_tensor(negative_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(negative_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'labels': tf.ones(n_pairs, dtype=tf.float32)  # Dummy labels\n",
    "    }\n",
    "\n",
    "# Buat pasangan data untuk model Only_ChatGPT\n",
    "only_chatgpt_pairs = create_self_supervised_pairs(gpt_tokens_2, n_pairs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model triplet untuk Only_ChatGPT\n",
    "triplet_model_only_chatgpt = build_triplet_model(bi_encoder_only_chatgpt)\n",
    "\n",
    "# Compile model\n",
    "triplet_model_only_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=triplet_loss\n",
    ")\n",
    "\n",
    "# Training model Only_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Only_ChatGPT...\")\n",
    "history_only_chatgpt = triplet_model_only_chatgpt.fit(\n",
    "    x=[\n",
    "        only_chatgpt_pairs['anchor']['input_ids'],\n",
    "        only_chatgpt_pairs['anchor']['attention_mask'],\n",
    "        only_chatgpt_pairs['positive']['input_ids'],\n",
    "        only_chatgpt_pairs['positive']['attention_mask'],\n",
    "        only_chatgpt_pairs['negative']['input_ids'],\n",
    "        only_chatgpt_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    y=only_chatgpt_pairs['labels'],  # Tidak digunakan dalam triplet loss\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history untuk model Student_ChatGPT\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_student_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_student_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Student_ChatGPT: Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training history untuk model Only_ChatGPT\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_only_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_only_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Only_ChatGPT: Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings untuk kalimat Student, ChatGPT_1 dan ChatGPT_2\n",
    "\n",
    "# Fungsi untuk menghitung similarity score\n",
    "\n",
    "# Hitung similarity scores X-Student (Model Student_ChatGPT), X-ChatGPT1 (Model Student_ChatGPT), X-ChatGPT2 (Model Only_ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghasilkan embeddings\n",
    "def generate_embeddings(tokens, model):\n",
    "    \"\"\"\n",
    "    Menghasilkan embeddings untuk teks.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Token dari teks.\n",
    "        model: Model bi-encoder.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Embeddings.\n",
    "    \"\"\"\n",
    "    return model([tokens['input_ids'], tokens['attention_mask']])\n",
    "\n",
    "# Fungsi untuk menghitung similarity score\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Menghitung cosine similarity antara dua embeddings dengan normalisasi L2.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: Embedding pertama.\n",
    "        embedding2: Embedding kedua.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Similarity score antara 0 dan 1.\n",
    "    \"\"\"\n",
    "    # Pastikan embedding1 dan embedding2 adalah tensor\n",
    "    if not isinstance(embedding1, tf.Tensor):\n",
    "        embedding1 = tf.convert_to_tensor(embedding1, dtype=tf.float32)\n",
    "    if not isinstance(embedding2, tf.Tensor):\n",
    "        embedding2 = tf.convert_to_tensor(embedding2, dtype=tf.float32)\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"Embedding1 shape: {embedding1.shape}\")\n",
    "    print(f\"Embedding2 shape: {embedding2.shape}\")\n",
    "    \n",
    "    # Normalisasi embeddings (L2 norm)\n",
    "    embedding1_norm = tf.nn.l2_normalize(embedding1, axis=-1)\n",
    "    embedding2_norm = tf.nn.l2_normalize(embedding2, axis=-1)\n",
    "    \n",
    "    # Reshape embedding1 jika perlu\n",
    "    if len(embedding1_norm.shape) == 2 and embedding1_norm.shape[0] == 1:\n",
    "        # Jika single embedding dalam bentuk batch (1, dim)\n",
    "        similarities = tf.reduce_sum(embedding1_norm * embedding2_norm, axis=-1)\n",
    "    else:\n",
    "        # Jika multiple embeddings\n",
    "        similarities = tf.matmul(embedding1_norm, tf.transpose(embedding2_norm))\n",
    "        similarities = tf.reshape(similarities, [-1])\n",
    "    \n",
    "    # Convert to numpy and ensure values are between 0 and 1\n",
    "    similarities_np = similarities.numpy()\n",
    "    # Rescale dari [-1,1] ke [0,1]\n",
    "    similarities_np = (similarities_np + 1) / 2\n",
    "    \n",
    "    print(f\"Similarity values: {similarities_np}\")\n",
    "    print(f\"Mean similarity: {np.mean(similarities_np)}\")\n",
    "    \n",
    "    return similarities_np\n",
    "\n",
    "# Generate embeddings untuk semua data\n",
    "print(\"Generating embeddings for Student...\")\n",
    "student_embeddings_1 = generate_embeddings(std_tokens, bi_encoder_student_chatgpt)\n",
    "print(\"Generating embeddings for ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_embeddings_1 = generate_embeddings(gpt_tokens_1, bi_encoder_student_chatgpt)\n",
    "print(\"Generating embeddings for ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_embeddings_2 = generate_embeddings(gpt_tokens_2, bi_encoder_only_chatgpt)\n",
    "\n",
    "# Hitung similarity scores\n",
    "# 1. Similarity dengan Student (dari model Student_ChatGPT)\n",
    "student_similarity_scores = []\n",
    "for emb in student_embeddings_1:\n",
    "    # Ambil rata-rata similarity dengan 70% student embedding acak\n",
    "    sample_size = int(len(student_embeddings_1) * 0.7)  # 70% dari total data\n",
    "    random_indices = np.random.choice(len(student_embeddings_1), sample_size, replace=False)\n",
    "    random_embeddings = tf.gather(student_embeddings_1, random_indices)\n",
    "    similarities = compute_similarity(tf.expand_dims(emb, 0), random_embeddings)\n",
    "    student_similarity_scores.append(np.mean(similarities))\n",
    "\n",
    "# 2. Similarity dengan ChatGPT (dari model Student_ChatGPT)\n",
    "chatgpt1_similarity_scores = []\n",
    "for emb in chatgpt_embeddings_1:\n",
    "    # Ambil rata-rata similarity dengan 70% chatgpt embedding acak\n",
    "    sample_size = int(len(chatgpt_embeddings_1) * 0.7)  # 70% dari total data\n",
    "    random_indices = np.random.choice(len(chatgpt_embeddings_1), sample_size, replace=False)\n",
    "    random_embeddings = tf.gather(chatgpt_embeddings_1, random_indices)\n",
    "    similarities = compute_similarity(tf.expand_dims(emb, 0), random_embeddings)\n",
    "    chatgpt1_similarity_scores.append(np.mean(similarities))\n",
    "\n",
    "# 3. Similarity dengan ChatGPT Knowledge (dari model Only_ChatGPT)\n",
    "chatgpt2_similarity_scores = []\n",
    "for emb in chatgpt_embeddings_2:\n",
    "    # Ambil rata-rata similarity dengan 70% chatgpt knowledge embedding acak\n",
    "    sample_size = int(len(chatgpt_embeddings_2) * 0.7)  # 70% dari total data\n",
    "    random_indices = np.random.choice(len(chatgpt_embeddings_2), sample_size, replace=False)\n",
    "    random_embeddings = tf.gather(chatgpt_embeddings_2, random_indices)\n",
    "    similarities = compute_similarity(tf.expand_dims(emb, 0), random_embeddings)\n",
    "    chatgpt2_similarity_scores.append(np.mean(similarities))\n",
    "\n",
    "# Konversi ke numpy arrays\n",
    "student_similarity_scores = np.array(student_similarity_scores)\n",
    "chatgpt1_similarity_scores = np.array(chatgpt1_similarity_scores)\n",
    "chatgpt2_similarity_scores = np.array(chatgpt2_similarity_scores)\n",
    "\n",
    "reference_embeddings = {\n",
    "    'student': {\n",
    "        'embeddings': student_embeddings_1.numpy(),\n",
    "        'similarity_scores': student_similarity_scores\n",
    "    },\n",
    "    'chatgpt': {\n",
    "        'embeddings': chatgpt_embeddings_1.numpy(),\n",
    "        'similarity_scores': chatgpt1_similarity_scores\n",
    "    },\n",
    "    'chatgpt_knowledge': {\n",
    "        'embeddings': chatgpt_embeddings_2.numpy(),\n",
    "        'similarity_scores': chatgpt2_similarity_scores\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model Bi-Encoder untuk student_chatgpt\n",
    "\n",
    "# Simpan model Bi-Encoder untuk only_chatgpt\n",
    "\n",
    "# Simpan tokenizer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model bi-encoder untuk Student_ChatGPT\n",
    "bi_encoder_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model bi-encoder untuk Only_ChatGPT\n",
    "bi_encoder_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Save tokenizer configuration\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "with open('saved_models/reference_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(reference_embeddings, f)\n",
    "\n",
    "print(\"Reference embeddings saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pada bagian klasifikasi, dataset dari examples dan knowledge digabung, lalu tiap data menghasilkan 2 embeddings dari model, 2 similarity score(1. lebih mirip Student atau ChatGPT, 2. Seberapa mirip dengan Knowledge/Pengetahuan ChatGPT), dan fitur stylometric \n",
    "#Dataset dibagi menjadi data training 70%, Validation 20%, Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk ekstraksi fitur stylometric\n",
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur stylometric dari teks:\n",
    "    - Panjang kata rata-rata\n",
    "    - Rasio kata unik\n",
    "    - Rasio tanda baca\n",
    "    - Panjang kalimat\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input teks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Fitur stylometric.\n",
    "    \"\"\"\n",
    "    # Basic features\n",
    "    n_chars = len(text)\n",
    "    n_words = len(text.split())\n",
    "    avg_word_length = n_chars / n_words if n_words > 0 else 0\n",
    "    unique_word_ratio = len(set(text.split())) / n_words if n_words > 0 else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    punctuation_ratio = len(re.findall(r'[.,!?;:]', text)) / n_chars if n_chars > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'sentence_length': n_words\n",
    "    }\n",
    "\n",
    "# Ekstraksi fitur untuk semua dataset\n",
    "print(\"Ekstraksi fitur stylometric untuk Student...\")\n",
    "student_features = [extract_stylometric_features(text) for text in std_par]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_features_1 = [extract_stylometric_features(text) for text in gpt_par_1]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_features_2 = [extract_stylometric_features(text) for text in gpt_par_2]\n",
    "\n",
    "# Konversi fitur ke DataFrame\n",
    "student_features_df = pd.DataFrame(student_features)\n",
    "chatgpt_features_1_df = pd.DataFrame(chatgpt_features_1)\n",
    "chatgpt_features_2_df = pd.DataFrame(chatgpt_features_2)\n",
    "\n",
    "# Tampilkan beberapa fitur hasil ekstraksi\n",
    "print(\"\\nFitur Stylometric Student:\")\n",
    "display(student_features_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Student_ChatGPT):\")\n",
    "display(chatgpt_features_1_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Only_ChatGPT):\")\n",
    "display(chatgpt_features_2_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua fitur untuk normalisasi\n",
    "all_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)\n",
    "\n",
    "# Normalisasi fitur menggunakan StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(all_features)\n",
    "\n",
    "# Pisahkan kembali fitur yang telah dinormalisasi\n",
    "n_student = len(student_features_df)\n",
    "n_chatgpt_1 = len(chatgpt_features_1_df)\n",
    "\n",
    "student_features_normalized = normalized_features[:n_student]\n",
    "chatgpt_features_1_normalized = normalized_features[n_student:n_student + n_chatgpt_1]\n",
    "chatgpt_features_2_normalized = normalized_features[n_student + n_chatgpt_1:]\n",
    "\n",
    "print(\"Fitur Student setelah normalisasi:\")\n",
    "print(student_features_normalized[:5])\n",
    "\n",
    "# Simpan scaler untuk inference nanti\n",
    "with open('scaler_stylometric.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahkan label pada dataset untuk visualisasi\n",
    "student_features_df['label'] = 'Esai Siswa'\n",
    "chatgpt_features_1_df['label'] = 'Esai ChatGPT'\n",
    "chatgpt_features_2_df['label'] = 'Pengetahuan ChatGPT'\n",
    "\n",
    "# Gabungkan dataset\n",
    "combined_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(['avg_word_length', 'unique_word_ratio', 'punctuation_ratio', 'sentence_length']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(x='label', y=feature, data=combined_features)\n",
    "    plt.title(f'Distribusi {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.violinplot(x='label', y='avg_word_length', data=combined_features, inner='box')\n",
    "plt.title('Distribusi Panjang Kata Rata-Rata')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(combined_features, hue='label', vars=['avg_word_length', 'unique_word_ratio', 'punctuation_ratio', 'sentence_length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persiapkan similarity scores untuk input model\n",
    "# Gabungkan similarity scores dari ketiga model\n",
    "student_similarity_combined = np.column_stack([\n",
    "    student_similarity_scores,\n",
    "    np.zeros_like(student_similarity_scores),  # Placeholder untuk ChatGPT1\n",
    "    np.zeros_like(student_similarity_scores)   # Placeholder untuk ChatGPT2\n",
    "])\n",
    "\n",
    "chatgpt1_similarity_combined = np.column_stack([\n",
    "    np.zeros_like(chatgpt1_similarity_scores),  # Placeholder untuk Student\n",
    "    chatgpt1_similarity_scores,\n",
    "    chatgpt2_similarity_scores[:len(chatgpt1_similarity_scores)]  # Ambil sebanyak data ChatGPT1\n",
    "])\n",
    "\n",
    "# Input layers untuk tiga jenis fitur\n",
    "bert_embedding_input = tf.keras.layers.Input(\n",
    "    shape=(128,),  # Shape embeddings (diambil dari output model bi-encoder)\n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding\"\n",
    ")\n",
    "stylometric_input = tf.keras.layers.Input(\n",
    "    shape=(4,),  # Shape jumlah fitur stylometric\n",
    "    dtype=tf.float32, \n",
    "    name=\"stylometric_features\"\n",
    ")\n",
    "similarity_score_input = tf.keras.layers.Input(\n",
    "    shape=(3,),  # Shape score similarity (Student, ChatGPT1, ChatGPT2)\n",
    "    dtype=tf.float32, \n",
    "    name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "# Dense layer untuk masing-masing input\n",
    "bert_dense = tf.keras.layers.Dense(64, activation=\"relu\")(bert_embedding_input)\n",
    "style_dense = tf.keras.layers.Dense(16, activation=\"relu\")(stylometric_input)\n",
    "sim_dense = tf.keras.layers.Dense(8, activation=\"relu\")(similarity_score_input)\n",
    "\n",
    "# Gabungkan semua fitur\n",
    "combined = tf.keras.layers.Concatenate()([bert_dense, style_dense, sim_dense])\n",
    "\n",
    "# Dense layers untuk klasifikasi\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(combined)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Buat model classifier\n",
    "classifier = tf.keras.Model(\n",
    "    inputs=[bert_embedding_input, stylometric_input, similarity_score_input],\n",
    "    outputs=output,\n",
    "    name=\"text_classifier\"\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "# Tampilkan arsitektur model\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persiapkan embeddings untuk input model\n",
    "# Ambil embeddings dari model bi-encoder\n",
    "student_embeddings_reduced = student_embeddings_1.numpy()\n",
    "chatgpt1_embeddings_reduced = chatgpt_embeddings_1.numpy()\n",
    "\n",
    "# Label untuk data\n",
    "student_labels = np.zeros(len(student_embeddings_reduced))  # Label 0 untuk Student\n",
    "chatgpt_labels = np.ones(len(chatgpt1_embeddings_reduced))  # Label 1 untuk ChatGPT\n",
    "\n",
    "# Combine embedding data\n",
    "all_embeddings = np.vstack([student_embeddings_reduced, chatgpt1_embeddings_reduced])\n",
    "\n",
    "# Combine stylometric features\n",
    "all_stylometric_features = np.vstack([student_features_normalized, chatgpt_features_1_normalized])\n",
    "\n",
    "# Combine similarity scores\n",
    "all_similarity_scores = np.vstack([student_similarity_combined, chatgpt1_similarity_combined])\n",
    "\n",
    "# Combine labels\n",
    "all_labels = np.hstack([student_labels, chatgpt_labels])\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(all_labels)),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.33,  # 10% of total (0.33 * 0.3 = 0.1)\n",
    "    random_state=42,\n",
    "    stratify=all_labels[temp_idx]\n",
    ")\n",
    "\n",
    "# Prepare inputs for training, validation, and test\n",
    "train_inputs = {\n",
    "    \"bert_embedding\": all_embeddings[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    \"bert_embedding\": all_embeddings[val_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[val_idx],\n",
    "    \"similarity_score\": all_similarity_scores[val_idx]\n",
    "}\n",
    "\n",
    "test_inputs = {\n",
    "    \"bert_embedding\": all_embeddings[test_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[test_idx],\n",
    "    \"similarity_score\": all_similarity_scores[test_idx]\n",
    "}\n",
    "\n",
    "train_labels = all_labels[train_idx]\n",
    "val_labels = all_labels[val_idx]\n",
    "test_labels = all_labels[test_idx]\n",
    "\n",
    "print(f\"Training data: {len(train_labels)}\")\n",
    "print(f\"Validation data: {len(val_labels)}\")\n",
    "print(f\"Test data: {len(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifier\n",
    "print(\"Training Text Classifier...\")\n",
    "history_classifier = classifier.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_classifier.history['loss'], label='Training Loss')\n",
    "plt.plot(history_classifier.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_classifier.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_classifier.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi model pada validation set\n",
    "val_loss, val_acc, val_precision, val_recall, val_auc = classifier.evaluate(val_inputs, val_labels)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Prediksi pada validation set\n",
    "val_predictions = classifier.predict(val_inputs)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_predictions_binary, target_names=['Student', 'ChatGPT']))\n",
    "\n",
    "# Evaluasi model pada test set\n",
    "test_loss, test_acc, test_precision, test_recall, test_auc = classifier.evaluate(test_inputs, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Prediksi pada test set\n",
    "test_predictions = classifier.predict(test_inputs)\n",
    "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix untuk test set\n",
    "cm_test = confusion_matrix(test_labels, test_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix untuk test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Test Set Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report untuk test set\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions_binary, target_names=['Student', 'ChatGPT']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat direktori jika belum ada\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model bi-encoder untuk Student_ChatGPT\n",
    "bi_encoder_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model bi-encoder untuk Only_ChatGPT\n",
    "bi_encoder_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Simpan model classifier\n",
    "classifier.save('saved_models/text_classifier.h5')\n",
    "\n",
    "# Simpan konfigurasi tokenizer\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "# Simpan scaler untuk fitur stylometric\n",
    "with open(\"saved_models/scaler_stylometric.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model dan konfigurasi berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inferensi Model\n",
    "#Import semua package yang dibutuhkan\n",
    "#Load semua model yang dibutuhkan\n",
    "#Run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
