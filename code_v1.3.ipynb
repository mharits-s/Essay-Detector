{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_excel(\"example_datasets/examples-datasets-mar25.xlsx\")\n",
    "only_chatgpt = pd.read_excel(\"example_datasets/knowledge-datasets-mar25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan informasi dataset\n",
    "print(\"Dataset Student_ChatGPT:\")\n",
    "print(student_chatgpt.info())\n",
    "print(\"\\nDataset Only_ChatGPT:\")\n",
    "print(only_chatgpt.info())\n",
    "\n",
    "# Tampilkan beberapa baris awal dataset\n",
    "print(\"\\nContoh Data Student_ChatGPT:\")\n",
    "display(student_chatgpt.head())\n",
    "\n",
    "print(\"\\nContoh Data Only_ChatGPT:\")\n",
    "display(only_chatgpt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing teks:\n",
    "    - Mengubah teks menjadi huruf kecil\n",
    "    - Membersihkan whitespace berlebih\n",
    "    - Menjaga teks sebagai paragraf utuh\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks input.\n",
    "        \n",
    "    Returns:\n",
    "        str: Teks yang telah diproses atau None jika tidak valid.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Bersihkan teks\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Ganti multiple whitespace dengan satu spasi\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data Student_ChatGPT\n",
    "std_par = []\n",
    "gpt_par_1 = []  # ChatGPT dari dataset Student_ChatGPT\n",
    "\n",
    "# Proses teks Student\n",
    "for text in student_chatgpt['Pelajar']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        std_par.append(processed)\n",
    "\n",
    "# Proses teks ChatGPT (dari Student_ChatGPT)\n",
    "for text in student_chatgpt['GPT']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        gpt_par_1.append(processed)\n",
    "\n",
    "# Preprocessing data Only_ChatGPT\n",
    "gpt_par_2 = []  # ChatGPT dari dataset Only_ChatGPT\n",
    "\n",
    "# Proses teks ChatGPT (dari Only_ChatGPT)\n",
    "for text in only_chatgpt['GPT']:\n",
    "    processed = preprocess_text(text)\n",
    "    if processed:\n",
    "        gpt_par_2.append(processed)\n",
    "\n",
    "# Tampilkan jumlah data hasil preprocessing\n",
    "print(f\"Total paragraf Student: {len(std_par)}\")\n",
    "print(f\"Total paragraf ChatGPT (Student_ChatGPT): {len(gpt_par_1)}\")\n",
    "print(f\"Total paragraf ChatGPT (Only_ChatGPT): {len(gpt_par_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(texts, max_length=256):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi data\n",
    "print(\"Tokenisasi kalimat Student...\")\n",
    "std_tokens = tokenize_text(std_par)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\")\n",
    "gpt_tokens_1 = tokenize_text(gpt_par_1)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\")\n",
    "gpt_tokens_2 = tokenize_text(gpt_par_2)\n",
    "\n",
    "# Tampilkan hasil tokenisasi (contoh: Student)\n",
    "print(\"\\nContoh hasil tokenisasi:\")\n",
    "print(std_tokens['input_ids'][:1])  # Input token ID\n",
    "print(std_tokens['attention_mask'][:1])  # Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memeriksa distribusi panjang token untuk memastikan max_length cukup\n",
    "student_lengths = [sum(mask) for mask in std_tokens['attention_mask'].numpy()]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(student_lengths, bins=30)\n",
    "plt.title('Distribusi Panjang Token (Student)')\n",
    "plt.xlabel('Jumlah Token Aktif')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.show()\n",
    "print(f\"Rata-rata panjang token: {np.mean(student_lengths):.2f}\")\n",
    "print(f\"Persentase terpotong: {sum(l == 128 for l in student_lengths) / len(student_lengths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendekode token untuk memastikan tokenisasi berfungsi dengan baik\n",
    "sample_text = std_par[0]\n",
    "sample_tokens = tokenizer.encode(sample_text)\n",
    "print(f\"Teks asli: {sample_text}\")\n",
    "print(f\"Token ID: {sample_tokens}\")\n",
    "print(f\"Token dekode: {tokenizer.decode(sample_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika tokenisasi memakan waktu lama, pertimbangkan untuk menyimpannya\n",
    "tokenized_data = {\n",
    "    'student': std_tokens,\n",
    "    'chatgpt_1 ': gpt_tokens_1 ,\n",
    "    'chatgpt_2': gpt_tokens_2\n",
    "}\n",
    "\n",
    "# Menyimpan input_ids dan attention_mask sebagai numpy arrays\n",
    "tokenized_numpy = {\n",
    "    'student': {\n",
    "        'input_ids': std_tokens['input_ids'].numpy(),\n",
    "        'attention_mask': std_tokens['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt ': {\n",
    "        'input_ids': gpt_tokens_1 ['input_ids'].numpy(),\n",
    "        'attention_mask': gpt_tokens_1 ['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt_2': {\n",
    "        'input_ids': gpt_tokens_2['input_ids'].numpy(),\n",
    "        'attention_mask': gpt_tokens_2['attention_mask'].numpy()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('tokenized_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_numpy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "#Buat Model fine tuning Bi-Encoder Student_ChatGPT\n",
    "\n",
    "#Buat Model fine tuning Bi-Encoder Only_ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Freeze BERT layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Fungsi untuk membuat Bi-Encoder\n",
    "def build_bi_encoder(bert_model):\n",
    "    \"\"\"\n",
    "    Membuat model Bi-Encoder dengan IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        bert_model (TFBertModel): Model dasar IndoBERT.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Model Bi-Encoder.\n",
    "    \"\"\"\n",
    "    # Input layer untuk token ID dan attention mask\n",
    "    input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Extract CLS token embeddings dari IndoBERT\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0][:, 0, :]  # [CLS] token\n",
    "    \n",
    "    # Dense layer untuk fine-tuning\n",
    "    dense1 = tf.keras.layers.Dense(256, activation=\"relu\")(bert_output)\n",
    "    dropout1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "    dense2 = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1)\n",
    "    dropout2 = tf.keras.layers.Dropout(0.1)(dense2)\n",
    "    dense3 = tf.keras.layers.Dense(128)(dropout2)\n",
    "    \n",
    "    # Normalisasi output (L2 normalization)\n",
    "    normalized_output = tf.nn.l2_normalize(dense3, axis=1)\n",
    "    \n",
    "    # Model Bi-Encoder\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=normalized_output)\n",
    "\n",
    "# Buat dua Bi-Encoder\n",
    "bi_encoder_student_chatgpt = build_bi_encoder(bert_model)\n",
    "bi_encoder_only_chatgpt = build_bi_encoder(bert_model)\n",
    "\n",
    "# Tampilkan arsitektur\n",
    "print(\"Bi-Encoder untuk Student_ChatGPT:\")\n",
    "bi_encoder_student_chatgpt.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Datasets Student_ChatGPT (Gunakan Model Fine-Tuning IndoBERT di atas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrastive_pairs(student_tokens, chatgpt_tokens, max_pairs=None):\n",
    "    \"\"\"\n",
    "    Membuat pasangan data untuk contrastive learning dengan jumlah yang menyesuaikan dataset.\n",
    "    \n",
    "    Args:\n",
    "        student_tokens: Token dari teks student.\n",
    "        chatgpt_tokens: Token dari teks ChatGPT.\n",
    "        max_pairs: Jumlah maksimum pasangan (opsional). Jika None, akan menggunakan semua kombinasi yang mungkin.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Pasangan anchor, positive, negative, dan label serta jumlah total pasangan.\n",
    "    \"\"\"\n",
    "    # Import yang diperlukan\n",
    "    import random\n",
    "    \n",
    "    # Jumlah data\n",
    "    n_student = student_tokens['input_ids'].shape[0]\n",
    "    n_chatgpt = chatgpt_tokens['input_ids'].shape[0]\n",
    "    \n",
    "    # Hitung jumlah pasangan yang mungkin\n",
    "    max_student_pairs = (n_student * (n_student - 1)) // 2  # Kombinasi student-student\n",
    "    max_chatgpt_pairs = (n_chatgpt * (n_chatgpt - 1)) // 2  # Kombinasi chatgpt-chatgpt\n",
    "    max_negative_pairs = n_student * n_chatgpt  # Kombinasi student-chatgpt\n",
    "    \n",
    "    # Tentukan jumlah pasangan yang akan dibuat\n",
    "    if max_pairs is None:\n",
    "        # Gunakan jumlah minimum dari pasangan positif untuk keseimbangan\n",
    "        n_pos_student = min(max_student_pairs, max_chatgpt_pairs) // 2\n",
    "        n_pos_chatgpt = n_pos_student\n",
    "        # Batasi jumlah pasangan negatif agar seimbang dengan positif\n",
    "        n_neg_pairs = min(max_negative_pairs, 2 * n_pos_student)\n",
    "        # Total pasangan\n",
    "        total_pairs = 2 * n_pos_student + n_neg_pairs\n",
    "    else:\n",
    "        # Jika max_pairs ditentukan, gunakan itu dengan proporsi yang sama\n",
    "        n_pos_student = max_pairs // 4\n",
    "        n_pos_chatgpt = max_pairs // 4\n",
    "        n_neg_pairs = max_pairs // 2\n",
    "        total_pairs = max_pairs\n",
    "    \n",
    "    # Pastikan tidak melebihi jumlah maksimum yang mungkin\n",
    "    n_pos_student = min(n_pos_student, max_student_pairs)\n",
    "    n_pos_chatgpt = min(n_pos_chatgpt, max_chatgpt_pairs)\n",
    "    n_neg_pairs = min(n_neg_pairs, max_negative_pairs)\n",
    "    \n",
    "    # Inisialisasi array untuk pasangan data\n",
    "    anchor_input_ids = []\n",
    "    anchor_attention_mask = []\n",
    "    positive_input_ids = []\n",
    "    positive_attention_mask = []\n",
    "    negative_input_ids = []\n",
    "    negative_attention_mask = []\n",
    "    labels = []\n",
    "    \n",
    "    # Membuat pasangan positif (student-student)\n",
    "    if n_pos_student > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        student_pairs = [(i, j) for i in range(n_student) for j in range(i+1, n_student)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(student_pairs, n_pos_student)\n",
    "        \n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][idx1])\n",
    "            \n",
    "            positive_input_ids.append(student_tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(student_tokens['attention_mask'][idx2])\n",
    "            \n",
    "            # Negative dari ChatGPT\n",
    "            neg_idx = np.random.choice(n_chatgpt)\n",
    "            negative_input_ids.append(chatgpt_tokens['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens['attention_mask'][neg_idx])\n",
    "            \n",
    "            labels.append(1)  # 1 untuk pasangan positif\n",
    "    \n",
    "    # Membuat pasangan positif (chatgpt-chatgpt)\n",
    "    if n_pos_chatgpt > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        chatgpt_pairs = [(i, j) for i in range(n_chatgpt) for j in range(i+1, n_chatgpt)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(chatgpt_pairs, n_pos_chatgpt)\n",
    "        \n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            anchor_input_ids.append(chatgpt_tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(chatgpt_tokens['attention_mask'][idx1])\n",
    "            \n",
    "            positive_input_ids.append(chatgpt_tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(chatgpt_tokens['attention_mask'][idx2])\n",
    "            \n",
    "            # Negative dari Student\n",
    "            neg_idx = np.random.choice(n_student)\n",
    "            negative_input_ids.append(student_tokens['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(student_tokens['attention_mask'][neg_idx])\n",
    "            \n",
    "            labels.append(1)  # 1 untuk pasangan positif\n",
    "    \n",
    "    # Membuat pasangan negatif (student-chatgpt)\n",
    "    if n_neg_pairs > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        negative_pairs = [(i, j) for i in range(n_student) for j in range(n_chatgpt)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(negative_pairs, n_neg_pairs)\n",
    "        \n",
    "        for student_idx, chatgpt_idx in selected_pairs:\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][student_idx])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][student_idx])\n",
    "            \n",
    "            negative_input_ids.append(chatgpt_tokens['input_ids'][chatgpt_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens['attention_mask'][chatgpt_idx])\n",
    "            \n",
    "            # Positive dari Student (berbeda dengan anchor)\n",
    "            available_pos = [i for i in range(n_student) if i != student_idx]\n",
    "            if available_pos:  # Pastikan ada indeks yang tersedia\n",
    "                pos_idx = np.random.choice(available_pos)\n",
    "                positive_input_ids.append(student_tokens['input_ids'][pos_idx])\n",
    "                positive_attention_mask.append(student_tokens['attention_mask'][pos_idx])\n",
    "                \n",
    "                labels.append(0)  # 0 untuk pasangan negatif\n",
    "    \n",
    "    # Hitung jumlah pasangan yang sebenarnya dibuat\n",
    "    actual_pairs = len(labels)\n",
    "    \n",
    "    # Konversi ke tensor\n",
    "    return {\n",
    "        'anchor': {\n",
    "            'input_ids': tf.convert_to_tensor(anchor_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(anchor_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'positive': {\n",
    "            'input_ids': tf.convert_to_tensor(positive_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(positive_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': tf.convert_to_tensor(negative_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(negative_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'labels': tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    }, actual_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat pasangan data untuk model Student_ChatGPT tanpa menentukan n_pairs\n",
    "student_chatgpt_pairs, total_pairs = create_contrastive_pairs(std_tokens, gpt_tokens_1)\n",
    "\n",
    "# Tampilkan jumlah pasangan yang dibuat\n",
    "print(f\"Total contrastive pairs yang dibuat: {total_pairs}\")\n",
    "print(f\"- Pasangan positif student-student: {sum(1 for label in student_chatgpt_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Pasangan positif chatgpt-chatgpt: {sum(1 for label in student_chatgpt_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Pasangan negatif student-chatgpt: {sum(1 for label in student_chatgpt_pairs['labels'].numpy() if label == 0)}\")\n",
    "\n",
    "# Jika ingin membatasi jumlah maksimum pasangan\n",
    "# student_chatgpt_pairs, total_pairs = create_contrastive_pairs(student_tokens, chatgpt_tokens_1, max_pairs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi contrastive loss\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Contrastive loss untuk triplet (anchor, positive, negative).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Label (1 untuk pasangan positif, 0 untuk pasangan negatif).\n",
    "        y_pred: Jarak antara anchor-positive dan anchor-negative.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Nilai loss.\n",
    "    \"\"\"\n",
    "    margin = 0.5\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model untuk training dengan triplet loss\n",
    "def build_triplet_model(bi_encoder):\n",
    "    \"\"\"\n",
    "    Membangun model untuk training dengan triplet loss.\n",
    "    \n",
    "    Args:\n",
    "        bi_encoder: Model bi-encoder yang akan dilatih.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Model untuk training dengan triplet loss.\n",
    "    \"\"\"\n",
    "    # Input untuk anchor, positive, dan negative\n",
    "    anchor_input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"anchor_input_ids\")\n",
    "    anchor_attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"anchor_attention_mask\")\n",
    "    \n",
    "    positive_input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"positive_input_ids\")\n",
    "    positive_attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"positive_attention_mask\")\n",
    "    \n",
    "    negative_input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"negative_input_ids\")\n",
    "    negative_attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name=\"negative_attention_mask\")\n",
    "    \n",
    "    # Embedding untuk anchor, positive, dan negative\n",
    "    anchor_embedding = bi_encoder([anchor_input_ids, anchor_attention_mask])\n",
    "    positive_embedding = bi_encoder([positive_input_ids, positive_attention_mask])\n",
    "    negative_embedding = bi_encoder([negative_input_ids, negative_attention_mask])\n",
    "    \n",
    "    # Hitung cosine similarity\n",
    "    pos_similarity = tf.reduce_sum(anchor_embedding * positive_embedding, axis=1)\n",
    "    neg_similarity = tf.reduce_sum(anchor_embedding * negative_embedding, axis=1)\n",
    "    \n",
    "    # Output model adalah perbedaan similarity\n",
    "    output = tf.stack([pos_similarity, neg_similarity], axis=1)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[\n",
    "            anchor_input_ids, anchor_attention_mask,\n",
    "            positive_input_ids, positive_attention_mask,\n",
    "            negative_input_ids, negative_attention_mask\n",
    "        ],\n",
    "        outputs=output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model triplet untuk Student_ChatGPT\n",
    "triplet_model_student_chatgpt = build_triplet_model(bi_encoder_student_chatgpt)\n",
    "\n",
    "# Custom loss function untuk triplet\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Triplet loss: mendorong similarity positif lebih tinggi dari similarity negatif.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tidak digunakan dalam triplet loss.\n",
    "        y_pred: Stack dari [positive_similarity, negative_similarity].\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Nilai loss.\n",
    "    \"\"\"\n",
    "    pos_sim = y_pred[:, 0]\n",
    "    neg_sim = y_pred[:, 1]\n",
    "    margin = 0.5\n",
    "    \n",
    "    # Triplet loss: max(0, margin - (pos_sim - neg_sim))\n",
    "    loss = tf.maximum(0., margin - (pos_sim - neg_sim))\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "triplet_model_student_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=triplet_loss\n",
    ")\n",
    "\n",
    "# Training model Student_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Student_ChatGPT...\")\n",
    "history_student_chatgpt = triplet_model_student_chatgpt.fit(\n",
    "    x=[\n",
    "        student_chatgpt_pairs['anchor']['input_ids'],\n",
    "        student_chatgpt_pairs['anchor']['attention_mask'],\n",
    "        student_chatgpt_pairs['positive']['input_ids'],\n",
    "        student_chatgpt_pairs['positive']['attention_mask'],\n",
    "        student_chatgpt_pairs['negative']['input_ids'],\n",
    "        student_chatgpt_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    y=student_chatgpt_pairs['labels'],  # Tidak digunakan dalam triplet loss\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Datasets Only_ChatGPT (Self-Supervised, Gunakan Model Fine-Tuning IndoBERT di atas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_self_supervised_pairs(tokens, max_pairs=None):\n",
    "    \"\"\"\n",
    "    Membuat pasangan data untuk self-supervised learning dengan jumlah yang menyesuaikan dataset.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Token dari teks.\n",
    "        max_pairs: Jumlah maksimum pasangan (opsional). Jika None, akan menggunakan semua kombinasi yang mungkin.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Pasangan anchor, positive, negative, dan jumlah pasangan yang dibuat.\n",
    "    \"\"\"\n",
    "    # Import yang diperlukan\n",
    "    import random\n",
    "    \n",
    "    # Jumlah data\n",
    "    n_samples = tokens['input_ids'].shape[0]\n",
    "    \n",
    "    # Hitung jumlah maksimum pasangan yang mungkin\n",
    "    # Untuk setiap anchor, kita perlu 2 sampel lain (positive dan negative)\n",
    "    # Jadi kita perlu minimal 3 sampel berbeda\n",
    "    if n_samples < 3:\n",
    "        raise ValueError(\"Dataset harus memiliki minimal 3 sampel untuk membuat pasangan triplet\")\n",
    "    \n",
    "    # Jumlah maksimum kombinasi triplet (anchor, positive, negative) yang mungkin\n",
    "    # Ini adalah kombinasi n_samples pilih 3\n",
    "    max_possible_triplets = (n_samples * (n_samples - 1) * (n_samples - 2)) // 6\n",
    "    \n",
    "    # Tentukan jumlah pasangan yang akan dibuat\n",
    "    if max_pairs is None:\n",
    "        # Gunakan semua kombinasi yang mungkin, tetapi batasi untuk menghindari memori yang terlalu besar\n",
    "        n_pairs = min(max_possible_triplets, 1800)  # Batasi maksimum 10000 pasangan\n",
    "    else:\n",
    "        # Jika max_pairs ditentukan, gunakan itu\n",
    "        n_pairs = min(max_pairs, max_possible_triplets)\n",
    "    \n",
    "    # Inisialisasi array untuk pasangan data\n",
    "    anchor_input_ids = []\n",
    "    anchor_attention_mask = []\n",
    "    positive_input_ids = []\n",
    "    positive_attention_mask = []\n",
    "    negative_input_ids = []\n",
    "    negative_attention_mask = []\n",
    "    \n",
    "    # Jika jumlah sampel cukup besar, kita bisa mengambil sampel acak\n",
    "    if n_pairs < max_possible_triplets // 2:\n",
    "        # Membuat pasangan dengan sampling acak\n",
    "        for _ in range(n_pairs):\n",
    "            # Pilih tiga indeks berbeda secara acak\n",
    "            idx1, idx2, idx3 = np.random.choice(n_samples, 3, replace=False)\n",
    "            \n",
    "            anchor_input_ids.append(tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(tokens['attention_mask'][idx1])\n",
    "            \n",
    "            positive_input_ids.append(tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(tokens['attention_mask'][idx2])\n",
    "            \n",
    "            negative_input_ids.append(tokens['input_ids'][idx3])\n",
    "            negative_attention_mask.append(tokens['attention_mask'][idx3])\n",
    "    else:\n",
    "        # Jika kita ingin menggunakan sebagian besar kombinasi yang mungkin,\n",
    "        # lebih efisien untuk membuat semua kombinasi dan mengambil sampel\n",
    "        all_triplets = [(i, j, k) \n",
    "                        for i in range(n_samples) \n",
    "                        for j in range(n_samples) \n",
    "                        for k in range(n_samples) \n",
    "                        if i != j and i != k and j != k]\n",
    "        \n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_triplets = random.sample(all_triplets, n_pairs)\n",
    "        \n",
    "        for idx1, idx2, idx3 in selected_triplets:\n",
    "            anchor_input_ids.append(tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(tokens['attention_mask'][idx1])\n",
    "            \n",
    "            positive_input_ids.append(tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(tokens['attention_mask'][idx2])\n",
    "            \n",
    "            negative_input_ids.append(tokens['input_ids'][idx3])\n",
    "            negative_attention_mask.append(tokens['attention_mask'][idx3])\n",
    "    \n",
    "    # Konversi ke tensor\n",
    "    return {\n",
    "        'anchor': {\n",
    "            'input_ids': tf.convert_to_tensor(anchor_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(anchor_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'positive': {\n",
    "            'input_ids': tf.convert_to_tensor(positive_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(positive_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': tf.convert_to_tensor(negative_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(negative_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'labels': tf.ones(n_pairs, dtype=tf.float32)  # Dummy labels\n",
    "    }, n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat pasangan data untuk model Only_ChatGPT tanpa menentukan n_pairs\n",
    "only_chatgpt_pairs, total_pairs = create_self_supervised_pairs(gpt_tokens_2)\n",
    "\n",
    "# Tampilkan jumlah pasangan yang dibuat\n",
    "print(f\"Total self-supervised pairs yang dibuat: {total_pairs}\")\n",
    "\n",
    "# Jika ingin membatasi jumlah maksimum pasangan\n",
    "# only_chatgpt_pairs, total_pairs = create_self_supervised_pairs(chatgpt_tokens_2, max_pairs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model triplet untuk Only_ChatGPT\n",
    "triplet_model_only_chatgpt = build_triplet_model(bi_encoder_only_chatgpt)\n",
    "\n",
    "# Compile model\n",
    "triplet_model_only_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=triplet_loss\n",
    ")\n",
    "\n",
    "# Training model Only_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Only_ChatGPT...\")\n",
    "history_only_chatgpt = triplet_model_only_chatgpt.fit(\n",
    "    x=[\n",
    "        only_chatgpt_pairs['anchor']['input_ids'],\n",
    "        only_chatgpt_pairs['anchor']['attention_mask'],\n",
    "        only_chatgpt_pairs['positive']['input_ids'],\n",
    "        only_chatgpt_pairs['positive']['attention_mask'],\n",
    "        only_chatgpt_pairs['negative']['input_ids'],\n",
    "        only_chatgpt_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    y=only_chatgpt_pairs['labels'],  # Tidak digunakan dalam triplet loss\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history untuk model Student_ChatGPT\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_student_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_student_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Student_ChatGPT: Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training history untuk model Only_ChatGPT\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_only_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_only_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Only_ChatGPT: Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings untuk kalimat Student, ChatGPT_1 dan ChatGPT_2\n",
    "\n",
    "# Fungsi untuk menghitung similarity score\n",
    "\n",
    "# Hitung similarity scores X-Student (Model Student_ChatGPT), X-ChatGPT1 (Model Student_ChatGPT), X-ChatGPT2 (Model Only_ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# # 1. Muat tokenizer yang sudah disimpan\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"saved_models_v2/tokenizer\")\n",
    "\n",
    "# # 2. Definisikan custom_objects untuk memuat model dengan TFBertModel\n",
    "# custom_objects = {'TFBertModel': TFBertModel}\n",
    "\n",
    "# # Gunakan custom_object_scope saat memuat model\n",
    "# with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "#     # Muat model bi-encoder yang sudah dilatih\n",
    "#     bi_encoder_student_chatgpt = tf.keras.models.load_model('saved_models_v2/bi_encoder_student_chatgpt.h5')\n",
    "#     bi_encoder_only_chatgpt = tf.keras.models.load_model('saved_models_v2/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# # 3. Muat data token yang sudah diproses (jika sudah disimpan sebelumnya)\n",
    "# try:\n",
    "#     with open('tokenized_data.pkl', 'rb') as f:\n",
    "#         tokenized_numpy = pickle.load(f)\n",
    "        \n",
    "#     # Konversi kembali ke format TensorFlow\n",
    "#     std_tokens = {\n",
    "#         'input_ids': tf.convert_to_tensor(tokenized_numpy['student']['input_ids'], dtype=tf.int32),\n",
    "#         'attention_mask': tf.convert_to_tensor(tokenized_numpy['student']['attention_mask'], dtype=tf.int32)\n",
    "#     }\n",
    "    \n",
    "#     gpt_tokens_1 = {\n",
    "#         'input_ids': tf.convert_to_tensor(tokenized_numpy['chatgpt ']['input_ids'], dtype=tf.int32),\n",
    "#         'attention_mask': tf.convert_to_tensor(tokenized_numpy['chatgpt ']['attention_mask'], dtype=tf.int32)\n",
    "#     }\n",
    "    \n",
    "#     gpt_tokens_2 = {\n",
    "#         'input_ids': tf.convert_to_tensor(tokenized_numpy['chatgpt_2']['input_ids'], dtype=tf.int32),\n",
    "#         'attention_mask': tf.convert_to_tensor(tokenized_numpy['chatgpt_2']['attention_mask'], dtype=tf.int32)\n",
    "#     }\n",
    "    \n",
    "#     print(\"Data token berhasil dimuat dari file!\")\n",
    "    \n",
    "# except (FileNotFoundError, KeyError) as e:\n",
    "#     print(f\"Error saat memuat data token: {e}\")\n",
    "#     print(\"Perlu memproses data lagi...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghasilkan embeddings\n",
    "def generate_embeddings(tokens, model):\n",
    "    \"\"\"\n",
    "    Menghasilkan embeddings untuk teks.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Token dari teks.\n",
    "        model: Model bi-encoder.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Embeddings.\n",
    "    \"\"\"\n",
    "    return model([tokens['input_ids'], tokens['attention_mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung similarity score dengan agregasi maksimum\n",
    "def compute_similarity_max(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Menghitung similarity score dengan metode agregasi maksimum.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: Embedding pertama (teks input).\n",
    "        embedding2: Embedding kedua (referensi).\n",
    "        \n",
    "    Returns:\n",
    "        float: Similarity score maksimum.\n",
    "    \"\"\"\n",
    "    # Normalisasi embeddings (L2 norm)\n",
    "    embedding1_norm = tf.nn.l2_normalize(embedding1, axis=-1)\n",
    "    embedding2_norm = tf.nn.l2_normalize(embedding2, axis=-1)\n",
    "    \n",
    "    # Hitung cosine similarity\n",
    "    similarities = tf.matmul(embedding1_norm, tf.transpose(embedding2_norm))\n",
    "    similarities = tf.reshape(similarities, [-1])  # Flatten\n",
    "    \n",
    "    # Ambil nilai maksimum\n",
    "    max_similarity = tf.reduce_max(similarities).numpy()\n",
    "    return max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model bi-encoder untuk Student_ChatGPT\n",
    "bi_encoder_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model bi-encoder untuk Only_ChatGPT\n",
    "bi_encoder_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Save tokenizer configuration\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'TFBertModel': TFBertModel}\n",
    "\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    # Muat model bi-encoder yang sudah dilatih\n",
    "    bi_encoder_student_chatgpt = tf.keras.models.load_model('saved_models_v2/bi_encoder_student_chatgpt.h5')\n",
    "    bi_encoder_only_chatgpt = tf.keras.models.load_model('saved_models_v2/bi_encoder_only_chatgpt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('saved_models_v2/tokenizer')\n",
    "\n",
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(texts, max_length=256):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi data\n",
    "print(\"Tokenisasi kalimat Student...\")\n",
    "std_tokens = tokenize_text(std_par)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\")\n",
    "gpt_tokens_1 = tokenize_text(gpt_par_1)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\")\n",
    "gpt_tokens_2 = tokenize_text(gpt_par_2)\n",
    "\n",
    "# Tampilkan hasil tokenisasi (contoh: Student)\n",
    "print(\"\\nContoh hasil tokenisasi:\")\n",
    "print(std_tokens['input_ids'][:1])  # Input token ID\n",
    "print(std_tokens['attention_mask'][:1])  # Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendekode token untuk memastikan tokenisasi berfungsi dengan baik\n",
    "sample_text = std_par[0]\n",
    "sample_tokens = tokenizer.encode(sample_text)\n",
    "print(f\"Teks asli: {sample_text}\")\n",
    "print(f\"Token ID: {sample_tokens}\")\n",
    "print(f\"Token dekode: {tokenizer.decode(sample_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings untuk semua data\n",
    "print(\"Generating embeddings for Student...\")\n",
    "student_embeddings_1 = generate_embeddings(std_tokens, bi_encoder_student_chatgpt)\n",
    "print(\"Generating embeddings for ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_embeddings_1 = generate_embeddings(gpt_tokens_1, bi_encoder_student_chatgpt)\n",
    "print(\"Generating embeddings for ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_embeddings_2 = generate_embeddings(gpt_tokens_2, bi_encoder_only_chatgpt)\n",
    "\n",
    "# Hitung similarity scores\n",
    "# 1. Similarity dengan Student (dari model Student_ChatGPT)\n",
    "student_similarity_scores = []\n",
    "for emb in student_embeddings_1:\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), student_embeddings_1)\n",
    "    student_similarity_scores.append(max_similarity)\n",
    "\n",
    "# 2. Similarity dengan ChatGPT (dari model Student_ChatGPT)\n",
    "chatgpt1_similarity_scores = []\n",
    "for emb in student_embeddings_1:\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_1)\n",
    "    chatgpt1_similarity_scores.append(max_similarity)\n",
    "\n",
    "# 3. Similarity dengan ChatGPT Knowledge (dari model Only_ChatGPT)\n",
    "chatgpt2_similarity_scores = []\n",
    "# Tentukan jumlah yang diinginkan (sama dengan jumlah student_embeddings_1)\n",
    "desired_count = len(student_embeddings_1)  # 43\n",
    "# Pilih indeks secara acak dari chatgpt_embeddings_2\n",
    "np.random.seed(42)  # Untuk reproduksibilitas hasil\n",
    "random_indices = np.random.choice(len(chatgpt_embeddings_2), size=desired_count, replace=False)\n",
    "# Hitung similarity untuk embedding yang dipilih secara acak\n",
    "for idx in random_indices:\n",
    "    emb = chatgpt_embeddings_2[idx]  # Ambil embedding acak dari chatgpt_embeddings_2\n",
    "    # Hitung similarity dengan semua chatgpt_embeddings_2\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_2)\n",
    "    chatgpt2_similarity_scores.append(max_similarity)\n",
    "\n",
    "# Konversi ke numpy arrays\n",
    "student_similarity_scores = np.array(student_similarity_scores)\n",
    "chatgpt1_similarity_scores = np.array(chatgpt1_similarity_scores)\n",
    "chatgpt2_similarity_scores = np.array(chatgpt2_similarity_scores)\n",
    "\n",
    "# Simpan similarity scores untuk digunakan dalam klasifikasi\n",
    "reference_embeddings_1 = {\n",
    "    'student': {\n",
    "        'embeddings': student_embeddings_1.numpy(),\n",
    "        'similarity_scores': student_similarity_scores\n",
    "    },\n",
    "    'chatgpt': {\n",
    "        'embeddings': chatgpt_embeddings_1.numpy(),\n",
    "        'similarity_scores': chatgpt1_similarity_scores\n",
    "    }\n",
    "}\n",
    "\n",
    "reference_embeddings_2 = {\n",
    "    'chatgpt_knowledge': {\n",
    "        'embeddings': chatgpt_embeddings_2.numpy(),\n",
    "        'similarity_scores': chatgpt2_similarity_scores\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Jumlah chatgpt2_similarity_scores: {len(chatgpt2_similarity_scores)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model Bi-Encoder untuk student_chatgpt\n",
    "\n",
    "# Simpan model Bi-Encoder untuk only_chatgpt\n",
    "\n",
    "# Simpan tokenizer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_models/reference_embeddings_1.pkl', 'wb') as f:\n",
    "    pickle.dump(reference_embeddings_1, f)\n",
    "\n",
    "with open('saved_models/reference_embeddings_2.pkl', 'wb') as f:\n",
    "    pickle.dump(reference_embeddings_2, f)\n",
    "\n",
    "print(\"Reference embeddings saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pada bagian klasifikasi, dataset dari examples dan knowledge digabung, lalu tiap data menghasilkan 2 embeddings dari model, 2 similarity score(1. lebih mirip Student atau ChatGPT, 2. Seberapa mirip dengan Knowledge/Pengetahuan ChatGPT), dan fitur stylometric \n",
    "#Dataset dibagi menjadi data training 70%, Validation 20%, Test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk ekstraksi fitur stylometric\n",
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur stylometric dari teks:\n",
    "    - Panjang kata rata-rata\n",
    "    - Rasio kata unik\n",
    "    - Rasio tanda baca\n",
    "    - Panjang kalimat\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input teks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Fitur stylometric.\n",
    "    \"\"\"\n",
    "    # Basic features\n",
    "    n_chars = len(text)\n",
    "    n_words = len(text.split())\n",
    "    avg_word_length = n_chars / n_words if n_words > 0 else 0\n",
    "    unique_word_ratio = len(set(text.split())) / n_words if n_words > 0 else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    punctuation_ratio = len(re.findall(r'[.,!?;:]', text)) / n_chars if n_chars > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'sentence_length': n_words\n",
    "    }\n",
    "\n",
    "# Ekstraksi fitur untuk semua dataset\n",
    "print(\"Ekstraksi fitur stylometric untuk Student...\")\n",
    "student_features = [extract_stylometric_features(text) for text in std_par]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_features_1 = [extract_stylometric_features(text) for text in gpt_par_1]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_features_2 = [extract_stylometric_features(text) for text in gpt_par_2]\n",
    "\n",
    "# Konversi fitur ke DataFrame\n",
    "student_features_df = pd.DataFrame(student_features)\n",
    "chatgpt_features_1_df = pd.DataFrame(chatgpt_features_1)\n",
    "chatgpt_features_2_df = pd.DataFrame(chatgpt_features_2)\n",
    "\n",
    "# Tampilkan beberapa fitur hasil ekstraksi\n",
    "print(\"\\nFitur Stylometric Student:\")\n",
    "display(student_features_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Student_ChatGPT):\")\n",
    "display(chatgpt_features_1_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Only_ChatGPT):\")\n",
    "display(chatgpt_features_2_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua fitur untuk normalisasi\n",
    "all_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)\n",
    "\n",
    "# Normalisasi fitur menggunakan StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(all_features)\n",
    "\n",
    "# Pisahkan kembali fitur yang telah dinormalisasi\n",
    "n_student = len(student_features_df)\n",
    "n_chatgpt_1 = len(chatgpt_features_1_df)\n",
    "\n",
    "student_features_normalized = normalized_features[:n_student]\n",
    "chatgpt_features_1_normalized = normalized_features[n_student:n_student + n_chatgpt_1]\n",
    "chatgpt_features_2_normalized = normalized_features[n_student + n_chatgpt_1:]\n",
    "\n",
    "print(\"Fitur Student setelah normalisasi:\")\n",
    "print(student_features_normalized[:5])\n",
    "\n",
    "# Simpan scaler untuk inference nanti\n",
    "with open('scaler_stylometric.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahkan label pada dataset untuk visualisasi\n",
    "student_features_df['label'] = 'Esai Siswa'\n",
    "chatgpt_features_1_df['label'] = 'Esai ChatGPT'\n",
    "chatgpt_features_2_df['label'] = 'Pengetahuan ChatGPT'\n",
    "\n",
    "# Gabungkan dataset\n",
    "combined_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(['avg_word_length', 'unique_word_ratio', 'punctuation_ratio', 'sentence_length']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(x='label', y=feature, data=combined_features)\n",
    "    plt.title(f'Distribusi {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.violinplot(x='label', y='avg_word_length', data=combined_features, inner='box')\n",
    "plt.title('Distribusi Panjang Kata Rata-Rata')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(combined_features, hue='label', vars=['avg_word_length', 'unique_word_ratio', 'punctuation_ratio', 'sentence_length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persiapkan similarity scores untuk input model\n",
    "# Gabungkan similarity scores dari ketiga model\n",
    "student_similarity_combined = np.column_stack([\n",
    "    student_similarity_scores,\n",
    "    np.zeros_like(student_similarity_scores),  # Placeholder untuk ChatGPT1\n",
    "])\n",
    "\n",
    "chatgpt1_similarity_combined = np.column_stack([\n",
    "    np.zeros_like(chatgpt1_similarity_scores),  # Placeholder untuk Student\n",
    "    chatgpt1_similarity_scores,\n",
    "])\n",
    "\n",
    "chatgpt2_similarity_combined = np.column_stack([\n",
    "    np.zeros_like(chatgpt2_similarity_scores),  # Placeholder untuk Student/ChatGPT\n",
    "    chatgpt2_similarity_scores[:len(chatgpt1_similarity_scores)]  # Ambil sebanyak data ChatGPT1\n",
    "])\n",
    "\n",
    "# Input layers untuk tiga jenis fitur\n",
    "bert_embedding_input_1 = tf.keras.layers.Input(\n",
    "    shape=(128,),  # Shape embeddings (diambil dari output model bi-encoder Student_ChatGPT)\n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding_1\"\n",
    ")\n",
    "bert_embedding_input_2 = tf.keras.layers.Input(\n",
    "    shape=(128,),  # Shape embeddings (diambil dari output model bi-encoder Only_ChatGPT)\n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding_2\"\n",
    ")\n",
    "stylometric_input = tf.keras.layers.Input(\n",
    "    shape=(4,),  # Shape jumlah fitur stylometric\n",
    "    dtype=tf.float32, \n",
    "    name=\"stylometric_features\"\n",
    ")\n",
    "similarity_score_input = tf.keras.layers.Input(\n",
    "    shape=(3,),  # Shape score similarity (Student, ChatGPT1, ChatGPT2)\n",
    "    dtype=tf.float32, \n",
    "    name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "# Dense layer untuk masing-masing input\n",
    "bert_dense_1 = tf.keras.layers.Dense(128, activation=\"relu\")(bert_embedding_input_1)\n",
    "bert_dense_2 = tf.keras.layers.Dense(128, activation=\"relu\")(bert_embedding_input_2)\n",
    "style_dense = tf.keras.layers.Dense(64, activation=\"relu\")(stylometric_input)\n",
    "sim_dense = tf.keras.layers.Dense(16, activation=\"relu\")(similarity_score_input)\n",
    "\n",
    "# Gabungkan semua fitur\n",
    "combined = tf.keras.layers.Concatenate()([bert_dense_1, bert_dense_2, style_dense, sim_dense])\n",
    "\n",
    "# Dense layers untuk klasifikasi\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(combined)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Buat model classifier\n",
    "classifier = tf.keras.Model(\n",
    "    inputs=[bert_embedding_input_1, bert_embedding_input_2, stylometric_input, similarity_score_input],\n",
    "    outputs=output,\n",
    "    name=\"text_classifier\"\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "# Tampilkan arsitektur model\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"student_similarity_scores shape: {student_similarity_scores.shape}\")\n",
    "print(f\"chatgpt1_similarity_scores shape: {chatgpt1_similarity_scores.shape}\")\n",
    "print(f\"chatgpt2_similarity_scores shape: {chatgpt2_similarity_scores.shape}\")\n",
    "print(f\"student_embeddings_1 shape: {student_embeddings_1.shape}\")\n",
    "print(f\"chatgpt_embeddings_1 shape: {chatgpt_embeddings_1.shape}\")\n",
    "print(f\"chatgpt_embeddings_2 shape: {chatgpt_embeddings_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung similarity scores\n",
    "# 1. Similarity dengan Student (dari model Student_ChatGPT)\n",
    "student_similarity_scores = []\n",
    "for emb in student_embeddings_1:\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), student_embeddings_1)\n",
    "    student_similarity_scores.append(max_similarity)\n",
    "\n",
    "# 2. Similarity dengan ChatGPT (dari model Student_ChatGPT)\n",
    "chatgpt1_similarity_scores = []\n",
    "for emb in student_embeddings_1:\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_1)\n",
    "    chatgpt1_similarity_scores.append(max_similarity)\n",
    "\n",
    "# Juga hitung similarity untuk ChatGPT embeddings dengan Student dan ChatGPT\n",
    "chatgpt_student_similarity = []\n",
    "chatgpt_chatgpt1_similarity = []\n",
    "for emb in chatgpt_embeddings_1:\n",
    "    # Similarity dengan Student\n",
    "    max_similarity_student = compute_similarity_max(tf.expand_dims(emb, 0), student_embeddings_1)\n",
    "    chatgpt_student_similarity.append(max_similarity_student)\n",
    "    \n",
    "    # Similarity dengan ChatGPT\n",
    "    max_similarity_chatgpt = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_1)\n",
    "    chatgpt_chatgpt1_similarity.append(max_similarity_chatgpt)\n",
    "\n",
    "# 3. Similarity dengan ChatGPT Knowledge (dari model Only_ChatGPT)\n",
    "# Tentukan jumlah yang diinginkan (sama dengan jumlah student_embeddings_1 + chatgpt_embeddings_1)\n",
    "desired_count = len(student_embeddings_1) + len(chatgpt_embeddings_1)\n",
    "\n",
    "# Pilih indeks secara acak dari chatgpt_embeddings_2 (jika perlu dengan replacement)\n",
    "np.random.seed(42)  # Untuk reproduksibilitas hasil\n",
    "if chatgpt_embeddings_2.shape[0] >= desired_count:\n",
    "    random_indices = np.random.choice(chatgpt_embeddings_2.shape[0], size=desired_count, replace=False)\n",
    "else:\n",
    "    # Jika tidak cukup sampel, gunakan replacement\n",
    "    random_indices = np.random.choice(chatgpt_embeddings_2.shape[0], size=desired_count, replace=True)\n",
    "\n",
    "# Convert chatgpt_embeddings_2 to NumPy array first, then index\n",
    "chatgpt_embeddings_2_np = chatgpt_embeddings_2.numpy()\n",
    "selected_chatgpt2_embeddings = chatgpt_embeddings_2_np[random_indices]\n",
    "\n",
    "# Hitung similarity untuk semua sampel dengan ChatGPT Knowledge\n",
    "chatgpt2_similarity_scores = []\n",
    "for emb in selected_chatgpt2_embeddings:\n",
    "    # Convert back to tensor for the similarity function\n",
    "    emb_tensor = tf.convert_to_tensor(emb, dtype=tf.float32)\n",
    "    emb_tensor = tf.expand_dims(emb_tensor, 0)  # Add batch dimension\n",
    "    max_similarity = compute_similarity_max(emb_tensor, chatgpt_embeddings_2)\n",
    "    chatgpt2_similarity_scores.append(max_similarity)\n",
    "\n",
    "\n",
    "# Pisahkan similarity scores untuk student dan chatgpt\n",
    "student_chatgpt2_similarity = chatgpt2_similarity_scores[:len(student_embeddings_1)]\n",
    "chatgpt_chatgpt2_similarity = chatgpt2_similarity_scores[len(student_embeddings_1):desired_count]\n",
    "\n",
    "# Gabungkan similarity scores untuk student dan chatgpt\n",
    "student_similarity_combined = np.column_stack([\n",
    "    student_similarity_scores,\n",
    "    chatgpt1_similarity_scores,\n",
    "    student_chatgpt2_similarity\n",
    "])\n",
    "\n",
    "chatgpt_similarity_combined = np.column_stack([\n",
    "    chatgpt_student_similarity,\n",
    "    chatgpt_chatgpt1_similarity,\n",
    "    chatgpt_chatgpt2_similarity\n",
    "])\n",
    "\n",
    "all_similarity_scores = np.vstack([\n",
    "    student_similarity_combined,\n",
    "    chatgpt_similarity_combined\n",
    "])\n",
    "\n",
    "# Combine embeddings for model 1 (already correct)\n",
    "embeddings_model_1 = np.vstack([student_embeddings_1.numpy(), chatgpt_embeddings_1.numpy()])\n",
    "\n",
    "# Combine embeddings for model 2 (need to match the structure of embeddings_model_1)\n",
    "embeddings_model_2 = selected_chatgpt2_embeddings\n",
    "# Combine stylometric features (need to match the structure of embeddings_model_1)\n",
    "# Make sure we're only using the features that correspond to our selected samples\n",
    "student_features_selected = student_features_normalized\n",
    "chatgpt_features_selected = chatgpt_features_1_normalized\n",
    "\n",
    "all_stylometric_features = np.vstack([\n",
    "    student_features_selected,\n",
    "    chatgpt_features_selected\n",
    "])\n",
    "\n",
    "# Create labels\n",
    "student_labels = np.zeros(len(student_embeddings_1))  # Label 0 for Student\n",
    "chatgpt_labels = np.ones(len(chatgpt_embeddings_1))   # Label 1 for ChatGPT\n",
    "all_labels = np.hstack([student_labels, chatgpt_labels])\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"embeddings_model_1 shape: {embeddings_model_1.shape}\")\n",
    "print(f\"embeddings_model_2 shape: {embeddings_model_2.shape}\")\n",
    "print(f\"all_stylometric_features shape: {all_stylometric_features.shape}\")\n",
    "print(f\"all_similarity_scores shape: {all_similarity_scores.shape}\")\n",
    "print(f\"all_labels shape: {all_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(all_labels)),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.33,  # 10% dari total (0.33 * 0.3 = 0.1)\n",
    "    random_state=42,\n",
    "    stratify=all_labels[temp_idx]\n",
    ")\n",
    "\n",
    "# Prepare inputs for training, validation, and test\n",
    "train_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[train_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[val_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[val_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[val_idx],\n",
    "    \"similarity_score\": all_similarity_scores[val_idx]\n",
    "}\n",
    "\n",
    "test_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[test_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[test_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[test_idx],\n",
    "    \"similarity_score\": all_similarity_scores[test_idx]\n",
    "}\n",
    "\n",
    "train_labels = all_labels[train_idx]\n",
    "val_labels = all_labels[val_idx]\n",
    "test_labels = all_labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(all_labels)),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.33,  # 10% dari total (0.33 * 0.3 = 0.1)\n",
    "    random_state=42,\n",
    "    stratify=all_labels[temp_idx]\n",
    ")\n",
    "\n",
    "# Prepare inputs for training, validation, and test\n",
    "train_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[train_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[train_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "\n",
    "test_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[train_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "\n",
    "train_labels = all_labels[train_idx]\n",
    "val_labels = all_labels[val_idx]\n",
    "test_labels = all_labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifier\n",
    "print(\"Training Text Classifier...\")\n",
    "history_classifier = classifier.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_classifier.history['loss'], label='Training Loss')\n",
    "plt.plot(history_classifier.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_classifier.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_classifier.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi model pada validation set\n",
    "val_loss, val_acc, val_precision, val_recall, val_auc = classifier.evaluate(val_inputs, val_labels)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Prediksi pada validation set\n",
    "val_predictions = classifier.predict(val_inputs)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_predictions_binary, target_names=['Student', 'ChatGPT']))\n",
    "\n",
    "# Evaluasi model pada test set\n",
    "test_loss, test_acc, test_precision, test_recall, test_auc = classifier.evaluate(test_inputs, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Prediksi pada test set\n",
    "test_predictions = classifier.predict(test_inputs)\n",
    "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix untuk test set\n",
    "cm_test = confusion_matrix(test_labels, test_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix untuk test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Test Set Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report untuk test set\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions_binary, target_names=['Student', 'ChatGPT']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fungsi untuk plot ROC curve\n",
    "def plot_roc_curve(labels, predictions, title):\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {title}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "# Plot ROC curve untuk validation set\n",
    "val_auc_score = plot_roc_curve(val_labels, val_predictions, \"Validation Set\")\n",
    "print(f\"Validation AUC from ROC curve: {val_auc_score:.4f}\")\n",
    "\n",
    "# Plot ROC curve untuk test set\n",
    "test_auc_score = plot_roc_curve(test_labels, test_predictions, \"Test Set\")\n",
    "print(f\"Test AUC from ROC curve: {test_auc_score:.4f}\")\n",
    "\n",
    "# Plot kedua kurva ROC dalam satu grafik untuk perbandingan\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(val_labels, val_predictions)\n",
    "roc_auc_val = auc(fpr_val, tpr_val)\n",
    "plt.plot(fpr_val, tpr_val, color='darkorange', lw=2, label=f'Validation ROC (area = {roc_auc_val:.2f})')\n",
    "\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(test_labels, test_predictions)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "plt.plot(fpr_test, tpr_test, color='green', lw=2, label=f'Test ROC (area = {roc_auc_test:.2f})')\n",
    "\n",
    "# Garis diagonal\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat direktori jika belum ada\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model bi-encoder untuk Student_ChatGPT\n",
    "bi_encoder_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model bi-encoder untuk Only_ChatGPT\n",
    "bi_encoder_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Simpan model classifier\n",
    "classifier.save('saved_models/text_classifier.h5')\n",
    "\n",
    "# Simpan konfigurasi tokenizer\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "# Simpan scaler untuk fitur stylometric\n",
    "with open(\"saved_models/scaler_stylometric.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model dan konfigurasi berhasil disimpan!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
