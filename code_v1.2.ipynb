{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumentasi Model Deteksi & Klasifikasi Esai Siswa dan ChatGPT\n",
    "\n",
    "Notebook ini bertujuan untuk membangun sistem klasifikasi esai siswa dan ChatPGT menggunakan dua dataset: Student_ChatGPT dan Only_ChatGPT. Sistem ini mencakup arsitektur Bi-Encoder dan classifier untuk membedakan teks dari Student dan ChatGPT. Dokumentasi ini memandu Anda melalui proses pembentukan sistem secara bertahap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library\n",
    "\n",
    "Pada langkah ini, semua library yang diperlukan untuk pemrosesan teks, pembelajaran mesin, dan visualisasi data diimpor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Dua dataset utama diimpor:\n",
    "1. Dataset Student_ChatGPT: Berisi interaksi antara Student dan ChatGPT dengan format:\n",
    "- Kolom 1: Teks dari Student.\n",
    "- Kolom 2: Respon dari ChatGPT.\n",
    "2. Dataset Only_ChatGPT: Berisi teks yang dihasilkan oleh ChatGPT terkait pengetahuan satu mata pelajaran selama satu tahun (2 semester).\n",
    "\n",
    "Dataset ini digunakan untuk membangun model Bi-Encoder dan classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_excel(\"example_datasets/examples-datasets-mar25.xlsx\")\n",
    "only_chatgpt = pd.read_excel(\"example_datasets/knowledge-datasets-mar25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Student_ChatGPT:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Pelajar  43 non-null     object\n",
      " 1   GPT      43 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 820.0+ bytes\n",
      "None\n",
      "\n",
      "Dataset Only_ChatGPT:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118 entries, 0 to 117\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   GPT     118 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "\n",
      "Contoh Data Student_ChatGPT:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pelajar</th>\n",
       "      <th>GPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teknologi adalah segala sesuatu yang menciptak...</td>\n",
       "      <td>Teknologi adalah alat atau sistem yang dicipta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sekarang, mari kita lihat fakta umum yang seri...</td>\n",
       "      <td>Dalam kehidupan sehari-hari, teknologi memenga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jika ditelisik lebih jauh, sudah banyak kiprah...</td>\n",
       "      <td>Selain itu, tantangan besar lainnya adalah ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lalu sebenarnya apa saja peran pelajar yang da...</td>\n",
       "      <td>Masyarakat memiliki pandangan yang beragam ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salah satu pelajar yang berkiprah dalam menyok...</td>\n",
       "      <td>Salah satu contoh positif dari pemanfaatan tek...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Pelajar  \\\n",
       "0  Teknologi adalah segala sesuatu yang menciptak...   \n",
       "1  Sekarang, mari kita lihat fakta umum yang seri...   \n",
       "2  Jika ditelisik lebih jauh, sudah banyak kiprah...   \n",
       "3  Lalu sebenarnya apa saja peran pelajar yang da...   \n",
       "4  Salah satu pelajar yang berkiprah dalam menyok...   \n",
       "\n",
       "                                                 GPT  \n",
       "0  Teknologi adalah alat atau sistem yang dicipta...  \n",
       "1  Dalam kehidupan sehari-hari, teknologi memenga...  \n",
       "2  Selain itu, tantangan besar lainnya adalah ban...  \n",
       "3  Masyarakat memiliki pandangan yang beragam ter...  \n",
       "4  Salah satu contoh positif dari pemanfaatan tek...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contoh Data Only_ChatGPT:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teknologi adalah sekumpulan pengetahuan dan ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Di era digital ini, pelajar menghadapi berbaga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Masyarakat memiliki pandangan yang beragam ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salah satu contoh nyata dari pelajar yang berh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Namun, untuk dapat berhasil dalam memanfaatkan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 GPT\n",
       "0  Teknologi adalah sekumpulan pengetahuan dan ke...\n",
       "1  Di era digital ini, pelajar menghadapi berbaga...\n",
       "2  Masyarakat memiliki pandangan yang beragam ter...\n",
       "3  Salah satu contoh nyata dari pelajar yang berh...\n",
       "4  Namun, untuk dapat berhasil dalam memanfaatkan..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tampilkan informasi dataset\n",
    "print(\"Dataset Student_ChatGPT:\")\n",
    "print(student_chatgpt.info())\n",
    "print(\"\\nDataset Only_ChatGPT:\")\n",
    "print(only_chatgpt.info())\n",
    "\n",
    "# Tampilkan beberapa baris awal dataset\n",
    "print(\"\\nContoh Data Student_ChatGPT:\")\n",
    "display(student_chatgpt.head())\n",
    "\n",
    "print(\"\\nContoh Data Only_ChatGPT:\")\n",
    "display(only_chatgpt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data\n",
    "\n",
    "Langkah ini memproses data mentah menjadi format yang siap digunakan oleh model:\n",
    "1. Preprocessing Teks:\n",
    "- Semua teks diubah menjadi huruf kecil.\n",
    "- Teks dipisahkan menjadi kalimat menggunakan tokenisasi kalimat.\n",
    "2. Distribusi Data:\n",
    "- Total jumlah kalimat dari masing-masing sumber (Student, ChatGPT dari Student_ChatGPT, dan Only_ChatGPT) dihitung dan ditampilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total kalimat Student: 168\n",
      "Total kalimat ChatGPT (Student_ChatGPT): 186\n",
      "Total kalimat ChatGPT (Only_ChatGPT): 413\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing teks:\n",
    "    - Mengubah teks menjadi huruf kecil.\n",
    "    - Membagi teks menjadi kalimat (tokenisasi kalimat) menggunakan regex.\n",
    "    - Tanda baca terakhir (., ?, !) tetap disertakan dalam hasil tokenisasi.\n",
    "\n",
    "    Args:\n",
    "        text (str): Teks input.\n",
    "\n",
    "    Returns:\n",
    "        list: Daftar kalimat yang telah diproses.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return []\n",
    "\n",
    "    # Ubah teks menjadi huruf kecil dan hilangkan spasi di awal/akhir\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Tokenisasi kalimat menggunakan regex\n",
    "    sentences = re.findall(r'.+?[.!?](?=\\s|$)', text)\n",
    "    # Hapus elemen kosong dari hasil split (jika ada)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Preprocessing data Student_ChatGPT\n",
    "student_sentences = []\n",
    "chatgpt_sentences_1 = []  # ChatGPT dari dataset Student_ChatGPT\n",
    "\n",
    "# Proses teks Student\n",
    "for text in student_chatgpt['Pelajar']:\n",
    "    student_sentences.extend(preprocess_text(text))\n",
    "\n",
    "# Proses teks ChatGPT (dari Student_ChatGPT)\n",
    "for text in student_chatgpt['GPT']:\n",
    "    chatgpt_sentences_1.extend(preprocess_text(text))\n",
    "\n",
    "# Preprocessing data Only_ChatGPT\n",
    "chatgpt_sentences_2 = []  # ChatGPT dari dataset Only_ChatGPT\n",
    "\n",
    "# Proses teks ChatGPT (dari Only_ChatGPT)\n",
    "for text in only_chatgpt['GPT']:\n",
    "    chatgpt_sentences_2.extend(preprocess_text(text))\n",
    "\n",
    "# Tampilkan jumlah data hasil preprocessing\n",
    "print(f\"Total kalimat Student: {len(student_sentences)}\")\n",
    "print(f\"Total kalimat ChatGPT (Student_ChatGPT): {len(chatgpt_sentences_1)}\")\n",
    "print(f\"Total kalimat ChatGPT (Only_ChatGPT): {len(chatgpt_sentences_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inisialisasi BERT Tokenizer\n",
    "\n",
    "Teks dari dataset di-tokenisasi menggunakan tokenizer pretrained dari IndoBERT:\n",
    "- `input_ids`: Token ID untuk setiap kata dalam teks.\n",
    "- `attention_mask`: Masking untuk menandai kata yang relevan dan padding.\n",
    "\n",
    "Tokenisasi diperlukan agar teks dapat diproses oleh model BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(texts, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisasi kalimat Student...\n",
      "Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\n",
      "Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\n",
      "\n",
      "Contoh hasil tokenisasi:\n",
      "tf.Tensor(\n",
      "[[    2  1429   154  1517  1370    34  2800  7630    41 11811    90   666\n",
      "  30470     3     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 128), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenisasi data\n",
    "print(\"Tokenisasi kalimat Student...\")\n",
    "student_tokens = tokenize_text(student_sentences)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_tokens_1 = tokenize_text(chatgpt_sentences_1)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_tokens_2 = tokenize_text(chatgpt_sentences_2)\n",
    "\n",
    "# Tampilkan hasil tokenisasi (contoh: Student)\n",
    "print(\"\\nContoh hasil tokenisasi:\")\n",
    "print(student_tokens['input_ids'][:1])  # Input token ID\n",
    "print(student_tokens['attention_mask'][:1])  # Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rata-rata panjang token: 20.35\n",
      "Persentase terpotong: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Memeriksa distribusi panjang token untuk memastikan max_length cukup\n",
    "student_lengths = [sum(mask) for mask in student_tokens['attention_mask'].numpy()]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(student_lengths, bins=30)\n",
    "plt.title('Distribusi Panjang Token (Student)')\n",
    "plt.xlabel('Jumlah Token Aktif')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.show()\n",
    "print(f\"Rata-rata panjang token: {np.mean(student_lengths):.2f}\")\n",
    "print(f\"Persentase terpotong: {sum(l == 128 for l in student_lengths) / len(student_lengths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: teknologi adalah segala sesuatu yang menciptakan efisiensi dan efektivitas untuk manusia.\n",
      "Token ID: [2, 1429, 154, 1517, 1370, 34, 2800, 7630, 41, 11811, 90, 666, 30470, 3]\n",
      "Token dekode: [CLS] teknologi adalah segala sesuatu yang menciptakan efisiensi dan efektivitas untuk manusia. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Mendekode token untuk memastikan tokenisasi berfungsi dengan baik\n",
    "sample_text = student_sentences[0]\n",
    "sample_tokens = tokenizer.encode(sample_text)\n",
    "print(f\"Teks asli: {sample_text}\")\n",
    "print(f\"Token ID: {sample_tokens}\")\n",
    "print(f\"Token dekode: {tokenizer.decode(sample_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika tokenisasi memakan waktu lama, pertimbangkan untuk menyimpannya\n",
    "tokenized_data = {\n",
    "    'student': student_tokens,\n",
    "    'chatgpt_1': chatgpt_tokens_1,\n",
    "    'chatgpt_2': chatgpt_tokens_2\n",
    "}\n",
    "\n",
    "# Menyimpan input_ids dan attention_mask sebagai numpy arrays\n",
    "tokenized_numpy = {\n",
    "    'student': {\n",
    "        'input_ids': student_tokens['input_ids'].numpy(),\n",
    "        'attention_mask': student_tokens['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt_1': {\n",
    "        'input_ids': chatgpt_tokens_1['input_ids'].numpy(),\n",
    "        'attention_mask': chatgpt_tokens_1['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt_2': {\n",
    "        'input_ids': chatgpt_tokens_2['input_ids'].numpy(),\n",
    "        'attention_mask': chatgpt_tokens_2['attention_mask'].numpy()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('tokenized_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_numpy, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Membuat Model BERT Bi-Encoder\n",
    "\n",
    "Dua model Bi-Encoder dibuat:\n",
    "- Bi-Encoder untuk Only_ChatGPT: Digunakan untuk memahami teks dari dataset Only_ChatGPT.\n",
    "- Bi-Encoder untuk Student_ChatGPT: Digunakan untuk memahami interaksi antara Student dan ChatGPT.\n",
    "\n",
    "Arsitektur Bi-Encoder melibatkan:\n",
    "- IndoBERT sebagai backbone untuk menghasilkan embeddings.\n",
    "- Dense Layers untuk fine-tuning.\n",
    "- L2 Normalization untuk menghasilkan embeddings yang seragam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p2 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-Encoder untuk Student_ChatGPT:\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_3 (TFBertMod  TFBaseModelOutputWithPooli   1244413   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   44         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6  (None, 768)                  0         ['tf_bert_model_3[0][0]']     \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 128)                  98432     ['tf.__operators__.getitem_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_160 (Dropout)       (None, 128)                  0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 64)                   8256      ['dropout_160[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_161 (Dropout)       (None, 64)                   0         ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 64)                   4160      ['dropout_161[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_6 (TF  (None, 64)                   0         ['dense_20[0][0]']            \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124552192 (475.13 MB)\n",
      "Trainable params: 110848 (433.00 KB)\n",
      "Non-trainable params: 124441344 (474.71 MB)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Bi-Encoder untuk Student_ChatGPT:\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model_3 (TFBertMod  TFBaseModelOutputWithPooli   1244413   ['input_ids[0][0]',           \n",
      " el)                         ngAndCrossAttentions(last_   44         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6  (None, 768)                  0         ['tf_bert_model_3[0][0]']     \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 128)                  98432     ['tf.__operators__.getitem_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_160 (Dropout)       (None, 128)                  0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 64)                   8256      ['dropout_160[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_161 (Dropout)       (None, 64)                   0         ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 64)                   4160      ['dropout_161[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_6 (TF  (None, 64)                   0         ['dense_20[0][0]']            \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124552192 (475.13 MB)\n",
      "Trainable params: 110848 (433.00 KB)\n",
      "Non-trainable params: 124441344 (474.71 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Freeze BERT layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Fungsi untuk membuat Bi-Encoder\n",
    "def build_bi_encoder(bert_model):\n",
    "    \"\"\"\n",
    "    Membuat model Bi-Encoder dengan IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        bert_model (TFBertModel): Model dasar IndoBERT.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Model Bi-Encoder.\n",
    "    \"\"\"\n",
    "    # Input layer untuk token ID dan attention mask\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Extract CLS token embeddings dari IndoBERT\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0][:, 0, :]  # [CLS] token\n",
    "    \n",
    "    # Dense layer untuk fine-tuning\n",
    "    dense1 = tf.keras.layers.Dense(128, activation=\"relu\")(bert_output)\n",
    "    dropout1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "    dense2 = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1)\n",
    "    dropout2 = tf.keras.layers.Dropout(0.1)(dense2)\n",
    "    dense3 = tf.keras.layers.Dense(128)(dropout2)\n",
    "    \n",
    "    # Normalisasi output (L2 normalization)\n",
    "    normalized_output = tf.nn.l2_normalize(dense3, axis=1)\n",
    "    \n",
    "    # Model Bi-Encoder\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=normalized_output)\n",
    "\n",
    "# Buat dua Bi-Encoder\n",
    "bi_encoder_student_chatgpt = build_bi_encoder(bert_model)\n",
    "bi_encoder_only_chatgpt = build_bi_encoder(bert_model)\n",
    "\n",
    "# Tampilkan arsitektur\n",
    "print(\"Bi-Encoder untuk Student_ChatGPT:\")\n",
    "bi_encoder_student_chatgpt.summary()\n",
    "\n",
    "print(\"\\nBi-Encoder untuk Student_ChatGPT:\")\n",
    "bi_encoder_student_chatgpt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Constrastive Loss\n",
    "\n",
    "Contrastive loss digunakan untuk melatih Bi-Encoder dengan tujuan:\n",
    "- Pasangan kalimat yang mirip memiliki nilai similarity tinggi (loss rendah).\n",
    "- Pasangan kalimat yang tidak mirip memiliki nilai similarity rendah (loss tinggi).\n",
    "\n",
    "Datasets Student_ChatGPT: variable -> student_sentences & chatgpt_sentences_1\n",
    "Pasangan Positif:\n",
    "- Student - Student Dalam satu data, antar kalimat\n",
    "- Student - Student Beda data, Antar kalimat\n",
    "- ChatGPT - ChatGPT Dalam satu data, antar kalimat\n",
    "- ChatGPT - ChatGPT Beda data, Antar kalimat\n",
    "\n",
    "Pasangan Negatif:\n",
    "- Student - ChatGPT Beda data, Antar kalimat\n",
    "\n",
    "Datasets Only_ChatGPT: variable -> chatgpt_sentences_2\n",
    "Pasangan Positif:\n",
    "- ChatGPT - ChatGPT Dalam satu data, antar kalimat\n",
    "- ChatGPT - ChatGPT Beda data, Antar kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrastive_pairs(student_tokens, chatgpt_tokens, max_pairs=None):\n",
    "    \"\"\"\n",
    "    Membuat pasangan data untuk contrastive learning dengan jumlah yang menyesuaikan dataset.\n",
    "\n",
    "    Args:\n",
    "        student_tokens: Token dari teks student.\n",
    "        chatgpt_tokens: Token dari teks ChatGPT.\n",
    "        max_pairs: Jumlah maksimum pasangan (opsional). Jika None, akan menggunakan semua kombinasi yang mungkin.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Pasangan anchor, positive, negative, dan label serta jumlah total pasangan.\n",
    "    \"\"\"\n",
    "    # Import yang diperlukan\n",
    "    import random\n",
    "\n",
    "    # Jumlah data\n",
    "    n_student = student_tokens['input_ids'].shape[0]\n",
    "    n_chatgpt = chatgpt_tokens['input_ids'].shape[0]\n",
    "\n",
    "    # Hitung jumlah pasangan yang mungkin\n",
    "    max_student_pairs = (n_student * (n_student - 1)) // 2  # Kombinasi student-student\n",
    "    max_chatgpt_pairs = (n_chatgpt * (n_chatgpt - 1)) // 2  # Kombinasi chatgpt-chatgpt\n",
    "    max_negative_pairs = n_student * n_chatgpt  # Kombinasi student-chatgpt\n",
    "\n",
    "    # Tentukan jumlah pasangan yang akan dibuat\n",
    "    if max_pairs is None:\n",
    "        # Gunakan jumlah minimum dari pasangan positif untuk keseimbangan\n",
    "        n_pos_student = min(max_student_pairs, max_chatgpt_pairs) // 2\n",
    "        n_pos_chatgpt = n_pos_student\n",
    "        # Batasi jumlah pasangan negatif agar seimbang dengan positif\n",
    "        n_neg_pairs = min(max_negative_pairs, 2 * n_pos_student)\n",
    "        # Total pasangan\n",
    "        total_pairs = 2 * n_pos_student + n_neg_pairs\n",
    "    else:\n",
    "        # Jika max_pairs ditentukan, gunakan itu dengan proporsi yang sama\n",
    "        n_pos_student = max_pairs // 4\n",
    "        n_pos_chatgpt = max_pairs // 4\n",
    "        n_neg_pairs = max_pairs // 2\n",
    "        total_pairs = max_pairs\n",
    "\n",
    "    # Pastikan tidak melebihi jumlah maksimum yang mungkin\n",
    "    n_pos_student = min(n_pos_student, max_student_pairs)\n",
    "    n_pos_chatgpt = min(n_pos_chatgpt, max_chatgpt_pairs)\n",
    "    n_neg_pairs = min(n_neg_pairs, max_negative_pairs)\n",
    "\n",
    "    # Inisialisasi array untuk pasangan data\n",
    "    anchor_input_ids = []\n",
    "    anchor_attention_mask = []\n",
    "    positive_input_ids = []\n",
    "    positive_attention_mask = []\n",
    "    negative_input_ids = []\n",
    "    negative_attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    # Membuat pasangan positif (student-student)\n",
    "    if n_pos_student > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        student_pairs = [(i, j) for i in range(n_student) for j in range(i+1, n_student)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(student_pairs, n_pos_student)\n",
    "\n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][idx1])\n",
    "\n",
    "            positive_input_ids.append(student_tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(student_tokens['attention_mask'][idx2])\n",
    "\n",
    "            # Negative dari ChatGPT\n",
    "            neg_idx = np.random.choice(n_chatgpt)\n",
    "            negative_input_ids.append(chatgpt_tokens['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens['attention_mask'][neg_idx])\n",
    "\n",
    "            labels.append(1)  # 1 untuk pasangan positif\n",
    "\n",
    "    # Membuat pasangan positif (chatgpt-chatgpt)\n",
    "    if n_pos_chatgpt > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        chatgpt_pairs = [(i, j) for i in range(n_chatgpt) for j in range(i+1, n_chatgpt)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(chatgpt_pairs, n_pos_chatgpt)\n",
    "\n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            anchor_input_ids.append(chatgpt_tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(chatgpt_tokens['attention_mask'][idx1])\n",
    "\n",
    "            positive_input_ids.append(chatgpt_tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(chatgpt_tokens['attention_mask'][idx2])\n",
    "\n",
    "            # Negative dari Student\n",
    "            neg_idx = np.random.choice(n_student)\n",
    "            negative_input_ids.append(student_tokens['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(student_tokens['attention_mask'][neg_idx])\n",
    "\n",
    "            labels.append(1)  # 1 untuk pasangan positif\n",
    "\n",
    "    # Membuat pasangan negatif (student-chatgpt)\n",
    "    if n_neg_pairs > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        negative_pairs = [(i, j) for i in range(n_student) for j in range(n_chatgpt)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(negative_pairs, n_neg_pairs)\n",
    "\n",
    "        for student_idx, chatgpt_idx in selected_pairs:\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][student_idx])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][student_idx])\n",
    "\n",
    "            negative_input_ids.append(chatgpt_tokens['input_ids'][chatgpt_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens['attention_mask'][chatgpt_idx])\n",
    "\n",
    "            # Positive dari Student (berbeda dengan anchor)\n",
    "            available_pos = [i for i in range(n_student) if i != student_idx]\n",
    "            if available_pos:  # Pastikan ada indeks yang tersedia\n",
    "                pos_idx = np.random.choice(available_pos)\n",
    "                positive_input_ids.append(student_tokens['input_ids'][pos_idx])\n",
    "                positive_attention_mask.append(student_tokens['attention_mask'][pos_idx])\n",
    "\n",
    "                labels.append(0)  # 0 untuk pasangan negatif\n",
    "\n",
    "    # Hitung jumlah pasangan yang sebenarnya dibuat\n",
    "    actual_pairs = len(labels)\n",
    "\n",
    "    # Konversi ke tensor\n",
    "    return {\n",
    "        'anchor': {\n",
    "            'input_ids': tf.convert_to_tensor(anchor_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(anchor_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'positive': {\n",
    "            'input_ids': tf.convert_to_tensor(positive_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(positive_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': tf.convert_to_tensor(negative_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(negative_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'labels': tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    }, actual_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total contrastive pairs yang dibuat: 31152\n",
      "- Pasangan positif student-student: 7788\n",
      "- Pasangan positif chatgpt-chatgpt: 7788\n",
      "- Pasangan negatif student-chatgpt: 15576\n"
     ]
    }
   ],
   "source": [
    "# Buat pasangan data untuk model Student_ChatGPT tanpa menentukan n_pairs\n",
    "student_chatgpt_pairs, total_pairs = create_contrastive_pairs(student_tokens, chatgpt_tokens_1, max_pairs=5000)\n",
    "\n",
    "# Tampilkan jumlah pasangan yang dibuat\n",
    "print(f\"Total contrastive pairs yang dibuat: {total_pairs}\")\n",
    "print(f\"- Pasangan positif student-student: {sum(1 for label in student_chatgpt_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Pasangan positif chatgpt-chatgpt: {sum(1 for label in student_chatgpt_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Pasangan negatif student-chatgpt: {sum(1 for label in student_chatgpt_pairs['labels'].numpy() if label == 0)}\")\n",
    "\n",
    "# Jika ingin membatasi jumlah maksimum pasangan\n",
    "# student_chatgpt_pairs, total_pairs = create_contrastive_pairs(student_tokens, chatgpt_tokens_1, max_pairs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi contrastive loss\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Contrastive loss untuk triplet (anchor, positive, negative).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Label (1 untuk pasangan positif, 0 untuk pasangan negatif).\n",
    "        y_pred: Jarak antara anchor-positive dan anchor-negative.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Nilai loss.\n",
    "    \"\"\"\n",
    "    margin = 0.5\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model untuk training dengan triplet loss\n",
    "def build_triplet_model(bi_encoder):\n",
    "    \"\"\"\n",
    "    Membangun model untuk training dengan triplet loss.\n",
    "    \n",
    "    Args:\n",
    "        bi_encoder: Model bi-encoder yang akan dilatih.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Model untuk training dengan triplet loss.\n",
    "    \"\"\"\n",
    "    # Input untuk anchor, positive, dan negative\n",
    "    anchor_input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"anchor_input_ids\")\n",
    "    anchor_attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"anchor_attention_mask\")\n",
    "\n",
    "    positive_input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"positive_input_ids\")\n",
    "    positive_attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"positive_attention_mask\")\n",
    "    \n",
    "    negative_input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"negative_input_ids\")\n",
    "    negative_attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"negative_attention_mask\")\n",
    "    \n",
    "    # Embedding untuk anchor, positive, dan negative\n",
    "    anchor_embedding = bi_encoder([anchor_input_ids, anchor_attention_mask])\n",
    "    positive_embedding = bi_encoder([positive_input_ids, positive_attention_mask])\n",
    "    negative_embedding = bi_encoder([negative_input_ids, negative_attention_mask])\n",
    "    \n",
    "    # Hitung cosine similarity\n",
    "    pos_similarity = tf.reduce_sum(anchor_embedding * positive_embedding, axis=1)\n",
    "    neg_similarity = tf.reduce_sum(anchor_embedding * negative_embedding, axis=1)\n",
    "    \n",
    "    # Output model adalah perbedaan similarity\n",
    "    output = tf.stack([pos_similarity, neg_similarity], axis=1)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[\n",
    "            anchor_input_ids, anchor_attention_mask,\n",
    "            positive_input_ids, positive_attention_mask,\n",
    "            negative_input_ids, negative_attention_mask\n",
    "        ],\n",
    "        outputs=output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model triplet untuk Student_ChatGPT\n",
    "triplet_model_student_chatgpt = build_triplet_model(bi_encoder_student_chatgpt)\n",
    "\n",
    "# Custom loss function untuk triplet\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Triplet loss: mendorong similarity positif lebih tinggi dari similarity negatif.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tidak digunakan dalam triplet loss.\n",
    "        y_pred: Stack dari [positive_similarity, negative_similarity].\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Nilai loss.\n",
    "    \"\"\"\n",
    "    pos_sim = y_pred[:, 0]\n",
    "    neg_sim = y_pred[:, 1]\n",
    "    margin = 0.5\n",
    "    \n",
    "    # Triplet loss: max(0, margin - (pos_sim - neg_sim))\n",
    "    loss = tf.maximum(0., margin - (pos_sim - neg_sim))\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Bi-Encoder\n",
    "\n",
    "Kedua Bi-Encoder dilatih secara terpisah:\n",
    "1. Only_ChatGPT:\n",
    "- Pasangan positif: Kalimat yang saling berdekatan dalam satu bab.\n",
    "2. Student_ChatGPT:\n",
    "- Pasangan positif: Student dan ChatGPT untuk soal yang sama.\n",
    "- Pasangan negatif: Student dengan Student atau ChatGPT dengan ChatGPT.\n",
    "\n",
    "Hasil pelatihan divisualisasikan untuk memantau performa loss selama training dan validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Training Student - ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "triplet_model_student_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=triplet_loss\n",
    ")\n",
    "\n",
    "# Training model Student_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Student_ChatGPT...\")\n",
    "history_student_chatgpt = triplet_model_student_chatgpt.fit(\n",
    "    x=[\n",
    "        student_chatgpt_pairs['anchor']['input_ids'],\n",
    "        student_chatgpt_pairs['anchor']['attention_mask'],\n",
    "        student_chatgpt_pairs['positive']['input_ids'],\n",
    "        student_chatgpt_pairs['positive']['attention_mask'],\n",
    "        student_chatgpt_pairs['negative']['input_ids'],\n",
    "        student_chatgpt_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    y=student_chatgpt_pairs['labels'],  # Tidak digunakan dalam triplet loss\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Training Only ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrastive_pairs_2(student_tokens, chatgpt_tokens_1, chatgpt_tokens_2, max_pairs=None):\n",
    "    \"\"\"\n",
    "    Membuat pasangan data untuk contrastive learning dengan jumlah yang menyesuaikan dataset.\n",
    "\n",
    "    Args:\n",
    "        student_tokens: Token dari teks student.\n",
    "        chatgpt_tokens: Token dari teks ChatGPT.\n",
    "        max_pairs: Jumlah maksimum pasangan (opsional). Jika None, akan menggunakan semua kombinasi yang mungkin.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Pasangan anchor, positive, negative, dan label serta jumlah total pasangan.\n",
    "    \"\"\"\n",
    "\n",
    "    # Jumlah data\n",
    "    n_student = student_tokens['input_ids'].shape[0]\n",
    "    n_chatgpt_1 = chatgpt_tokens_1['input_ids'].shape[0]\n",
    "    n_chatgpt_2 = chatgpt_tokens_2['input_ids'].shape[0]\n",
    "\n",
    "    # Hitung jumlah pasangan yang mungkin\n",
    "    max_student_pairs = (n_student * (n_student - 1)) // 2  # Kombinasi student-student\n",
    "    max_chatgpt_pairs = (n_chatgpt_1 * (n_chatgpt_1 - 1)) // 2  # Kombinasi chatgpt-chatgpt\n",
    "    max_negative_pairs = n_student * n_chatgpt_2  # Kombinasi student-chatgpt\n",
    "\n",
    "    # Tentukan jumlah pasangan yang akan dibuat\n",
    "    if max_pairs is None:\n",
    "        # Gunakan jumlah minimum dari pasangan positif untuk keseimbangan\n",
    "        n_pos_student = min(max_student_pairs, max_chatgpt_pairs) // 2\n",
    "        n_pos_chatgpt = n_pos_student\n",
    "        # Batasi jumlah pasangan negatif agar seimbang dengan positif\n",
    "        n_neg_pairs = min(max_negative_pairs, 2 * n_pos_student)\n",
    "        # Total pasangan\n",
    "        total_pairs = 2 * n_pos_student + n_neg_pairs\n",
    "    else:\n",
    "        # Jika max_pairs ditentukan, gunakan itu dengan proporsi yang sama\n",
    "        n_pos_student = max_pairs // 4\n",
    "        n_pos_chatgpt = max_pairs // 4\n",
    "        n_neg_pairs = max_pairs // 2\n",
    "        total_pairs = max_pairs\n",
    "\n",
    "    # Pastikan tidak melebihi jumlah maksimum yang mungkin\n",
    "    n_pos_student = min(n_pos_student, max_student_pairs)\n",
    "    n_pos_chatgpt = min(n_pos_chatgpt, max_chatgpt_pairs)\n",
    "    n_neg_pairs = min(n_neg_pairs, max_negative_pairs)\n",
    "\n",
    "    # Inisialisasi array untuk pasangan data\n",
    "    anchor_input_ids = []\n",
    "    anchor_attention_mask = []\n",
    "    positive_input_ids = []\n",
    "    positive_attention_mask = []\n",
    "    negative_input_ids = []\n",
    "    negative_attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    # Membuat pasangan positif (student-student)\n",
    "    if n_pos_student > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        student_pairs = [(i, j) for i in range(n_student) for j in range(i+1, n_student)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(student_pairs, n_pos_student)\n",
    "\n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][idx1])\n",
    "\n",
    "            positive_input_ids.append(student_tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(student_tokens['attention_mask'][idx2])\n",
    "\n",
    "            # Negative dari ChatGPT\n",
    "            neg_idx = np.random.choice(n_chatgpt_2)\n",
    "            negative_input_ids.append(chatgpt_tokens_2['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens_2['attention_mask'][neg_idx])\n",
    "\n",
    "            labels.append(1)  # 1 untuk pasangan positif\n",
    "\n",
    "    # Membuat pasangan positif (chatgpt-chatgpt)\n",
    "    if n_pos_chatgpt > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        chatgpt_pairs = [(i, j) for i in range(n_chatgpt_1) for j in range(i+1, n_chatgpt_1)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(chatgpt_pairs, n_pos_chatgpt)\n",
    "\n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            anchor_input_ids.append(chatgpt_tokens_1['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(chatgpt_tokens_1['attention_mask'][idx1])\n",
    "\n",
    "            positive_input_ids.append(chatgpt_tokens_2['input_ids'][idx2])\n",
    "            positive_attention_mask.append(chatgpt_tokens_2['attention_mask'][idx2])\n",
    "\n",
    "            # Negative dari Student\n",
    "            neg_idx = np.random.choice(n_student)\n",
    "            negative_input_ids.append(student_tokens['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(student_tokens['attention_mask'][neg_idx])\n",
    "\n",
    "            labels.append(1)  # 1 untuk pasangan positif\n",
    "\n",
    "    # Membuat pasangan negatif (student-chatgpt)\n",
    "    if n_neg_pairs > 0:\n",
    "        # Buat semua kemungkinan pasangan indeks\n",
    "        negative_pairs = [(i, j) for i in range(n_student) for j in range(n_chatgpt_2)]\n",
    "        # Acak dan ambil sebanyak yang diperlukan\n",
    "        selected_pairs = random.sample(negative_pairs, n_neg_pairs)\n",
    "\n",
    "        for student_idx, chatgpt_idx in selected_pairs:\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][student_idx])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][student_idx])\n",
    "\n",
    "            negative_input_ids.append(chatgpt_tokens_2['input_ids'][chatgpt_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens_2['attention_mask'][chatgpt_idx])\n",
    "\n",
    "            # Positive dari Student (berbeda dengan anchor)\n",
    "            available_pos = [i for i in range(n_student) if i != student_idx]\n",
    "            if available_pos:  # Pastikan ada indeks yang tersedia\n",
    "                pos_idx = np.random.choice(available_pos)\n",
    "                positive_input_ids.append(student_tokens['input_ids'][pos_idx])\n",
    "                positive_attention_mask.append(student_tokens['attention_mask'][pos_idx])\n",
    "\n",
    "                labels.append(0)  # 0 untuk pasangan negatif\n",
    "\n",
    "    # Hitung jumlah pasangan yang sebenarnya dibuat\n",
    "    actual_pairs = len(labels)\n",
    "\n",
    "    # Konversi ke tensor\n",
    "    return {\n",
    "        'anchor': {\n",
    "            'input_ids': tf.convert_to_tensor(anchor_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(anchor_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'positive': {\n",
    "            'input_ids': tf.convert_to_tensor(positive_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(positive_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': tf.convert_to_tensor(negative_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(negative_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'labels': tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    }, actual_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat pasangan data untuk model Only_ChatGPT tanpa menentukan n_pairs\n",
    "only_chatgpt_pairs, total_pairs = create_contrastive_pairs_2(student_tokens, chatgpt_tokens_1, chatgpt_tokens_2, max_pairs=5000)\n",
    "\n",
    "# Tampilkan jumlah pasangan yang dibuat\n",
    "print(f\"Total Knowledge GPT pairs yang dibuat: {total_pairs}\")\n",
    "print(f\"- Pasangan positif student-student: {sum(1 for label in only_chatgpt_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Pasangan positif chatgpt-chatgpt: {sum(1 for label in only_chatgpt_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Pasangan negatif student-chatgpt: {sum(1 for label in only_chatgpt_pairs['labels'].numpy() if label == 0)}\")\n",
    "\n",
    "# Jika ingin membatasi jumlah maksimum pasangan\n",
    "# only_chatgpt_pairs, total_pairs = create_self_supervised_pairs(chatgpt_tokens_2, max_pairs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model triplet untuk Only_ChatGPT\n",
    "triplet_model_only_chatgpt = build_triplet_model(bi_encoder_only_chatgpt)\n",
    "\n",
    "# Compile model\n",
    "triplet_model_only_chatgpt.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=triplet_loss\n",
    ")\n",
    "\n",
    "# Training model Only_ChatGPT\n",
    "print(\"Training Bi-Encoder untuk Only_ChatGPT...\")\n",
    "history_only_chatgpt = triplet_model_only_chatgpt.fit(\n",
    "    x=[\n",
    "        only_chatgpt_pairs['anchor']['input_ids'],\n",
    "        only_chatgpt_pairs['anchor']['attention_mask'],\n",
    "        only_chatgpt_pairs['positive']['input_ids'],\n",
    "        only_chatgpt_pairs['positive']['attention_mask'],\n",
    "        only_chatgpt_pairs['negative']['input_ids'],\n",
    "        only_chatgpt_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    y=only_chatgpt_pairs['labels'],  # Tidak digunakan dalam triplet loss\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history untuk model Student_ChatGPT\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_student_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_student_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Student_ChatGPT: Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training history untuk model Only_ChatGPT\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_only_chatgpt.history['loss'], label='Training Loss')\n",
    "plt.plot(history_only_chatgpt.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Only_ChatGPT: Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluasi Bi-Encoder\n",
    "\n",
    "Bi-Encoder dievaluasi dengan menghitung cosine similarity antara embeddings yang dihasilkan:\n",
    "- Student vs ChatGPT (Student_ChatGPT).\n",
    "- ChatGPT (Student_ChatGPT) vs ChatGPT (Only_ChatGPT).\n",
    "\n",
    "Similarity score digunakan untuk memahami seberapa dekat embeddings dari berbagai sumber teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghasilkan embeddings\n",
    "def generate_embeddings(tokens, model):\n",
    "    \"\"\"\n",
    "    Menghasilkan embeddings untuk teks.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Token dari teks.\n",
    "        model: Model bi-encoder.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Embeddings.\n",
    "    \"\"\"\n",
    "    return model([tokens['input_ids'], tokens['attention_mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung similarity score dengan agregasi maksimum\n",
    "def compute_similarity_max(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Menghitung similarity score dengan metode agregasi maksimum.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: Embedding pertama (teks input).\n",
    "        embedding2: Embedding kedua (referensi).\n",
    "        \n",
    "    Returns:\n",
    "        float: Similarity score maksimum.\n",
    "    \"\"\"\n",
    "    # Normalisasi embeddings (L2 norm)\n",
    "    embedding1_norm = tf.nn.l2_normalize(embedding1, axis=-1)\n",
    "    embedding2_norm = tf.nn.l2_normalize(embedding2, axis=-1)\n",
    "    \n",
    "    # Hitung cosine similarity\n",
    "    similarities = tf.matmul(embedding1_norm, tf.transpose(embedding2_norm))\n",
    "    similarities = tf.reshape(similarities, [-1])  # Flatten\n",
    "    \n",
    "    # Ambil nilai maksimum\n",
    "    max_similarity = tf.reduce_max(similarities).numpy()\n",
    "    return max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model bi-encoder untuk Student_ChatGPT\n",
    "bi_encoder_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model bi-encoder untuk Only_ChatGPT\n",
    "bi_encoder_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Save tokenizer configuration\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'TFBertModel': TFBertModel}\n",
    "\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    # Muat model bi-encoder yang sudah dilatih\n",
    "    bi_encoder_student_chatgpt = tf.keras.models.load_model('saved_models_v2/bi_encoder_student_chatgpt.h5')\n",
    "    bi_encoder_only_chatgpt = tf.keras.models.load_model('saved_models_v2/bi_encoder_only_chatgpt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi tokenizer IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('saved_models_v2/tokenizer')\n",
    "\n",
    "# Fungsi untuk tokenisasi teks\n",
    "def tokenize_text(texts, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi data\n",
    "print(\"Tokenisasi kalimat Student...\")\n",
    "student_tokens = tokenize_text(student_sentences)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_tokens_1 = tokenize_text(chatgpt_sentences_1)\n",
    "\n",
    "print(\"Tokenisasi kalimat ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_tokens_2 = tokenize_text(chatgpt_sentences_2)\n",
    "\n",
    "# Tampilkan hasil tokenisasi (contoh: Student)\n",
    "print(\"\\nContoh hasil tokenisasi:\")\n",
    "print(student_tokens['input_ids'][:1])  # Input token ID\n",
    "print(student_tokens['attention_mask'][:1])  # Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings untuk semua data\n",
    "print(\"Generating embeddings for Student...\")\n",
    "student_embeddings_1 = generate_embeddings(student_tokens, bi_encoder_student_chatgpt)\n",
    "print(\"Generating embeddings for ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_embeddings_1 = generate_embeddings(chatgpt_tokens_1, bi_encoder_student_chatgpt)\n",
    "print(\"Generating embeddings for ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_embeddings_2 = generate_embeddings(chatgpt_tokens_2, bi_encoder_only_chatgpt)\n",
    "\n",
    "# Cari jumlah pasangan yang paling sedikit\n",
    "min_pairs = min(\n",
    "    student_embeddings_1.shape[0],\n",
    "    chatgpt_embeddings_1.shape[0],\n",
    "    chatgpt_embeddings_2.shape[0]\n",
    ")\n",
    "\n",
    "# Cari jumlah pasangan yang paling sedikit\n",
    "min_pairs = min(\n",
    "    student_embeddings_1.shape[0],\n",
    "    chatgpt_embeddings_1.shape[0],\n",
    "    chatgpt_embeddings_2.shape[0]\n",
    ")\n",
    "\n",
    "# Pilih secara acak untuk keadilan\n",
    "np.random.seed(42)\n",
    "random_indices_student = np.random.choice(student_embeddings_1.shape[0], size=min_pairs, replace=False)\n",
    "random_indices_chatgpt1 = np.random.choice(chatgpt_embeddings_1.shape[0], size=min_pairs, replace=False)\n",
    "random_indices_chatgpt2 = np.random.choice(chatgpt_embeddings_2.shape[0], size=min_pairs, replace=False)\n",
    "\n",
    "# Convert NumPy indices to TensorFlow tensors\n",
    "random_indices_student_tf = tf.constant(random_indices_student, dtype=tf.int32)\n",
    "random_indices_chatgpt1_tf = tf.constant(random_indices_chatgpt1, dtype=tf.int32)\n",
    "random_indices_chatgpt2_tf = tf.constant(random_indices_chatgpt2, dtype=tf.int32)\n",
    "\n",
    "# Potong array agar jumlah pasangan berimbang - using tf.gather\n",
    "student_emb1_balanced = tf.gather(student_embeddings_1, random_indices_student_tf)\n",
    "chatgpt1_emb1_balanced = tf.gather(chatgpt_embeddings_1, random_indices_chatgpt1_tf)\n",
    "chatgpt2_emb2_balanced = tf.gather(chatgpt_embeddings_2, random_indices_chatgpt2_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Similarity dengan Student (dari model Student_ChatGPT)\n",
    "student_student_sim_scores = []\n",
    "student_gpt1_sim_scores = []\n",
    "student_gpt2_sim_scores = []\n",
    "for emb in student_embeddings_1:\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), student_embeddings_1)\n",
    "    student_student_sim_scores.append(max_similarity)\n",
    "\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_1)\n",
    "    student_gpt1_sim_scores.append(max_similarity)\n",
    "\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_2)\n",
    "    student_gpt2_sim_scores.append(max_similarity)\n",
    "\n",
    "\n",
    "# 2. Similarity dengan ChatGPT (dari model Student_ChatGPT)\n",
    "gpt1_student_sim_scores = []\n",
    "gpt1_gpt1_sim_scores = []\n",
    "gpt1_gpt2_sim_scores = []\n",
    "for emb in chatgpt_embeddings_1:\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), student_embeddings_1)\n",
    "    gpt1_student_sim_scores.append(max_similarity)\n",
    "\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_1)\n",
    "    gpt1_gpt1_sim_scores.append(max_similarity)\n",
    "\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_2)\n",
    "    gpt1_gpt2_sim_scores.append(max_similarity)\n",
    "\n",
    "# 3. Similarity dengan ChatGPT Knowledge (dari model Only_ChatGPT)\n",
    "gpt2_student_sim_scores = []\n",
    "gpt2_gpt1_sim_scores = []\n",
    "gpt2_gpt2_sim_scores =[]\n",
    "desired_count = len(student_embeddings_1)\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(chatgpt_embeddings_2), size=desired_count, replace=False)\n",
    "for idx in random_indices:\n",
    "    emb = chatgpt_embeddings_2[idx]\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), student_embeddings_1)\n",
    "    gpt2_student_sim_scores.append(max_similarity)\n",
    "\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_1)\n",
    "    gpt2_gpt1_sim_scores.append(max_similarity)\n",
    "\n",
    "    max_similarity = compute_similarity_max(tf.expand_dims(emb, 0), chatgpt_embeddings_2)\n",
    "    gpt2_gpt2_sim_scores.append(max_similarity)\n",
    "\n",
    "# Konversi ke numpy arrays\n",
    "student_similarity_scores = np.array([student_student_sim_scores, student_gpt1_sim_scores, student_gpt2_sim_scores])\n",
    "chatgpt1_similarity_scores = np.array([gpt1_student_sim_scores, gpt1_gpt1_sim_scores, gpt1_gpt2_sim_scores])\n",
    "chatgpt2_similarity_scores = np.array([gpt2_student_sim_scores, gpt2_gpt1_sim_scores, gpt2_gpt2_sim_scores])\n",
    "\n",
    "# Hitung jumlah similarity scores untuk masing-masing kategori\n",
    "num_student_scores = student_similarity_scores.size\n",
    "num_chatgpt1_scores = chatgpt1_similarity_scores.size\n",
    "num_chatgpt2_scores = chatgpt2_similarity_scores.size\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Jumlah similarity scores untuk Student (balanced): {num_student_scores}\")\n",
    "print(f\"Jumlah similarity scores untuk ChatGPT1(balanced): {num_chatgpt1_scores}\")\n",
    "print(f\"Jumlah similarity scores untuk ChatGPT2(balanced): {num_chatgpt2_scores}\")\n",
    "\n",
    "# Cari jumlah pasangan yang paling sedikit\n",
    "min_pairs = min(\n",
    "    student_similarity_scores.shape[1],\n",
    "    chatgpt1_similarity_scores.shape[1],\n",
    "    chatgpt2_similarity_scores.shape[1]\n",
    ")\n",
    "\n",
    "# Pilih secara acak untuk keadilan\n",
    "np.random.seed(42)\n",
    "random_indices_student = np.random.choice(student_similarity_scores.shape[1], size=min_pairs, replace=False)\n",
    "random_indices_chatgpt1 = np.random.choice(chatgpt1_similarity_scores.shape[1], size=min_pairs, replace=False)\n",
    "random_indices_chatgpt2 = np.random.choice(chatgpt2_similarity_scores.shape[1], size=min_pairs, replace=False)\n",
    "\n",
    "# Potong array agar jumlah pasangan berimbang\n",
    "student_similarity_scores_balanced = student_similarity_scores[:, random_indices_student]\n",
    "chatgpt1_similarity_scores_balanced = chatgpt1_similarity_scores[:, random_indices_chatgpt1]\n",
    "chatgpt2_similarity_scores_balanced = chatgpt2_similarity_scores[:, random_indices_chatgpt2]\n",
    "# Tampilkan hasil\n",
    "print(f\"Jumlah pasangan setelah penyeimbangan: {min_pairs}\")\n",
    "\n",
    "# Hitung jumlah similarity scores untuk masing-masing kategori\n",
    "num_student_scores_balanced = student_similarity_scores_balanced.size\n",
    "num_chatgpt1_scores_balanced = chatgpt1_similarity_scores_balanced.size\n",
    "num_chatgpt2_scores_balanced = chatgpt2_similarity_scores_balanced.size\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Jumlah similarity scores untuk Student (balanced): {num_student_scores_balanced}\")\n",
    "print(f\"Jumlah similarity scores untuk ChatGPT1(balanced): {num_chatgpt1_scores_balanced}\")\n",
    "print(f\"Jumlah similarity scores untuk ChatGPT2(balanced): {num_chatgpt2_scores_balanced}\")\n",
    "\n",
    "# Simpan similarity scores untuk digunakan dalam klasifikasi\n",
    "reference_embeddings_1 = {\n",
    "    'student': {\n",
    "        'embeddings': student_embeddings_1.numpy(),\n",
    "        'similarity_scores': student_similarity_scores_balanced\n",
    "    },\n",
    "    'chatgpt': {\n",
    "        'embeddings': chatgpt_embeddings_1.numpy(),\n",
    "        'similarity_scores': chatgpt1_similarity_scores_balanced\n",
    "    }\n",
    "}\n",
    "\n",
    "reference_embeddings_2 = {\n",
    "    'chatgpt_knowledge': {\n",
    "        'embeddings': chatgpt_embeddings_2.numpy(),\n",
    "        'similarity_scores': chatgpt2_similarity_scores_balanced\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_models/reference_embeddings_1.pkl', 'wb') as f:\n",
    "    pickle.dump(reference_embeddings_1, f)\n",
    "\n",
    "with open('saved_models/reference_embeddings_2.pkl', 'wb') as f:\n",
    "    pickle.dump(reference_embeddings_2, f)\n",
    "\n",
    "print(\"Reference embeddings saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur stylometric dari teks:\n",
    "    - Panjang kata rata-rata\n",
    "    - Rasio kata unik\n",
    "    - Rasio tanda baca\n",
    "    - Panjang kalimat\n",
    "    - Variasi panjang kata\n",
    "    - Rasio kata fungsi\n",
    "    - Jumlah koma\n",
    "    - Jumlah titik\n",
    "\n",
    "    Args:\n",
    "        text (str): Input teks.\n",
    "\n",
    "    Returns:\n",
    "        dict: Fitur stylometric.\n",
    "    \"\"\"\n",
    "    # Tokenisasi kata dan kalimat\n",
    "    words = re.findall(r'\\b\\w+\\b', text)  # Token kata\n",
    "    sentences = re.split(r'[.!?]', text)  # Token kalimat\n",
    "\n",
    "    # Basic features\n",
    "    n_chars = len(text)\n",
    "    n_words = len(words)\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if n_words > 0 else 0\n",
    "    unique_word_ratio = len(set(words)) / n_words if n_words > 0 else 0\n",
    "\n",
    "    # Syntactic features\n",
    "    punctuation_count = len(re.findall(r'[.,!?;:]', text))\n",
    "    punctuation_ratio = punctuation_count / n_chars if n_chars > 0 else 0\n",
    "\n",
    "    # Variasi panjang kata\n",
    "    word_length_std = np.std([len(word) for word in words]) if n_words > 0 else 0\n",
    "\n",
    "    # Rasio kata fungsi\n",
    "    function_words = {'a','ada','adalah','adanya','adapun','agak','agaknya','agar','akan','akankah','akhir',\n",
    "            'akhiri','akhirnya','aku','akulah','amat','amatlah','anda','andalah','antar','antara',\n",
    "            'antaranya','apa','apaan','apabila','apakah','apalagi','apatah','arti','artinya','asal',\n",
    "            'asalkan','atas','atau','ataukah','ataupun','awal','awalnya','b','bagai','bagaikan',\n",
    "            'bagaimana','bagaimanakah','bagaimanapun','bagainamakah','bagi','bagian','bahkan','bahwa',\n",
    "            'bahwasannya','bahwasanya','baik','baiklah','bakal','bakalan','balik','banyak','bapak',\n",
    "            'baru','bawah','beberapa','begini','beginian','beginikah','beginilah','begitu','begitukah',\n",
    "            'begitulah','begitupun','bekerja','belakang','belakangan','belum','belumlah','benar',\n",
    "            'benarkah','benarlah','berada','berakhir','berakhirlah','berakhirnya','berapa','berapakah',\n",
    "            'berapalah','berapapun','berarti','berawal','berbagai','berdatangan','beri','berikan',\n",
    "            'berikut','berikutnya','berjumlah','berkali-kali','berkata','berkehendak','berkeinginan',\n",
    "            'berkenaan','berlainan','berlalu','berlangsung','berlebihan','bermacam','bermacam-macam',\n",
    "            'bermaksud','bermula','bersama','bersama-sama','bersiap','bersiap-siap','bertanya',\n",
    "            'bertanya-tanya','berturut','berturut-turut','bertutur','berujar','berupa','besar',\n",
    "            'betul','betulkah','biasa','biasanya','bila','bilakah','bisa','bisakah','boleh','bolehkah',\n",
    "            'bolehlah','buat','bukan','bukankah','bukanlah','bukannya','bulan','bung','c','cara',\n",
    "            'caranya','cukup','cukupkah','cukuplah','cuma','d','dahulu','dalam','dan','dapat','dari',\n",
    "            'daripada','datang','dekat','demi','demikian','demikianlah','dengan','depan','di','dia',\n",
    "            'diakhiri','diakhirinya','dialah','diantara','diantaranya','diberi','diberikan','diberikannya',\n",
    "            'dibuat','dibuatnya','didapat','didatangkan','digunakan','diibaratkan','diibaratkannya',\n",
    "            'diingat','diingatkan','diinginkan','dijawab','dijelaskan','dijelaskannya','dikarenakan',\n",
    "            'dikatakan','dikatakannya','dikerjakan','diketahui','diketahuinya','dikira','dilakukan',\n",
    "            'dilalui','dilihat','dimaksud','dimaksudkan','dimaksudkannya','dimaksudnya','diminta',\n",
    "            'dimintai','dimisalkan','dimulai','dimulailah','dimulainya','dimungkinkan','dini','dipastikan',\n",
    "            'diperbuat','diperbuatnya','dipergunakan','diperkirakan','diperlihatkan','diperlukan',\n",
    "            'diperlukannya','dipersoalkan','dipertanyakan','dipunyai','diri','dirinya','disampaikan',\n",
    "            'disebut','disebutkan','disebutkannya','disini','disinilah','ditambahkan','ditandaskan',\n",
    "            'ditanya','ditanyai','ditanyakan','ditegaskan','ditujukan','ditunjuk','ditunjuki','ditunjukkan',\n",
    "            'ditunjukkannya','ditunjuknya','dituturkan','dituturkannya','diucapkan','diucapkannya',\n",
    "            'diungkapkan','dong','dua','dulu','e','empat','enak','enggak','enggaknya','entah','entahlah',\n",
    "            'f','g','guna','gunakan','h','hadap','hai','hal','halo','hallo','hampir','hanya','hanyalah',\n",
    "            'hari','harus','haruslah','harusnya','helo','hello','hendak','hendaklah','hendaknya','hingga',\n",
    "            'i','ia','ialah','ibarat','ibaratkan','ibaratnya','ibu','ikut','ingat','ingat-ingat','ingin',\n",
    "            'inginkah','inginkan','ini','inikah','inilah','itu','itukah','itulah','j','jadi','jadilah',\n",
    "            'jadinya','jangan','jangankan','janganlah','jauh','jawab','jawaban','jawabnya','jelas',\n",
    "            'jelaskan','jelaslah','jelasnya','jika','jikalau','juga','jumlah','jumlahnya','justru',\n",
    "            'k','kadar','kala','kalau','kalaulah','kalaupun','kali','kalian','kami','kamilah','kamu',\n",
    "            'kamulah','kan','kapan','kapankah','kapanpun','karena','karenanya','kasus','kata','katakan',\n",
    "            'katakanlah','katanya','ke','keadaan','kebetulan','kecil','kedua','keduanya','keinginan',\n",
    "            'kelamaan','kelihatan','kelihatannya','kelima','keluar','kembali','kemudian','kemungkinan',\n",
    "            'kemungkinannya','kena','kenapa','kepada','kepadanya','kerja','kesampaian','keseluruhan',\n",
    "            'keseluruhannya','keterlaluan','ketika','khusus','khususnya','kini','kinilah','kira',\n",
    "            'kira-kira','kiranya','kita','kitalah','kok','kurang','l','lagi','lagian','lah','lain',\n",
    "            'lainnya','laku','lalu','lama','lamanya','langsung','lanjut','lanjutnya','lebih','lewat',\n",
    "            'lihat','lima','luar','m','macam','maka','makanya','makin','maksud','malah','malahan',\n",
    "            'mampu','mampukah','mana','manakala','manalagi','masa','masalah','masalahnya','masih',\n",
    "            'masihkah','masing','masing-masing','masuk','mata','mau','maupun','melainkan','melakukan',\n",
    "            'melalui','melihat','melihatnya','memang','memastikan','memberi','memberikan','membuat',\n",
    "            'memerlukan','memihak','meminta','memintakan','memisalkan','memperbuat','mempergunakan',\n",
    "            'memperkirakan','memperlihatkan','mempersiapkan','mempersoalkan','mempertanyakan','mempunyai',\n",
    "            'memulai','memungkinkan','menaiki','menambahkan','menandaskan','menanti','menanti-nanti',\n",
    "            'menantikan','menanya','menanyai','menanyakan','mendapat','mendapatkan','mendatang','mendatangi',\n",
    "            'mendatangkan','menegaskan','mengakhiri','mengapa','mengatakan','mengatakannya','mengenai',\n",
    "            'mengerjakan','mengetahui','menggunakan','menghendaki','mengibaratkan','mengibaratkannya',\n",
    "            'mengingat','mengingatkan','menginginkan','mengira','mengucapkan','mengucapkannya','mengungkapkan',\n",
    "            'menjadi','menjawab','menjelaskan','menuju','menunjuk','menunjuki','menunjukkan','menunjuknya',\n",
    "            'menurut','menuturkan','menyampaikan','menyangkut','menyatakan','menyebutkan','menyeluruh',\n",
    "            'menyiapkan','merasa','mereka','merekalah','merupakan','meski','meskipun','meyakini','meyakinkan',\n",
    "            'minta','mirip','misal','misalkan','misalnya','mohon','mula','mulai','mulailah','mulanya','mungkin',\n",
    "            'mungkinkah','n','nah','naik','namun','nanti','nantinya','nya','nyaris','nyata','nyatanya',\n",
    "            'o','oleh','olehnya','orang','p','pada','padahal','padanya','pak','paling','panjang','pantas',\n",
    "            'para','pasti','pastilah','penting','pentingnya','per','percuma','perlu','perlukah','perlunya',\n",
    "            'pernah','persoalan','pertama','pertama-tama','pertanyaan','pertanyakan','pihak','pihaknya',\n",
    "            'pukul','pula','pun','punya','q','r','rasa','rasanya','rupa','rupanya','s','saat','saatnya','saja',\n",
    "            'sajalah','salam','saling','sama','sama-sama','sambil','sampai','sampai-sampai','sampaikan','sana',\n",
    "            'sangat','sangatlah','sangkut','satu','saya','sayalah','se','sebab','sebabnya','sebagai',\n",
    "            'sebagaimana','sebagainya','sebagian','sebaik','sebaik-baiknya','sebaiknya','sebaliknya',\n",
    "            'sebanyak','sebegini','sebegitu','sebelum','sebelumnya','sebenarnya','seberapa','sebesar',\n",
    "            'sebetulnya','sebisanya','sebuah','sebut','sebutlah','sebutnya','secara','secukupnya','sedang',\n",
    "            'sedangkan','sedemikian','sedikit','sedikitnya','seenaknya','segala','segalanya','segera',\n",
    "            'seharusnya','sehingga','seingat','sejak','sejauh','sejenak','sejumlah','sekadar','sekadarnya',\n",
    "            'sekali','sekali-kali','sekalian','sekaligus','sekalipun','sekarang','sekaranglah','sekecil',\n",
    "            'seketika','sekiranya','sekitar','sekitarnya','sekurang-kurangnya','sekurangnya','sela','selain',\n",
    "            'selaku','selalu','selama','selama-lamanya','selamanya','selanjutnya','seluruh','seluruhnya',\n",
    "            'semacam','semakin','semampu','semampunya','semasa','semasih','semata','semata-mata','semaunya',\n",
    "            'sementara','semisal','semisalnya','sempat','semua','semuanya','semula','sendiri','sendirian',\n",
    "            'sendirinya','seolah','seolah-olah','seorang','sepanjang','sepantasnya','sepantasnyalah',\n",
    "            'seperlunya','seperti','sepertinya','sepihak','sering','seringnya','serta','serupa','sesaat',\n",
    "            'sesama','sesampai','sesegera','sesekali','seseorang','sesuatu','sesuatunya','sesudah',\n",
    "            'sesudahnya','setelah','setempat','setengah','seterusnya','setiap','setiba','setibanya',\n",
    "            'setidak-tidaknya','setidaknya','setinggi','seusai','sewaktu','siap','siapa','siapakah',\n",
    "            'siapapun','sini','sinilah','soal','soalnya','suatu','sudah','sudahkah','sudahlah','supaya',\n",
    "            't','tadi','tadinya','tahu','tak','tambah','tambahnya','tampak','tampaknya','tandas','tandasnya',\n",
    "            'tanpa','tanya','tanyakan','tanyanya','tapi','tegas','tegasnya','telah','tempat','tentang','tentu',\n",
    "            'tentulah','tentunya','tepat','terakhir','terasa','terbanyak','terdahulu','terdapat','terdiri',\n",
    "            'terhadap','terhadapnya','teringat','teringat-ingat','terjadi','terjadilah','terjadinya','terkira',\n",
    "            'terlalu','terlebih','terlihat','termasuk','ternyata','tersampaikan','tersebut','tersebutlah',\n",
    "            'tertentu','tertuju','terus','terutama','tetap','tetapi','tiap','tiba','tiba-tiba','tidak',\n",
    "            'tidakkah','tidaklah','tiga','toh','tuju','tunjuk','turut','tutur','tuturnya','u','ucap','ucapnya',\n",
    "            'ujar','ujarnya','umumnya','ungkap','ungkapnya','untuk','usah','usai','v','w','waduh','wah','wahai',\n",
    "            'waktunya','walau','walaupun','wong','x','y','ya','yaitu','yakin','yakni','yang','z'}\n",
    "    function_word_count = sum(1 for word in words if word.lower() in function_words)\n",
    "    function_word_ratio = function_word_count / n_words if n_words > 0 else 0\n",
    "\n",
    "    # Distribusi tanda baca\n",
    "    punctuation_distribution = Counter(re.findall(r'[.,!?;:]', text))\n",
    "    comma_count = punctuation_distribution.get(',', 0)\n",
    "    period_count = punctuation_distribution.get('.', 0)\n",
    "\n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'sentence_length': n_words,\n",
    "        'word_length_std': word_length_std,\n",
    "        'function_word_ratio': function_word_ratio,\n",
    "        'comma_count': comma_count,\n",
    "        'period_count': period_count,\n",
    "    }\n",
    "\n",
    "min_sentences = min(len(student_sentences), len(chatgpt_sentences_1), len(chatgpt_sentences_2))\n",
    "\n",
    "random_indices = random.sample(range(min_sentences), min_sentences)\n",
    "\n",
    "student_sentences = [student_sentences[i] for i in random_indices]\n",
    "chatgpt_sentences_1 = [chatgpt_sentences_1[i] for i in random_indices]\n",
    "chatgpt_sentences_2 = [chatgpt_sentences_2[i] for i in random_indices]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk Student...\")\n",
    "student_features = [extract_stylometric_features(text) for text in student_sentences]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Student_ChatGPT)...\")\n",
    "chatgpt_features_1 = [extract_stylometric_features(text) for text in chatgpt_sentences_1]\n",
    "\n",
    "print(\"Ekstraksi fitur stylometric untuk ChatGPT (Only_ChatGPT)...\")\n",
    "chatgpt_features_2 = [extract_stylometric_features(text) for text in chatgpt_sentences_2]\n",
    "\n",
    "# Konversi fitur ke DataFrame\n",
    "student_features_df = pd.DataFrame(student_features)\n",
    "chatgpt_features_1_df = pd.DataFrame(chatgpt_features_1)\n",
    "chatgpt_features_2_df = pd.DataFrame(chatgpt_features_2)\n",
    "\n",
    "# Tampilkan beberapa fitur hasil ekstraksi\n",
    "print(\"\\nFitur Stylometric Student:\")\n",
    "display(student_features_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Student_ChatGPT):\")\n",
    "display(chatgpt_features_1_df.head())\n",
    "\n",
    "print(\"\\nFitur Stylometric ChatGPT (Only_ChatGPT):\")\n",
    "display(chatgpt_features_2_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua fitur untuk normalisasi\n",
    "all_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)\n",
    "\n",
    "# Normalisasi fitur menggunakan StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(all_features)\n",
    "\n",
    "# Pisahkan kembali fitur yang telah dinormalisasi\n",
    "n_student = len(student_features_df)\n",
    "n_chatgpt_1 = len(chatgpt_features_1_df)\n",
    "\n",
    "student_features_normalized = normalized_features[:n_student]\n",
    "chatgpt_features_1_normalized = normalized_features[n_student:n_student + n_chatgpt_1]\n",
    "chatgpt_features_2_normalized = normalized_features[n_student + n_chatgpt_1:]\n",
    "\n",
    "print(\"Fitur Student setelah normalisasi:\")\n",
    "print(student_features_normalized[:5])\n",
    "\n",
    "# Simpan scaler untuk inference nanti\n",
    "with open('scaler_stylometric.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambahkan label pada dataset untuk visualisasi\n",
    "student_features_df['label'] = 'Esai Siswa'\n",
    "chatgpt_features_1_df['label'] = 'Esai ChatGPT'\n",
    "chatgpt_features_2_df['label'] = 'Pengetahuan ChatGPT'\n",
    "\n",
    "# Gabungkan dataset\n",
    "combined_features = pd.concat([student_features_df, chatgpt_features_1_df, chatgpt_features_2_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "for i, feature in enumerate([\n",
    "    'avg_word_length', \n",
    "    'unique_word_ratio', \n",
    "    'punctuation_ratio', \n",
    "    'sentence_length', \n",
    "    'word_length_std',  \n",
    "    'function_word_ratio', \n",
    "    'comma_count', \n",
    "    'period_count'\n",
    "]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(x='label', y=feature, data=combined_features)\n",
    "    plt.title(f'Distribusi {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.violinplot(x='label', y='avg_word_length', data=combined_features, inner='box')\n",
    "plt.title('Distribusi Panjang Kata Rata-Rata')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(combined_features, hue='label', vars=['avg_word_length', \n",
    "    'unique_word_ratio', \n",
    "    'punctuation_ratio', \n",
    "    'sentence_length', \n",
    "    'word_length_std', \n",
    "    'function_word_ratio', \n",
    "    'comma_count', \n",
    "    'period_count'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Membuat Model BERT Single Sentence Classifier\n",
    "\n",
    "Model classifier dibuat untuk membedakan teks berdasarkan tiga input:\n",
    "1. Embeddings BERT: Representasi teks dari Bi-Encoder.\n",
    "2. Fitur Stylometric: Representasi linguistik tambahan.\n",
    "3. Similarity Score: Skor kesamaan embeddings.\n",
    "\n",
    "Arsitektur classifier:\n",
    "- Tiga input terpisah dihubungkan melalui dense layers.\n",
    "- Output berupa probabilitas apakah teks berasal dari Student atau ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persiapkan similarity scores untuk input model\n",
    "# Gabungkan similarity scores dari ketiga model\n",
    "student_similarity_combined = np.column_stack([\n",
    "    student_similarity_scores,\n",
    "    np.zeros_like(student_similarity_scores),  # Placeholder untuk ChatGPT1\n",
    "])\n",
    "\n",
    "chatgpt1_similarity_combined = np.column_stack([\n",
    "    np.zeros_like(chatgpt1_similarity_scores),  # Placeholder untuk Student\n",
    "    chatgpt1_similarity_scores,\n",
    "])\n",
    "\n",
    "chatgpt2_similarity_combined = np.column_stack([\n",
    "    np.zeros_like(chatgpt2_similarity_scores),  # Placeholder untuk Student/ChatGPT\n",
    "    chatgpt2_similarity_scores[:len(chatgpt1_similarity_scores)]  # Ambil sebanyak data ChatGPT1\n",
    "])\n",
    "\n",
    "# Input layers untuk tiga jenis fitur\n",
    "bert_embedding_input_1 = tf.keras.layers.Input(\n",
    "    shape=(128,),  # Shape embeddings (diambil dari output model bi-encoder Student_ChatGPT)\n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding_1\"\n",
    ")\n",
    "bert_embedding_input_2 = tf.keras.layers.Input(\n",
    "    shape=(128,),  # Shape embeddings (diambil dari output model bi-encoder Only_ChatGPT)\n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding_2\"\n",
    ")\n",
    "stylometric_input = tf.keras.layers.Input(\n",
    "    shape=(8,),  # Shape jumlah fitur stylometric\n",
    "    dtype=tf.float32, \n",
    "    name=\"stylometric_features\"\n",
    ")\n",
    "similarity_score_input = tf.keras.layers.Input(\n",
    "    shape=(3,),  # Shape score similarity (Student, ChatGPT1, ChatGPT2)\n",
    "    dtype=tf.float32, \n",
    "    name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "# Dense layer untuk masing-masing input\n",
    "bert_dense_1 = tf.keras.layers.Dense(128, activation=\"relu\")(bert_embedding_input_1)\n",
    "bert_dense_2 = tf.keras.layers.Dense(128, activation=\"relu\")(bert_embedding_input_2)\n",
    "style_dense = tf.keras.layers.Dense(64, activation=\"relu\")(stylometric_input)\n",
    "sim_dense = tf.keras.layers.Dense(16, activation=\"relu\")(similarity_score_input)\n",
    "\n",
    "# Gabungkan semua fitur\n",
    "combined = tf.keras.layers.Concatenate()([bert_dense_1, bert_dense_2, style_dense, sim_dense])\n",
    "\n",
    "# Dense layers untuk klasifikasi\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(combined)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Buat model classifier\n",
    "classifier = tf.keras.Model(\n",
    "    inputs=[bert_embedding_input_1, bert_embedding_input_2, stylometric_input, similarity_score_input],\n",
    "    outputs=output,\n",
    "    name=\"text_classifier\"\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "# Tampilkan arsitektur model\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"student_similarity_scores shape: {student_similarity_scores_balanced.shape}\")\n",
    "print(f\"chatgpt1_similarity_scores shape: {chatgpt1_similarity_scores_balanced.shape}\")\n",
    "print(f\"chatgpt2_similarity_scores shape: {chatgpt2_similarity_scores_balanced.shape}\")\n",
    "print(f\"student_embeddings_1 shape: {student_emb1_balanced.shape}\")\n",
    "print(f\"chatgpt_embeddings_1 shape: {chatgpt1_emb1_balanced.shape}\")\n",
    "print(f\"chatgpt_embeddings_2 shape: {chatgpt2_emb2_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine embeddings for model 1 (already correct)\n",
    "embeddings_model_1 = np.vstack([student_emb1_balanced.numpy(), chatgpt1_emb1_balanced.numpy()])\n",
    "embeddings_model_2 = np.vstack([student_emb1_balanced.numpy(), chatgpt1_emb1_balanced.numpy()])\n",
    "\n",
    "student_features_selected = student_features_normalized\n",
    "chatgpt_features_selected = chatgpt_features_1_normalized\n",
    "\n",
    "all_stylometric_features = np.vstack([\n",
    "    student_features_selected,\n",
    "    chatgpt_features_selected\n",
    "])\n",
    "\n",
    "all_similarity_scores = np.vstack([\n",
    "    student_similarity_scores_balanced,\n",
    "    chatgpt1_similarity_scores_balanced,\n",
    "])\n",
    "all_similarity_scores = all_similarity_scores.T.reshape(len(embeddings_model_1), 3)\n",
    "\n",
    "# Create labels\n",
    "student_labels = np.zeros(len(student_embeddings_1))  # Label 0 for Student\n",
    "chatgpt_labels = np.ones(len(chatgpt1_emb1_balanced))  # Label 1 for ChatGPT\n",
    "all_labels = np.hstack([student_labels, chatgpt_labels])\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"embeddings_model_1 shape: {embeddings_model_1.shape}\")\n",
    "print(f\"embeddings_model_2 shape: {embeddings_model_2.shape}\")\n",
    "print(f\"all_stylometric_features shape: {all_stylometric_features.shape}\")\n",
    "print(f\"all_similarity_scores shape: {all_similarity_scores.shape}\")\n",
    "print(f\"all_labels shape: {all_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Membuat Data Training\n",
    "\n",
    "Data untuk training dan validation dibuat dengan menggabungkan:\n",
    "- Embeddings dari Bi-Encoder.\n",
    "- Fitur stylometric yang telah dinormalisasi.\n",
    "- Similarity scores.\n",
    "- Label: 0 untuk Student dan 1 untuk ChatGPT.\n",
    "\n",
    "Dataset kemudian dibagi menjadi training dan validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(all_labels)),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.33,  # 10% dari total (0.33 * 0.3 = 0.1)\n",
    "    random_state=42,\n",
    "    stratify=all_labels[temp_idx]\n",
    ")\n",
    "\n",
    "# Prepare inputs for training, validation, and test\n",
    "train_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[train_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[train_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[train_idx],\n",
    "    \"similarity_score\": all_similarity_scores[train_idx]\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[val_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[val_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[val_idx],\n",
    "    \"similarity_score\": all_similarity_scores[val_idx]\n",
    "}\n",
    "\n",
    "test_inputs = {\n",
    "    \"bert_embedding_1\": embeddings_model_1[test_idx],\n",
    "    \"bert_embedding_2\": embeddings_model_2[test_idx],\n",
    "    \"stylometric_features\": all_stylometric_features[test_idx],\n",
    "    \"similarity_score\": all_similarity_scores[test_idx]\n",
    "}\n",
    "\n",
    "train_labels = all_labels[train_idx]\n",
    "val_labels = all_labels[val_idx]\n",
    "test_labels = all_labels[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Train Model Classifier\n",
    "\n",
    "Model classifier dilatih menggunakan dataset yang telah disiapkan. `Callback EarlyStopping` digunakan untuk menghentikan pelatihan jika performa tidak meningkat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifier\n",
    "print(\"Training Text Classifier...\")\n",
    "history_classifier = classifier.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_classifier.history['loss'], label='Training Loss')\n",
    "plt.plot(history_classifier.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_classifier.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_classifier.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Evaluasi Model\n",
    "\n",
    "Model dievaluasi pada dataset validation:\n",
    "- **Confusion Matrix**: Menampilkan jumlah prediksi benar dan salah untuk masing-masing label.\n",
    "- **Classification Report**: Menampilkan metrik seperti precision, recall, F1-score, dan accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Evaluasi model pada validation set\n",
    "val_loss, val_acc, val_precision, val_recall, val_auc = classifier.evaluate(val_inputs, val_labels)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "# Prediksi pada validation set\n",
    "val_predictions = classifier.predict(val_inputs)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_predictions_binary, target_names=['Student', 'ChatGPT']))\n",
    "\n",
    "# Evaluasi model pada test set\n",
    "test_loss, test_acc, test_precision, test_recall, test_auc = classifier.evaluate(test_inputs, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Prediksi pada test set\n",
    "test_predictions = classifier.predict(test_inputs)\n",
    "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix untuk test set\n",
    "cm_test = confusion_matrix(test_labels, test_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix untuk test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=['Student', 'ChatGPT'], yticklabels=['Student', 'ChatGPT'])\n",
    "plt.title('Test Set Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report untuk test set\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions_binary, target_names=['Student', 'ChatGPT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fungsi untuk plot ROC curve\n",
    "def plot_roc_curve(labels, predictions, title):\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {title}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "# Plot ROC curve untuk validation set\n",
    "val_auc_score = plot_roc_curve(val_labels, val_predictions, \"Validation Set\")\n",
    "print(f\"Validation AUC from ROC curve: {val_auc_score:.4f}\")\n",
    "\n",
    "# Plot ROC curve untuk test set\n",
    "test_auc_score = plot_roc_curve(test_labels, test_predictions, \"Test Set\")\n",
    "print(f\"Test AUC from ROC curve: {test_auc_score:.4f}\")\n",
    "\n",
    "# Plot kedua kurva ROC dalam satu grafik untuk perbandingan\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Validation set\n",
    "fpr_val, tpr_val, _ = roc_curve(val_labels, val_predictions)\n",
    "roc_auc_val = auc(fpr_val, tpr_val)\n",
    "plt.plot(fpr_val, tpr_val, color='darkorange', lw=2, label=f'Validation ROC (area = {roc_auc_val:.2f})')\n",
    "\n",
    "# Test set\n",
    "fpr_test, tpr_test, _ = roc_curve(test_labels, test_predictions)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "plt.plot(fpr_test, tpr_test, color='green', lw=2, label=f'Test ROC (area = {roc_auc_test:.2f})')\n",
    "\n",
    "# Garis diagonal\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Simpan Model Classifier\n",
    "\n",
    "Model classifier yang telah dilatih disimpan bersama dengan konfigurasi tokenizer dan scaler untuk inference di masa depan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat direktori jika belum ada\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model bi-encoder untuk Student_ChatGPT\n",
    "bi_encoder_student_chatgpt.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model bi-encoder untuk Only_ChatGPT\n",
    "bi_encoder_only_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Simpan model classifier\n",
    "classifier.save('saved_models/text_classifier.h5')\n",
    "\n",
    "# Simpan konfigurasi tokenizer\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "# Simpan scaler untuk fitur stylometric\n",
    "with open(\"saved_models/scaler_stylometric.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model dan konfigurasi berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Prediction Inference\n",
    "\n",
    "Fungsi inference dibuat untuk memprediksi apakah sebuah teks berasal dari Student atau ChatGPT:\n",
    "- Teks di-preprocess dan di-tokenisasi.\n",
    "- Embeddings dihasilkan oleh Bi-Encoder.\n",
    "- Fitur stylometric diekstraksi.\n",
    "- Model classifier menghasilkan prediksi probabilitas.\n",
    "\n",
    "Contoh hasil prediksi menampilkan label dan confidence score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
