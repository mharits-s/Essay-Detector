{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set seed untuk reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_csv(\"student_chatgpt.csv\")\n",
    "only_chatgpt = pd.read_csv(\"only_chatgpt.csv\")\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Student-ChatGPT Dataset Info:\")\n",
    "print(f\"Number of rows: {len(student_chatgpt)}\")\n",
    "print(\"\\nSample of student_chatgpt data:\")\n",
    "print(student_chatgpt.head(2))\n",
    "\n",
    "print(\"\\nOnly-ChatGPT Dataset Info:\")\n",
    "print(f\"Number of rows: {len(only_chatgpt)}\")\n",
    "print(\"\\nSample of only_chatgpt data:\")\n",
    "print(only_chatgpt.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase and segment into sentences\"\"\"\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    return sentences\n",
    "\n",
    "# Process student essays\n",
    "student_sentences = []\n",
    "student_labels = []\n",
    "for text in student_chatgpt['student']:\n",
    "    sentences = preprocess_text(text)\n",
    "    student_sentences.extend(sentences)\n",
    "    student_labels.extend([0] * len(sentences))  # 0 for student\n",
    "\n",
    "# Process ChatGPT responses from student_chatgpt dataset\n",
    "chatgpt_sentences_1 = []\n",
    "chatgpt_labels_1 = []\n",
    "for text in student_chatgpt['chatgpt']:\n",
    "    sentences = preprocess_text(text)\n",
    "    chatgpt_sentences_1.extend(sentences)\n",
    "    chatgpt_labels_1.extend([1] * len(sentences))  # 1 for ChatGPT\n",
    "\n",
    "# Process only_chatgpt responses\n",
    "chatgpt_sentences_2 = []\n",
    "chatgpt_labels_2 = []\n",
    "for text in only_chatgpt['chatgpt']:\n",
    "    sentences = preprocess_text(text)\n",
    "    chatgpt_sentences_2.extend(sentences)\n",
    "    chatgpt_labels_2.extend([1] * len(sentences))\n",
    "\n",
    "print(f\"Number of student sentences: {len(student_sentences)}\")\n",
    "print(f\"Number of ChatGPT sentences (from student_chatgpt): {len(chatgpt_sentences_1)}\")\n",
    "print(f\"Number of ChatGPT sentences (from only_chatgpt): {len(chatgpt_sentences_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inisialisasi BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer for IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(texts, max_length=512):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Membuat Model BERT Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base BERT model\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Create Bi-Encoder for student data\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# BERT embeddings\n",
    "bert_outputs = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "cls_token = bert_outputs[:, 0, :]  # Get [CLS] token representation\n",
    "\n",
    "# Dense layers for embedding\n",
    "dense1 = tf.keras.layers.Dense(256, activation=\"relu\")(cls_token)\n",
    "dropout = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "output = tf.keras.layers.Dense(128)(dropout)\n",
    "\n",
    "# Normalize output embedding\n",
    "normalized_output = tf.nn.l2_normalize(output, axis=1)\n",
    "\n",
    "# Create the models\n",
    "bi_encoder_student = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=normalized_output)\n",
    "bi_encoder_chatgpt = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=normalized_output)\n",
    "\n",
    "# Compile the models\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "bi_encoder_student.compile(optimizer=optimizer, loss=tf.keras.losses.CosineSimilarity())\n",
    "bi_encoder_chatgpt.compile(optimizer=optimizer, loss=tf.keras.losses.CosineSimilarity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Bi-Encoder\n",
    "\n",
    "training masi salah logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for student bi-encoder\n",
    "student_train_encodings = tokenize_text(student_sentences)\n",
    "chatgpt1_train_encodings = tokenize_text(chatgpt_sentences_1)\n",
    "chatgpt2_train_encodings = tokenize_text(chatgpt_sentences_2)\n",
    "\n",
    "# Train student bi-encoder\n",
    "print(\"Training Student Bi-Encoder...\")\n",
    "bi_encoder_student.fit(\n",
    "    {\"input_ids\": student_train_encodings['input_ids'],\n",
    "     \"attention_mask\": student_train_encodings['attention_mask']},\n",
    "    np.zeros(len(student_sentences)),  # Dummy target for self-supervised learning\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Train ChatGPT bi-encoder\n",
    "print(\"\\nTraining ChatGPT Bi-Encoder...\")\n",
    "bi_encoder_chatgpt.fit(\n",
    "    {\"input_ids\": tf.concat([chatgpt1_train_encodings['input_ids'], \n",
    "                           chatgpt2_train_encodings['input_ids']], axis=0),\n",
    "     \"attention_mask\": tf.concat([chatgpt1_train_encodings['attention_mask'],\n",
    "                                chatgpt2_train_encodings['attention_mask']], axis=0)},\n",
    "    np.zeros(len(chatgpt_sentences_1) + len(chatgpt_sentences_2)),  # Dummy target\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluasi Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi Bi-Encoder student_chatgpt\n",
    "student_test_embeddings = bi_encoder_student.predict({\"input_ids\": student_test_encodings['input_ids'], \"attention_mask\": student_test_encodings['attention_mask']})\n",
    "print(f\"Shape embeddings student_chatgpt: {student_test_embeddings.shape}\")\n",
    "\n",
    "# Evaluasi Bi-Encoder only_chatgpt\n",
    "chatgpt_test_embeddings = bi_encoder_chatgpt.predict({\"input_ids\": chatgpt_test_encodings['input_ids'], \"attention_mask\": chatgpt_test_encodings['attention_mask']})\n",
    "print(f\"Shape embeddings only_chatgpt: {chatgpt_test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Simpan Model Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model Bi-Encoder untuk dataset student_chatgpt\n",
    "bi_encoder_student.save(\"bi_encoder_student_chatgpt_model\")\n",
    "\n",
    "# Simpan model Bi-Encoder untuk dataset only_chatgpt\n",
    "bi_encoder_chatgpt.save(\"bi_encoder_only_chatgpt_model\")\n",
    "\n",
    "print(\"Model Bi-Encoder telah disimpan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "student_embeddings = bi_encoder_student.predict(student_train_encodings)\n",
    "chatgpt_embeddings = bi_encoder_chatgpt.predict(tf.concat([chatgpt1_train_encodings['input_ids'],\n",
    "                                                         chatgpt2_train_encodings['input_ids']], axis=0))\n",
    "\n",
    "# Function to compute similarity and get top-k\n",
    "def compute_similarity_and_top_k(embeddings1, embeddings2, k=50):\n",
    "    normalized_emb1 = tf.nn.l2_normalize(embeddings1, axis=1)\n",
    "    normalized_emb2 = tf.nn.l2_normalize(embeddings2, axis=1)\n",
    "    similarity_matrix = tf.matmul(normalized_emb1, normalized_emb2, transpose_b=True)\n",
    "    return tf.nn.top_k(similarity_matrix, k=k)\n",
    "\n",
    "# Get similar sentences for each input\n",
    "similar_pairs = []\n",
    "similar_labels = []\n",
    "\n",
    "# Process each student sentence\n",
    "for i, student_sent in enumerate(student_sentences):\n",
    "    # Get top-25 similar student sentences\n",
    "    _, student_indices = compute_similarity_and_top_k(\n",
    "        student_embeddings[i:i+1], student_embeddings, k=25)\n",
    "    \n",
    "    # Get top-25 similar ChatGPT sentences\n",
    "    _, chatgpt_indices = compute_similarity_and_top_k(\n",
    "        student_embeddings[i:i+1], chatgpt_embeddings, k=25)\n",
    "    \n",
    "    # Add pairs to training data\n",
    "    for idx in student_indices[0]:\n",
    "        similar_pairs.append((student_sent, student_sentences[idx]))\n",
    "        similar_labels.append(0)\n",
    "    \n",
    "    for idx in chatgpt_indices[0]:\n",
    "        if idx < len(chatgpt_sentences_1):\n",
    "            similar_pairs.append((student_sent, chatgpt_sentences_1[idx]))\n",
    "        else:\n",
    "            similar_pairs.append((student_sent, chatgpt_sentences_2[idx - len(chatgpt_sentences_1)]))\n",
    "        similar_labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Membuat Model BERT Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer untuk Cross-Encoder\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# BERT embeddings\n",
    "bert_outputs = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "cls_token = bert_outputs[:, 0, :]  # Representasi token CLS\n",
    "\n",
    "# Dense layers untuk klasifikasi\n",
    "dense1 = tf.keras.layers.Dense(256, activation=\"relu\")(cls_token)\n",
    "dropout1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "dense2 = tf.keras.layers.Dense(64, activation=\"relu\")(dropout1)\n",
    "dropout2 = tf.keras.layers.Dropout(0.1)(dense2)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout2)\n",
    "\n",
    "# Model Cross-Encoder\n",
    "cross_encoder = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Kompilasi Cross-Encoder\n",
    "cross_encoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Membuat Pasangan Data untuk Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cross-encoder training data\n",
    "cross_encoder_inputs = []\n",
    "for pair in similar_pairs:\n",
    "    encoding = tokenizer(\n",
    "        pair[0],\n",
    "        pair[1],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    cross_encoder_inputs.append({\n",
    "        \"input_ids\": encoding['input_ids'][0],\n",
    "        \"attention_mask\": encoding['attention_mask'][0]\n",
    "    })\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids = tf.stack([x[\"input_ids\"] for x in cross_encoder_inputs])\n",
    "attention_masks = tf.stack([x[\"attention_mask\"] for x in cross_encoder_inputs])\n",
    "labels = tf.convert_to_tensor(similar_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.fit(\n",
    "    {\"input_ids\": input_ids, \"attention_mask\": attention_masks},\n",
    "    labels,\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_encoder.evaluate(\n",
    "    {\"input_ids\": test_encodings['input_ids'], \"attention_mask\": test_encodings['attention_mask']},\n",
    "    y_test\n",
    ")\n",
    "print(f\"Cross-Encoder Accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Simpan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "bi_encoder_student.save(\"bi_encoder_student_model.h5\")\n",
    "bi_encoder_chatgpt.save(\"bi_encoder_chatgpt_model.h5\")\n",
    "cross_encoder.save(\"cross_encoder_model.h5\")\n",
    "\n",
    "print(\"All models have been saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
