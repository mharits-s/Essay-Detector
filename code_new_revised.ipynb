{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Set seed untuk reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Dataset structure should be like this,\n",
    "\n",
    "Dataset Student_ChatGPT:\n",
    "\n",
    "col 1 - Student (comes from 1 to 2 problem topics)\n",
    "\n",
    "col 2 - ChatGPT (same as Student, comes from 1 to 2 problem topics)\n",
    "\n",
    "---\n",
    "\n",
    "Dataset Only_ChatGPT\n",
    "\n",
    "col 1 - ChatGPT (Knowledge of the material for 1 year or 2 semesters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "student_chatgpt = pd.read_csv(\"student_chatgpt.csv\")\n",
    "only_chatgpt = pd.read_csv(\"only_chatgpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Student-ChatGPT shape: {student_chatgpt.shape}\")\n",
    "print(f\"Only-ChatGPT shape: {only_chatgpt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase and segment into sentences\"\"\"\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    return sentences\n",
    "\n",
    "# Process student essays\n",
    "student_sentences = []\n",
    "student_labels = []\n",
    "for text in student_chatgpt['student']:\n",
    "    sentences = preprocess_text(text)\n",
    "    student_sentences.extend(sentences)\n",
    "    student_labels.extend([0] * len(sentences))  # 0 for student\n",
    "\n",
    "# Process ChatGPT responses from student_chatgpt dataset\n",
    "chatgpt_sentences_1 = []\n",
    "chatgpt_labels_1 = []\n",
    "for text in student_chatgpt['chatgpt']:\n",
    "    sentences = preprocess_text(text)\n",
    "    chatgpt_sentences_1.extend(sentences)\n",
    "    chatgpt_labels_1.extend([1] * len(sentences))  # 1 for ChatGPT\n",
    "\n",
    "# Process only_chatgpt responses\n",
    "chatgpt_sentences_2 = []\n",
    "chatgpt_labels_2 = []\n",
    "for text in only_chatgpt['chatgpt']:\n",
    "    sentences = preprocess_text(text)\n",
    "    chatgpt_sentences_2.extend(sentences)\n",
    "    chatgpt_labels_2.extend([1] * len(sentences))\n",
    "\n",
    "print(f\"Number of student sentences: {len(student_sentences)}\")\n",
    "print(f\"Number of ChatGPT sentences (from student_chatgpt): {len(chatgpt_sentences_1)}\")\n",
    "print(f\"Number of ChatGPT sentences (from only_chatgpt): {len(chatgpt_sentences_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Extract stylometric features from text:\n",
    "    1. Basic features (character count, word count)\n",
    "    2. Lexical features (avg word length, unique words ratio)\n",
    "    3. Syntactic features (punctuation, uppercase ratio)\n",
    "    4. POS tag features (noun ratio, verb ratio)\n",
    "    \"\"\"\n",
    "    # Basic features\n",
    "    n_chars = len(text)\n",
    "    n_words = len(text.split())\n",
    "    \n",
    "    # Lexical features\n",
    "    avg_word_length = n_chars / n_words if n_words > 0 else 0\n",
    "    unique_words = len(set(text.split())) / n_words if n_words > 0 else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    punctuation_marks = len(re.findall(r'[.,!?;:]', text)) / n_chars\n",
    "    uppercase_ratio = sum(1 for c in text if c.isupper()) / n_chars\n",
    "    \n",
    "    # POS features\n",
    "    pos_tags = pos_tag(word_tokenize(text))\n",
    "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
    "    noun_ratio = pos_counts.get('NN', 0) / len(pos_tags) if pos_tags else 0\n",
    "    verb_ratio = sum(pos_counts.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']) / len(pos_tags) if pos_tags else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_words_ratio': unique_words,\n",
    "        'punctuation_ratio': punctuation_marks,\n",
    "        'uppercase_ratio': uppercase_ratio,\n",
    "        'noun_ratio': noun_ratio,\n",
    "        'verb_ratio': verb_ratio,\n",
    "        'sentence_length': n_words\n",
    "    }\n",
    "\n",
    "print(\"Extracting stylometric features...\")\n",
    "\n",
    "# Extract features for each dataset\n",
    "student_features = [extract_stylometric_features(sent) for sent in student_sentences]\n",
    "student_features_array = np.array([[feat[k] for k in feat.keys()] for feat in student_features])\n",
    "\n",
    "chatgpt1_features = [extract_stylometric_features(sent) for sent in chatgpt_sentences_1]\n",
    "chatgpt1_features_array = np.array([[feat[k] for k in feat.keys()] for feat in chatgpt1_features])\n",
    "\n",
    "chatgpt2_features = [extract_stylometric_features(sent) for sent in chatgpt_sentences_2]\n",
    "chatgpt2_features_array = np.array([[feat[k] for k in feat.keys()] for feat in chatgpt2_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inisialisasi BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer for IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(texts, max_length=512):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Membuat Model BERT Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model dasar IndoBERT\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Arsitektur untuk Bi-Encoder ChatGPT\n",
    "input_ids_chatgpt = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask_chatgpt = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Dapatkan embeddings dari BERT\n",
    "bert_outputs_chatgpt = bert_model(input_ids_chatgpt, attention_mask=attention_mask_chatgpt)[0]\n",
    "cls_token_chatgpt = bert_outputs_chatgpt[:, 0, :]  # Ambil token [CLS]\n",
    "\n",
    "# Layer-layer tambahan untuk fine-tuning\n",
    "dense1_chatgpt = tf.keras.layers.Dense(256, activation=\"relu\")(cls_token_chatgpt)\n",
    "dropout1_chatgpt = tf.keras.layers.Dropout(0.1)(dense1_chatgpt)\n",
    "dense2_chatgpt = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1_chatgpt)\n",
    "dropout2_chatgpt = tf.keras.layers.Dropout(0.1)(dense2_chatgpt)\n",
    "output_chatgpt = tf.keras.layers.Dense(128)(dropout2_chatgpt)\n",
    "\n",
    "# Normalisasi output\n",
    "normalized_output_chatgpt = tf.nn.l2_normalize(output_chatgpt, axis=1)\n",
    "\n",
    "# Buat model Bi-Encoder untuk ChatGPT\n",
    "bi_encoder_chatgpt = tf.keras.Model(\n",
    "    inputs=[input_ids_chatgpt, attention_mask_chatgpt],\n",
    "    outputs=normalized_output_chatgpt,\n",
    "    name=\"bi_encoder_chatgpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arsitektur untuk Bi-Encoder Student (struktur yang sama, variable berbeda)\n",
    "input_ids_student = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask_student = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "bert_outputs_student = bert_model(input_ids_student, attention_mask=attention_mask_student)[0]\n",
    "cls_token_student = bert_outputs_student[:, 0, :]\n",
    "\n",
    "dense1_student = tf.keras.layers.Dense(256, activation=\"relu\")(cls_token_student)\n",
    "dropout1_student = tf.keras.layers.Dropout(0.1)(dense1_student)\n",
    "dense2_student = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1_student)\n",
    "dropout2_student = tf.keras.layers.Dropout(0.1)(dense2_student)\n",
    "output_student = tf.keras.layers.Dense(128)(dropout2_student)\n",
    "\n",
    "normalized_output_student = tf.nn.l2_normalize(output_student, axis=1)\n",
    "\n",
    "bi_encoder_student = tf.keras.Model(\n",
    "    inputs=[input_ids_student, attention_mask_student],\n",
    "    outputs=normalized_output_student,\n",
    "    name=\"bi_encoder_student\"\n",
    ")\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Bi-Encoder\n",
    "\n",
    "Problem resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi contrastive loss\n",
    "def contrastive_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # y_true: 1 untuk similar pairs, 0 untuk dissimilar pairs\n",
    "        # y_pred: cosine similarity antara dua embeddings\n",
    "        \n",
    "        # Konversi similarity ke distance\n",
    "        distance = 1 - y_pred\n",
    "        \n",
    "        # Loss untuk similar pairs\n",
    "        positive_loss = y_true * tf.square(distance)\n",
    "        \n",
    "        # Loss untuk dissimilar pairs\n",
    "        negative_loss = (1 - y_true) * tf.square(tf.maximum(margin - distance, 0))\n",
    "        \n",
    "        return tf.reduce_mean(positive_loss + negative_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Training bi_encoder_chatgpt dengan data only_chatgpt\n",
    "\n",
    "# Persiapkan data untuk training\n",
    "print(\"Mempersiapkan data training untuk ChatGPT Bi-Encoder...\")\n",
    "\n",
    "# Tokenisasi data ChatGPT\n",
    "chatgpt2_train_encodings = tokenize_text(chatgpt_sentences_2)\n",
    "\n",
    "# Buat positive pairs dari kalimat yang berdekatan\n",
    "batch_size = 32\n",
    "\n",
    "# Compile model dengan contrastive loss\n",
    "bi_encoder_chatgpt.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=contrastive_loss(margin=1.0)\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"Training Bi-Encoder ChatGPT...\")\n",
    "history_chatgpt = bi_encoder_chatgpt.fit(\n",
    "    {\n",
    "        \"input_ids\": chatgpt2_train_encodings['input_ids'],\n",
    "        \"attention_mask\": chatgpt2_train_encodings['attention_mask']\n",
    "    },\n",
    "    # Target matrix: diagonal adalah 1 (similar pairs), sisanya 0\n",
    "    tf.eye(len(chatgpt_sentences_2)),\n",
    "    epochs=3,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Training bi_encoder_student dengan data student_chatgpt\n",
    "\n",
    "# Persiapkan data training\n",
    "print(\"Mempersiapkan data training untuk Student Bi-Encoder...\")\n",
    "\n",
    "# Tokenisasi data Student dan ChatGPT\n",
    "student_train_encodings = tokenize_text(student_sentences)\n",
    "chatgpt1_train_encodings = tokenize_text(chatgpt_sentences_1)\n",
    "\n",
    "# Combine data untuk training\n",
    "combined_input_ids = tf.concat([\n",
    "    student_train_encodings['input_ids'],\n",
    "    chatgpt1_train_encodings['input_ids']\n",
    "], axis=0)\n",
    "\n",
    "combined_attention_mask = tf.concat([\n",
    "    student_train_encodings['attention_mask'],\n",
    "    chatgpt1_train_encodings['attention_mask']\n",
    "], axis=0)\n",
    "\n",
    "# Buat similarity matrix\n",
    "total_samples = len(student_sentences) + len(chatgpt_sentences_1)\n",
    "similarity_matrix = tf.zeros((total_samples, total_samples))\n",
    "\n",
    "# Set similarity 1 untuk pasangan dari sumber yang sama\n",
    "student_size = len(student_sentences)\n",
    "similarity_matrix = tf.tensor_scatter_nd_update(\n",
    "    similarity_matrix,\n",
    "    tf.where(tf.eye(student_size) > 0),\n",
    "    tf.ones(student_size)\n",
    ")\n",
    "\n",
    "chatgpt_start = student_size\n",
    "chatgpt_size = len(chatgpt_sentences_1)\n",
    "similarity_matrix = tf.tensor_scatter_nd_update(\n",
    "    similarity_matrix,\n",
    "    tf.where(tf.eye(chatgpt_size) > 0) + chatgpt_start,\n",
    "    tf.ones(chatgpt_size)\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "bi_encoder_student.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=contrastive_loss(margin=1.0)\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"Training Bi-Encoder Student...\")\n",
    "history_student = bi_encoder_student.fit(\n",
    "    {\n",
    "        \"input_ids\": combined_input_ids,\n",
    "        \"attention_mask\": combined_attention_mask\n",
    "    },\n",
    "    similarity_matrix,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history untuk kedua model\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot untuk model ChatGPT\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_chatgpt.history['loss'], label='Training Loss (ChatGPT)')\n",
    "plt.plot(history_chatgpt.history['val_loss'], label='Validation Loss (ChatGPT)')\n",
    "plt.title('Training History - Bi-Encoder ChatGPT')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot untuk model Student\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_student.history['loss'], label='Training Loss (Student)')\n",
    "plt.plot(history_student.history['val_loss'], label='Validation Loss (Student)')\n",
    "plt.title('Training History - Bi-Encoder Student')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluasi Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siapkan data test untuk evaluasi\n",
    "# Gunakan sebagian kecil data yang belum digunakan dalam training\n",
    "test_student = student_sentences[-100:]  # Ambil 100 kalimat terakhir untuk testing\n",
    "test_chatgpt_1 = chatgpt_sentences_1[-100:]  # Dari student_chatgpt dataset\n",
    "test_chatgpt_2 = chatgpt_sentences_2[-100:]  # Dari only_chatgpt dataset\n",
    "\n",
    "# Tokenisasi data test\n",
    "test_student_encodings = tokenize_text(test_student)\n",
    "test_chatgpt1_encodings = tokenize_text(test_chatgpt_1)\n",
    "test_chatgpt2_encodings = tokenize_text(test_chatgpt_2)\n",
    "\n",
    "# Generate embeddings untuk data test\n",
    "print(\"Generating test embeddings...\")\n",
    "test_student_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": test_student_encodings['input_ids'],\n",
    "    \"attention_mask\": test_student_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "test_chatgpt1_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": test_chatgpt1_encodings['input_ids'],\n",
    "    \"attention_mask\": test_chatgpt1_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "test_chatgpt2_embeddings = bi_encoder_chatgpt.predict({\n",
    "    \"input_ids\": test_chatgpt2_encodings['input_ids'],\n",
    "    \"attention_mask\": test_chatgpt2_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "# Hitung similarity scores\n",
    "def calculate_similarity_metrics(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Menghitung similarity scores antara dua set embeddings\n",
    "    Returns: mean similarity, min similarity, max similarity\n",
    "    \"\"\"\n",
    "    # Normalisasi embeddings\n",
    "    normalized_emb1 = tf.nn.l2_normalize(embeddings1, axis=1)\n",
    "    normalized_emb2 = tf.nn.l2_normalize(embeddings2, axis=1)\n",
    "    \n",
    "    # Hitung similarity matrix\n",
    "    similarity_matrix = tf.matmul(normalized_emb1, normalized_emb2, transpose_b=True)\n",
    "    \n",
    "    # Ambil metrics\n",
    "    mean_sim = tf.reduce_mean(similarity_matrix)\n",
    "    max_sim = tf.reduce_max(similarity_matrix)\n",
    "    min_sim = tf.reduce_min(similarity_matrix)\n",
    "    \n",
    "    return mean_sim.numpy(), min_sim.numpy(), max_sim.numpy()\n",
    "\n",
    "# Evaluasi similarity antara different types of text\n",
    "print(\"\\nEvaluasi Similarity Scores:\")\n",
    "\n",
    "# Student vs Student\n",
    "mean_sim, min_sim, max_sim = calculate_similarity_metrics(\n",
    "    test_student_embeddings[:50], test_student_embeddings[50:])\n",
    "print(\"\\nStudent vs Student Similarity:\")\n",
    "print(f\"Mean: {mean_sim:.3f}, Min: {min_sim:.3f}, Max: {max_sim:.3f}\")\n",
    "\n",
    "# Student vs ChatGPT (from student_chatgpt)\n",
    "mean_sim, min_sim, max_sim = calculate_similarity_metrics(\n",
    "    test_student_embeddings, test_chatgpt1_embeddings)\n",
    "print(\"\\nStudent vs ChatGPT (student_chatgpt) Similarity:\")\n",
    "print(f\"Mean: {mean_sim:.3f}, Min: {min_sim:.3f}, Max: {max_sim:.3f}\")\n",
    "\n",
    "# ChatGPT vs ChatGPT (across datasets)\n",
    "mean_sim, min_sim, max_sim = calculate_similarity_metrics(\n",
    "    test_chatgpt1_embeddings, test_chatgpt2_embeddings)\n",
    "print(\"\\nChatGPT vs ChatGPT (across datasets) Similarity:\")\n",
    "print(f\"Mean: {mean_sim:.3f}, Min: {min_sim:.3f}, Max: {max_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Simpan Model Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model Bi-Encoder untuk student_chatgpt\n",
    "print(\"Menyimpan model Bi-Encoder student_chatgpt...\")\n",
    "bi_encoder_student.save('saved_models/bi_encoder_student_chatgpt.h5')\n",
    "\n",
    "# Simpan model Bi-Encoder untuk only_chatgpt\n",
    "print(\"Menyimpan model Bi-Encoder only_chatgpt...\")\n",
    "bi_encoder_chatgpt.save('saved_models/bi_encoder_only_chatgpt.h5')\n",
    "\n",
    "# Simpan tokenizer configuration\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "print(\"Model dan tokenizer berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating embeddings for all sentences...\")\n",
    "\n",
    "# Generate embeddings untuk student sentences\n",
    "print(\"\\nGenerating student embeddings...\")\n",
    "student_encodings = tokenize_text(student_sentences)\n",
    "student_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": student_encodings['input_ids'],\n",
    "    \"attention_mask\": student_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "# Generate embeddings untuk ChatGPT dari student_chatgpt\n",
    "print(\"Generating ChatGPT (student_chatgpt) embeddings...\")\n",
    "chatgpt1_encodings = tokenize_text(chatgpt_sentences_1)\n",
    "chatgpt1_embeddings = bi_encoder_student.predict({\n",
    "    \"input_ids\": chatgpt1_encodings['input_ids'],\n",
    "    \"attention_mask\": chatgpt1_encodings['attention_mask']\n",
    "})\n",
    "\n",
    "# Generate embeddings untuk ChatGPT dari only_chatgpt\n",
    "print(\"Generating ChatGPT (only_chatgpt) embeddings...\")\n",
    "chatgpt2_encodings = tokenize_text(chatgpt_sentences_2)\n",
    "chatgpt2_embeddings = bi_encoder_chatgpt.predict({\n",
    "    \"input_ids\": chatgpt2_encodings['input_ids'],\n",
    "    \"attention_mask\": chatgpt2_encodings['attention_mask']\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mencari similar sentences\n",
    "def get_top_k_similar(query_embedding, target_embeddings, k=50):\n",
    "    \"\"\"\n",
    "    Mencari k kalimat yang paling mirip berdasarkan cosine similarity\n",
    "    \"\"\"\n",
    "    query_normalized = tf.nn.l2_normalize(query_embedding, axis=1)\n",
    "    target_normalized = tf.nn.l2_normalize(target_embeddings, axis=1)\n",
    "    \n",
    "    similarity_scores = tf.matmul(query_normalized, target_normalized, transpose_b=True)\n",
    "    scores, indices = tf.nn.top_k(similarity_scores[0], k=k)\n",
    "    \n",
    "    return indices.numpy(), scores.numpy()\n",
    "\n",
    "# List untuk menyimpan similar pairs\n",
    "similar_pairs = []\n",
    "\n",
    "# Proses setiap kalimat student\n",
    "for idx, student_embedding in enumerate(student_embeddings):\n",
    "    print(f\"\\rMemproses kalimat ke-{idx+1}/{len(student_embeddings)}\", end=\"\")\n",
    "    \n",
    "    query_emb = tf.reshape(student_embedding, (1, -1))\n",
    "    \n",
    "    # Cari similar sentences dari data student\n",
    "    student_indices, student_scores = get_top_k_similar(\n",
    "        query_emb, student_embeddings, k=50\n",
    "    )\n",
    "    \n",
    "    # Cari similar sentences dari data ChatGPT\n",
    "    combined_chatgpt_embeddings = np.vstack([chatgpt1_embeddings, chatgpt2_embeddings])\n",
    "    chatgpt_indices, chatgpt_scores = get_top_k_similar(\n",
    "        query_emb, combined_chatgpt_embeddings, k=50\n",
    "    )\n",
    "    \n",
    "    # Tambahkan ke similar pairs\n",
    "    for i, score in zip(student_indices, student_scores):\n",
    "        if i != idx:\n",
    "            similar_pairs.append({\n",
    "                'input_sentence': student_sentences[idx],\n",
    "                'similar_sentence': student_sentences[i],\n",
    "                'similarity_score': score,\n",
    "                'label': 0\n",
    "            })\n",
    "    \n",
    "    for i, score in zip(chatgpt_indices, chatgpt_scores):\n",
    "        if i < len(chatgpt_sentences_1):\n",
    "            similar_sentence = chatgpt_sentences_1[i]\n",
    "        else:\n",
    "            similar_sentence = chatgpt_sentences_2[i - len(chatgpt_sentences_1)]\n",
    "            \n",
    "        similar_pairs.append({\n",
    "            'input_sentence': student_sentences[idx],\n",
    "            'similar_sentence': similar_sentence,\n",
    "            'similarity_score': score,\n",
    "            'label': 1\n",
    "        })\n",
    "\n",
    "print(\"\\n\\nStatistik similar pairs:\")\n",
    "print(f\"Total pasangan: {len(similar_pairs)}\")\n",
    "print(f\"Pasangan student-student: {sum(1 for pair in similar_pairs if pair['label'] == 0)}\")\n",
    "print(f\"Pasangan student-chatgpt: {sum(1 for pair in similar_pairs if pair['label'] == 1)}\")\n",
    "\n",
    "# Simpan similar pairs\n",
    "with open('saved_models/similar_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(similar_pairs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Membuat Model BERT Single Sentence Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embeddings dari Bi-Encoder\n",
    "# 2. Fitur stylometric\n",
    "# 3. Similarity scores dari similar sentences\n",
    "\n",
    "# %%\n",
    "print(\"Membuat model Single Sentence Classifier...\")\n",
    "\n",
    "# Input layers untuk tiga jenis fitur\n",
    "bert_embedding_input = tf.keras.layers.Input(\n",
    "    shape=(128,), \n",
    "    dtype=tf.float32, \n",
    "    name=\"bert_embedding\"\n",
    ")\n",
    "stylometric_input = tf.keras.layers.Input(\n",
    "    shape=(7,), \n",
    "    dtype=tf.float32, \n",
    "    name=\"stylometric_features\"\n",
    ")\n",
    "similarity_score_input = tf.keras.layers.Input(\n",
    "    shape=(1,), \n",
    "    dtype=tf.float32, \n",
    "    name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "# Processing BERT embeddings\n",
    "bert_dense = tf.keras.layers.Dense(\n",
    "    256, \n",
    "    activation=\"relu\", \n",
    "    name=\"bert_dense\"\n",
    ")(bert_embedding_input)\n",
    "bert_dropout = tf.keras.layers.Dropout(0.2)(bert_dense)\n",
    "\n",
    "# Processing stylometric features\n",
    "style_dense = tf.keras.layers.Dense(\n",
    "    32, \n",
    "    activation=\"relu\", \n",
    "    name=\"style_dense\"\n",
    ")(stylometric_input)\n",
    "style_dropout = tf.keras.layers.Dropout(0.2)(style_dense)\n",
    "\n",
    "# Processing similarity score\n",
    "sim_dense = tf.keras.layers.Dense(\n",
    "    16, \n",
    "    activation=\"relu\", \n",
    "    name=\"sim_dense\"\n",
    ")(similarity_score_input)\n",
    "sim_dropout = tf.keras.layers.Dropout(0.2)(sim_dense)\n",
    "\n",
    "# Combine all features\n",
    "combined_features = tf.keras.layers.Concatenate()(\n",
    "    [bert_dropout, style_dropout, sim_dropout]\n",
    ")\n",
    "\n",
    "# Classification layers\n",
    "dense1 = tf.keras.layers.Dense(128, activation=\"relu\")(combined_features)\n",
    "dropout1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "dense2 = tf.keras.layers.Dense(64, activation=\"relu\")(dropout1)\n",
    "dropout2 = tf.keras.layers.Dropout(0.2)(dense2)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout2)\n",
    "\n",
    "# Create classifier model\n",
    "classifier = tf.keras.Model(\n",
    "    inputs=[\n",
    "        bert_embedding_input, \n",
    "        stylometric_input, \n",
    "        similarity_score_input\n",
    "    ],\n",
    "    outputs=output,\n",
    "    name=\"single_sentence_classifier\"\n",
    ")\n",
    "\n",
    "# Compile model dengan metrics yang lengkap\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        tf.keras.metrics.AUC(name=\"auc\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Membuat Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Menyiapkan data training untuk classifier...\")\n",
    "\n",
    "# Persiapkan data training dari similar pairs\n",
    "training_data = []\n",
    "training_labels = []\n",
    "\n",
    "for pair in similar_pairs:\n",
    "    # Ambil embedding dari kalimat input\n",
    "    input_sent_idx = student_sentences.index(pair['input_sentence'])\n",
    "    input_embedding = student_embeddings[input_sent_idx]\n",
    "    \n",
    "    # Ambil fitur stylometric\n",
    "    input_style_features = student_features_array[input_sent_idx]\n",
    "    \n",
    "    # Similarity score dari bi-encoder\n",
    "    similarity_score = pair['similarity_score']\n",
    "    \n",
    "    # Tambahkan ke training data\n",
    "    training_data.append({\n",
    "        'bert_embedding': input_embedding,\n",
    "        'stylometric_features': input_style_features,\n",
    "        'similarity_score': similarity_score\n",
    "    })\n",
    "    training_labels.append(pair['label'])\n",
    "\n",
    "# Konversi ke format numpy arrays\n",
    "train_embeddings = np.array([x['bert_embedding'] for x in training_data])\n",
    "train_style_features = np.array([x['stylometric_features'] for x in training_data])\n",
    "train_sim_scores = np.array([x['similarity_score'] for x in training_data]).reshape(-1, 1)\n",
    "train_labels = np.array(training_labels)\n",
    "\n",
    "# Split data menjadi training dan validation\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(training_data)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_labels\n",
    ")\n",
    "\n",
    "# Prepare training inputs\n",
    "train_inputs = {\n",
    "    'bert_embedding': train_embeddings[train_idx],\n",
    "    'stylometric_features': train_style_features[train_idx],\n",
    "    'similarity_score': train_sim_scores[train_idx]\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    'bert_embedding': train_embeddings[val_idx],\n",
    "    'stylometric_features': train_style_features[val_idx],\n",
    "    'similarity_score': train_sim_scores[val_idx]\n",
    "}\n",
    "\n",
    "train_labels_split = train_labels[train_idx]\n",
    "val_labels_split = train_labels[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dengan monitoring\n",
    "history = classifier.fit(\n",
    "    train_inputs,\n",
    "    train_labels_split,\n",
    "    validation_data=(val_inputs, val_labels_split),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot additional metrics\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['precision'], label='Precision')\n",
    "plt.plot(history.history['recall'], label='Recall')\n",
    "plt.plot(history.history['auc'], label='AUC')\n",
    "plt.title('Additional Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Melakukan evaluasi model...\")\n",
    "\n",
    "# Prediksi pada validation set\n",
    "val_predictions = classifier.predict(val_inputs)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Buat confusion matrix\n",
    "cm = confusion_matrix(val_labels_split, val_predictions_binary)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=['Student', 'ChatGPT'],\n",
    "    yticklabels=['Student', 'ChatGPT']\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Tampilkan classification report lengkap\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    val_labels_split, \n",
    "    val_predictions_binary,\n",
    "    target_names=['Student', 'ChatGPT']\n",
    "))\n",
    "\n",
    "# %% [markdown]\n",
    "# # 12. Analisis Error Cases\n",
    "# Menganalisis kasus-kasus di mana model membuat kesalahan untuk pemahaman lebih dalam\n",
    "\n",
    "# %%\n",
    "# Identifikasi error cases\n",
    "errors = {\n",
    "    'false_positives': [],\n",
    "    'false_negatives': []\n",
    "}\n",
    "\n",
    "for idx, (true_label, pred_label) in enumerate(zip(val_labels_split, val_predictions_binary)):\n",
    "    if true_label != pred_label:\n",
    "        # Ambil informasi tentang kasus error\n",
    "        error_case = {\n",
    "            'true_label': true_label,\n",
    "            'predicted_prob': val_predictions[idx][0],\n",
    "            'stylometric_features': val_inputs['stylometric_features'][idx],\n",
    "            'similarity_score': val_inputs['similarity_score'][idx][0]\n",
    "        }\n",
    "        \n",
    "        if true_label == 0 and pred_label == 1:\n",
    "            errors['false_positives'].append(error_case)\n",
    "        else:\n",
    "            errors['false_negatives'].append(error_case)\n",
    "\n",
    "print(\"Analisis Error Cases:\")\n",
    "print(f\"Total False Positives: {len(errors['false_positives'])}\")\n",
    "print(f\"Total False Negatives: {len(errors['false_negatives'])}\")\n",
    "\n",
    "# Visualisasi distribusi confidence scores untuk error cases\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot untuk False Positives\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist([case['predicted_prob'] for case in errors['false_positives']], bins=20)\n",
    "plt.title('Confidence Distribution - False Positives')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot untuk False Negatives\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist([case['predicted_prob'] for case in errors['false_negatives']], bins=20)\n",
    "plt.title('Confidence Distribution - False Negatives')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Simpan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat direktori jika belum ada\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "# Simpan model classifier\n",
    "classifier.save('saved_models/single_sentence_classifier.h5')\n",
    "\n",
    "# Simpan model bi-encoder\n",
    "bi_encoder_student.save('saved_models/bi_encoder_student.h5')\n",
    "bi_encoder_chatgpt.save('saved_models/bi_encoder_chatgpt.h5')\n",
    "\n",
    "# Simpan konfigurasi tokenizer\n",
    "tokenizer.save_pretrained('saved_models/tokenizer')\n",
    "\n",
    "# Simpan parameter normalisasi untuk fitur stylometric\n",
    "stylometric_params = {\n",
    "    'feature_names': list(student_features[0].keys()),\n",
    "    'mean': np.mean(train_style_features, axis=0),\n",
    "    'std': np.std(train_style_features, axis=0)\n",
    "}\n",
    "\n",
    "with open('saved_models/stylometric_params.pkl', 'wb') as f:\n",
    "    pickle.dump(stylometric_params, f)\n",
    "\n",
    "print(\"Model dan konfigurasi berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Prediction Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_source(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memprediksi sumber teks (Student atau ChatGPT)\n",
    "    Args:\n",
    "        text: Teks yang akan diprediksi\n",
    "    Returns:\n",
    "        Dictionary berisi hasil prediksi dan confidence score\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    sentences = preprocess_text(text)\n",
    "    \n",
    "    # Extract stylometric features\n",
    "    style_features = [extract_stylometric_features(sent) for sent in sentences]\n",
    "    style_features_array = np.array([[feat[k] for k in stylometric_params['feature_names']] \n",
    "                                   for feat in style_features])\n",
    "    \n",
    "    # Generate embeddings\n",
    "    encodings = tokenize_text(sentences)\n",
    "    embeddings = bi_encoder_student.predict({\n",
    "        \"input_ids\": encodings['input_ids'],\n",
    "        \"attention_mask\": encodings['attention_mask']\n",
    "    })\n",
    "    \n",
    "    # Get similarity scores\n",
    "    similarity_scores = []\n",
    "    for emb in embeddings:\n",
    "        _, scores = get_top_k_similar(\n",
    "            emb.reshape(1, -1),\n",
    "            student_embeddings,  # Bisa diganti dengan kombinasi embeddings\n",
    "            k=50\n",
    "        )\n",
    "        similarity_scores.append(np.mean(scores))\n",
    "    \n",
    "    # Prepare inputs for classifier\n",
    "    inputs = {\n",
    "        'bert_embedding': embeddings,\n",
    "        'stylometric_features': style_features_array,\n",
    "        'similarity_score': np.array(similarity_scores).reshape(-1, 1)\n",
    "    }\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = classifier.predict(inputs)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'predictions': predictions.tolist(),\n",
    "        'avg_confidence': float(np.mean(predictions)),\n",
    "        'sentence_count': len(sentences)\n",
    "    }\n",
    "\n",
    "# Test fungsi prediksi\n",
    "test_text = \"This is a test sentence. Let's see how it works.\"\n",
    "result = predict_text_source(test_text)\n",
    "print(\"\\nHasil test prediksi:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
