{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers ga support Keras 3, kudu downgrade pake pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p1 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "model = 'indobenchmark/indobert-base-p1'\n",
    "tokenizer = BertTokenizer.from_pretrained(model)\n",
    "model = TFBertModel.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9854233264923096\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and get embeddings from BERT\n",
    "def get_embeddings(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Use the last hidden state of the [CLS] token for sentence-level embeddings\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embeddings\n",
    "\n",
    "# Function to calculate cosine similarity between two embeddings\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Get embeddings for both texts\n",
    "    embeddings1 = get_embeddings(text1)\n",
    "    embeddings2 = get_embeddings(text2)\n",
    "    \n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1 = embeddings1.numpy()\n",
    "    embeddings2 = embeddings2.numpy()\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(embeddings1, embeddings2)\n",
    "    \n",
    "    return similarity[0][0]\n",
    "\n",
    "# Example usage\n",
    "text1 = \"Rubah cokelat yang cepat melompati anjing yang malas.\"\n",
    "text2 = \"Seekor hewan cokelat yang cepat melompati anjing yang sedang tidur.\"\n",
    "\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "print(f\"Cosine Similarity: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks Siswa : 'Kesehatan mental di kalangan pelajar itu penting banget, tapi seringkali nggak diperhatikan. Banyak yang mikir kalau kesehatan mental cuma soal “nggak gila” atau “nggak stres berat,” padahal lebih dari itu. Pelajar kayak kita sering ngerasain tekanan dari tugas, ujian, ekspektasi orang tua, dan lingkungan sekolah. Akibatnya, banyak dari kita yang jadi cemas, susah tidur, bahkan kadang sampai merasa depresi. Tapi, karena kurangnya pengetahuan, kita sering anggap remeh perasaan itu atau malah nggak tahu harus gimana ngatasinnya.\n",
      "\n",
      "Belum lagi, ngomongin kesehatan mental di sekolah tuh sering dianggap tabu. Malu, takut dikira lemah, atau malah takut dihakimi. Padahal, kalau kita bisa lebih terbuka tentang apa yang kita rasain, mungkin kita bisa dapet dukungan yang lebih baik, entah dari guru, teman, atau konselor. Hal-hal sederhana kayak istirahat yang cukup, punya hobi, atau ngobrol sama orang yang bisa dipercaya, bisa banget bantu jaga kesehatan mental kita.\n",
      "\n",
      "Jadi, penting banget buat kita sadar kalau kesehatan mental itu nggak kalah penting dari fisik. Jangan takut buat cari bantuan kalau merasa tertekan, dan yang paling penting, jangan remehkan perasaan sendiri.\n",
      "97.23% sepertinya mirip GPT\n",
      "Teks yang paling mirip: 'Kesehatan mental di kalangan pelajar seringkali diabaikan, padahal sangat penting untuk diperhatikan. Tekanan dari tugas sekolah, harapan orang tua, dan persiapan ujian sering membuat siswa merasa tertekan. Banyak dari kami merasa cemas atau stres, tapi takut untuk berbicara karena khawatir dianggap lemah. Kesehatan mental yang buruk dapat mempengaruhi prestasi akademis dan hubungan sosial.\n",
      "\n",
      "Selain itu, media sosial juga memberi pengaruh yang besar. Banyak pelajar merasa harus tampil sempurna di depan teman-temannya, dan ini bisa membuat mereka merasa rendah diri jika tidak sesuai standar. Padahal, setiap orang memiliki kemampuan dan kelebihan masing-masing.\n",
      "\n",
      "Sangat penting bagi sekolah untuk menyediakan layanan konseling dan mengajarkan kepada siswa cara mengelola stres. Dengan begitu, kesehatan mental pelajar bisa terjaga, dan kami bisa belajar dengan lebih baik tanpa tekanan yang berlebihan.'\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize and get embeddings from BERT\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embeddings\n",
    "\n",
    "# Function to calculate cosine similarity between two embeddings\n",
    "def calculate_similarity(text1, text2):\n",
    "    embeddings1 = get_embeddings(text1).numpy()\n",
    "    embeddings2 = get_embeddings(text2).numpy()\n",
    "    similarity = cosine_similarity(embeddings1, embeddings2)\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('examples-datasets.xlsx')\n",
    "\n",
    "# Choose the student text to test (e.g., first row)\n",
    "student_text = df['Pelajar'].iloc[0]\n",
    "\n",
    "max_similarity = 0\n",
    "closest_gpt_text = ''\n",
    "\n",
    "# Compare with each GPT text\n",
    "for gpt_text in df['GPT']:\n",
    "    similarity_score = calculate_similarity(student_text, gpt_text)\n",
    "    \n",
    "    # Check if this is the highest similarity found\n",
    "    if similarity_score > max_similarity:\n",
    "        max_similarity = similarity_score\n",
    "        closest_gpt_text = gpt_text\n",
    "\n",
    "# Determine if it's closer to student or GPT\n",
    "similarity_percentage = max_similarity * 100  # Convert to percentage\n",
    "if max_similarity > 0.5:  # Assuming a threshold for deciding between student and GPT\n",
    "    if similarity_percentage > 50:  # Adjust this threshold as needed\n",
    "        print(f\"Teks Siswa : '{student_text}\")\n",
    "        print(f\"{similarity_percentage:.2f}% sepertinya mirip GPT\")\n",
    "        print(f\"Teks yang paling mirip: '{closest_gpt_text}'\")\n",
    "    else:\n",
    "        print(f\"Teks Siswa : '{student_text}\")\n",
    "        print(f\"{similarity_percentage:.2f}% sepertinya buatan pelajar\")\n",
    "else:\n",
    "    print(f\"Teks Siswa : '{student_text}\")\n",
    "    print(f\"{similarity_percentage:.2f}% sepertinya buatan pelajar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
