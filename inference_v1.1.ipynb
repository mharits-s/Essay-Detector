{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library yang Diperlukan\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Preprocessing Teks\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing teks:\n",
    "    - Mengubah teks menjadi huruf kecil\n",
    "    - Membersihkan whitespace berlebih\n",
    "    - Menjaga teks sebagai paragraf utuh\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks input.\n",
    "        \n",
    "    Returns:\n",
    "        str: Teks yang telah diproses atau None jika tidak valid.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Bersihkan teks\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Ganti multiple whitespace dengan satu spasi\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Tokenisasi\n",
    "def tokenize_text(texts, tokenizer, max_length=256):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks menggunakan tokenizer IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Daftar teks yang akan di-tokenisasi.\n",
    "        tokenizer: Tokenizer BERT.\n",
    "        max_length (int): Panjang maksimum token.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Token hasil tokenisasi, termasuk input_ids dan attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Ekstraksi Fitur Stylometric\n",
    "def extract_stylometric_features(text):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur stylometric dari teks:\n",
    "    - Panjang kata rata-rata\n",
    "    - Rasio kata unik\n",
    "    - Rasio tanda baca\n",
    "    - Panjang kalimat\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input teks.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Fitur stylometric.\n",
    "    \"\"\"\n",
    "    # Basic features\n",
    "    n_chars = len(text)\n",
    "    n_words = len(text.split())\n",
    "    avg_word_length = n_chars / n_words if n_words > 0 else 0\n",
    "    unique_word_ratio = len(set(text.split())) / n_words if n_words > 0 else 0\n",
    "    \n",
    "    # Syntactic features\n",
    "    punctuation_ratio = len(re.findall(r'[.,!?;:]', text)) / n_chars if n_chars > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'sentence_length': n_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk Menghitung Similarity\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Menghitung cosine similarity antara dua embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: Embedding pertama.\n",
    "        embedding2: Embedding kedua.\n",
    "        \n",
    "    Returns:\n",
    "        float: Similarity score.\n",
    "    \"\"\"\n",
    "    # Cosine similarity: dot product of normalized vectors\n",
    "    return np.sum(embedding1 * embedding2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_dir='saved_models'):\n",
    "    \"\"\"\n",
    "    Load semua model dan konfigurasi yang diperlukan untuk inferensi.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Direktori tempat model disimpan.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Tokenizer, bi-encoder models, classifier, dan scaler.\n",
    "    \"\"\"\n",
    "    print(\"Loading models and configurations...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(f'{model_dir}/tokenizer')\n",
    "    \n",
    "    # Register custom objects\n",
    "    custom_objects = {'TFBertModel': TFBertModel}\n",
    "    \n",
    "    # Load bi-encoder models with custom objects\n",
    "    bi_encoder_student_chatgpt = tf.keras.models.load_model(\n",
    "        f'{model_dir}/bi_encoder_student_chatgpt.h5',\n",
    "        custom_objects=custom_objects\n",
    "    )\n",
    "    \n",
    "    bi_encoder_only_chatgpt = tf.keras.models.load_model(\n",
    "        f'{model_dir}/bi_encoder_only_chatgpt.h5',\n",
    "        custom_objects=custom_objects\n",
    "    )\n",
    "    \n",
    "    # Load classifier with custom objects\n",
    "    classifier = tf.keras.models.load_model(\n",
    "        f'{model_dir}/text_classifier.h5',\n",
    "        custom_objects=custom_objects\n",
    "    )\n",
    "    \n",
    "    # Load scaler\n",
    "    with open(f'{model_dir}/scaler_stylometric.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    # Load reference embeddings (jika ada)\n",
    "    reference_embeddings = {}\n",
    "    try:\n",
    "        if os.path.exists(f'{model_dir}/reference_embeddings.pkl'):\n",
    "            with open(f'{model_dir}/reference_embeddings.pkl', 'rb') as f:\n",
    "                reference_embeddings = pickle.load(f)\n",
    "        else:\n",
    "            print(\"Warning: reference_embeddings.pkl not found. Using empty reference embeddings.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load reference embeddings: {e}\")\n",
    "    \n",
    "    print(\"Models and configurations loaded successfully!\")\n",
    "    return tokenizer, bi_encoder_student_chatgpt, bi_encoder_only_chatgpt, classifier, scaler, reference_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Utama untuk Deteksi Teks\n",
    "def detect_text_source(text, tokenizer, bi_encoder_student_chatgpt, bi_encoder_only_chatgpt, \n",
    "                       classifier, scaler, reference_embeddings=None):\n",
    "    \"\"\"\n",
    "    Mendeteksi apakah teks berasal dari manusia atau AI.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks yang akan dideteksi.\n",
    "        tokenizer: Tokenizer untuk preprocessing teks.\n",
    "        bi_encoder_student_chatgpt: Model bi-encoder untuk Student_ChatGPT.\n",
    "        bi_encoder_only_chatgpt: Model bi-encoder untuk Only_ChatGPT.\n",
    "        classifier: Model klasifikasi.\n",
    "        scaler: Scaler untuk normalisasi fitur stylometric.\n",
    "        reference_embeddings: Embeddings referensi untuk similarity (opsional).\n",
    "        \n",
    "    Returns:\n",
    "        dict: Hasil deteksi dan confidence score.\n",
    "    \"\"\"\n",
    "    # Preprocessing teks\n",
    "    processed_text = preprocess_text(text)\n",
    "    if not processed_text:\n",
    "        return {\n",
    "            \"source\": \"Error\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"message\": \"Teks tidak valid atau kosong\"\n",
    "        }\n",
    "    \n",
    "    # Tokenisasi\n",
    "    tokens = tokenize_text([processed_text], tokenizer)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embedding_student_chatgpt = bi_encoder_student_chatgpt([tokens['input_ids'], tokens['attention_mask']])\n",
    "    embedding_only_chatgpt = bi_encoder_only_chatgpt([tokens['input_ids'], tokens['attention_mask']])\n",
    "    \n",
    "    # Ekstraksi fitur stylometric\n",
    "    stylometric_features = extract_stylometric_features(processed_text)\n",
    "    stylometric_features_df = pd.DataFrame([stylometric_features])\n",
    "    \n",
    "    # Normalisasi fitur stylometric\n",
    "    stylometric_features_normalized = scaler.transform(stylometric_features_df)\n",
    "    \n",
    "    # Hitung similarity scores dengan reference embeddings jika tersedia\n",
    "    student_sim = 0.5\n",
    "    chatgpt_sim = 0.5\n",
    "    knowledge_sim = 0.5\n",
    "    \n",
    "    if reference_embeddings and 'student' in reference_embeddings and 'chatgpt_1' in reference_embeddings and 'chatgpt_2' in reference_embeddings:\n",
    "        # Hitung similarity dengan student embeddings\n",
    "        student_similarities = compute_similarity(\n",
    "            embedding_student_chatgpt.numpy(), \n",
    "            reference_embeddings['student']\n",
    "        )\n",
    "        student_sim = np.mean(student_similarities)\n",
    "        \n",
    "        # Hitung similarity dengan chatgpt embeddings\n",
    "        chatgpt_similarities = compute_similarity(\n",
    "            embedding_student_chatgpt.numpy(), \n",
    "            reference_embeddings['chatgpt_1']\n",
    "        )\n",
    "        chatgpt_sim = np.mean(chatgpt_similarities)\n",
    "        \n",
    "        # Hitung similarity dengan knowledge embeddings\n",
    "        knowledge_similarities = compute_similarity(\n",
    "            embedding_only_chatgpt.numpy(), \n",
    "            reference_embeddings['chatgpt_2']\n",
    "        )\n",
    "        knowledge_sim = np.mean(knowledge_similarities)\n",
    "    \n",
    "    # Siapkan similarity scores untuk input model\n",
    "    similarity_scores = np.array([[student_sim, chatgpt_sim, knowledge_sim]])\n",
    "    \n",
    "    # Prediksi menggunakan classifier\n",
    "    prediction = classifier.predict({\n",
    "        \"bert_embedding\": embedding_student_chatgpt.numpy(),\n",
    "        \"stylometric_features\": stylometric_features_normalized,\n",
    "        \"similarity_score\": similarity_scores\n",
    "    })\n",
    "    \n",
    "    # Interpretasi hasil\n",
    "    confidence = float(prediction[0][0])\n",
    "    source = \"AI (ChatGPT)\" if confidence > 0.5 else \"Human (Student)\"\n",
    "    \n",
    "    return {\n",
    "        \"source\": source,\n",
    "        \"confidence\": confidence if source == \"AI (ChatGPT)\" else 1 - confidence,\n",
    "        \"stylometric_features\": stylometric_features,\n",
    "        \"embedding_similarity\": {\n",
    "            \"student_similarity\": student_sim,\n",
    "            \"chatgpt_similarity\": chatgpt_sim,\n",
    "            \"knowledge_similarity\": knowledge_sim\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model dan Konfigurasi\n",
    "try:\n",
    "    tokenizer, bi_encoder_student_chatgpt, bi_encoder_only_chatgpt, classifier, scaler, reference_embeddings = load_models()\n",
    "    models_loaded = True\n",
    "    print(\"All models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    models_loaded = False\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    print(\"Please make sure the models are available in the 'saved_models' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk Visualisasi Hasil\n",
    "def visualize_results(result):\n",
    "    \"\"\"\n",
    "    Visualisasi hasil deteksi.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): Hasil deteksi dari fungsi detect_text_source.\n",
    "    \"\"\"\n",
    "    if 'message' in result:\n",
    "        print(f\"Error: {result['message']}\")\n",
    "        return\n",
    "    \n",
    "    # Tampilkan hasil utama\n",
    "    print(f\"Detected Source: {result['source']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    \n",
    "    # Visualisasi fitur stylometric\n",
    "    features = result['stylometric_features']\n",
    "    feature_names = list(features.keys())\n",
    "    feature_values = list(features.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(feature_names, feature_values, color='skyblue')\n",
    "    plt.title('Stylometric Features')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualisasi similarity scores\n",
    "    similarities = result['embedding_similarity']\n",
    "    sim_names = list(similarities.keys())\n",
    "    sim_values = list(similarities.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(sim_names, sim_values, color='lightgreen')\n",
    "    plt.title('Embedding Similarity Scores')\n",
    "    plt.ylabel('Similarity')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Inferensi dengan Contoh Teks\n",
    "if models_loaded:\n",
    "    # Contoh teks untuk dideteksi\n",
    "    sample_texts = [\n",
    "        \"Saya menulis esai ini untuk tugas sekolah. Menurut pendapat saya, pemanasan global adalah masalah yang sangat serius dan kita semua harus bertindak sekarang. Saya pikir kita perlu lebih banyak kesadaran tentang masalah ini.\",\n",
    "        \"Pemanasan global merupakan fenomena peningkatan suhu rata-rata permukaan bumi yang disebabkan oleh berbagai faktor, terutama aktivitas manusia yang menghasilkan gas rumah kaca. Dampaknya sangat signifikan terhadap ekosistem global dan kehidupan manusia. Berbagai penelitian ilmiah telah menunjukkan bahwa perubahan iklim yang terjadi saat ini berlangsung pada tingkat yang mengkhawatirkan.\"\n",
    "    ]\n",
    "    \n",
    "    # Deteksi sumber teks\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Sample Text #{i+1}:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(text[:100] + \"...\" if len(text) > 100 else text)\n",
    "        print('-'*50)\n",
    "        \n",
    "        result = detect_text_source(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            bi_encoder_student_chatgpt, \n",
    "            bi_encoder_only_chatgpt, \n",
    "            classifier, \n",
    "            scaler,\n",
    "            reference_embeddings\n",
    "        )\n",
    "        \n",
    "        visualize_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk Aplikasi Web dengan Gradio\n",
    "def predict_source_for_gradio(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memprediksi sumber teks untuk aplikasi web Gradio.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks input.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Label, confidence, dan visualisasi.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Masukkan teks terlebih dahulu\", 0.0, None, None\n",
    "    \n",
    "    result = detect_text_source(\n",
    "        text, \n",
    "        tokenizer, \n",
    "        bi_encoder_student_chatgpt, \n",
    "        bi_encoder_only_chatgpt, \n",
    "        classifier, \n",
    "        scaler,\n",
    "        reference_embeddings\n",
    "    )\n",
    "    \n",
    "    if 'message' in result:\n",
    "        return result['message'], 0.0, None, None\n",
    "    \n",
    "    # Buat visualisasi fitur stylometric\n",
    "    features = result['stylometric_features']\n",
    "    feature_df = pd.DataFrame({\n",
    "        'Feature': list(features.keys()),\n",
    "        'Value': list(features.values())\n",
    "    })\n",
    "    fig1 = plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x='Feature', y='Value', data=feature_df)\n",
    "    plt.title('Stylometric Features')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Buat visualisasi similarity scores\n",
    "    similarities = result['embedding_similarity']\n",
    "    sim_df = pd.DataFrame({\n",
    "        'Type': list(similarities.keys()),\n",
    "        'Similarity': list(similarities.values())\n",
    "    })\n",
    "    fig2 = plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x='Type', y='Similarity', data=sim_df)\n",
    "    plt.title('Embedding Similarity Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return result['source'], result['confidence'], fig1, fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jalankan Aplikasi Web dengan Gradio (jika models_loaded)\n",
    "if models_loaded:\n",
    "    # Buat interface Gradio\n",
    "    iface = gr.Interface(\n",
    "        fn=predict_source_for_gradio,\n",
    "        inputs=gr.Textbox(lines=10, placeholder=\"Masukkan teks di sini...\"),\n",
    "        outputs=[\n",
    "            gr.Label(label=\"Sumber Teks\"),\n",
    "            gr.Number(label=\"Confidence Score\"),\n",
    "            gr.Plot(label=\"Fitur Stylometric\"),\n",
    "            gr.Plot(label=\"Similarity Scores\")\n",
    "        ],\n",
    "        title=\"Deteksi Teks AI vs Manusia\",\n",
    "        description=\"Aplikasi ini mendeteksi apakah teks ditulis oleh manusia (siswa) atau dihasilkan oleh AI (ChatGPT).\",\n",
    "        examples=[\n",
    "            [\"Saya menulis esai ini untuk tugas sekolah. Menurut pendapat saya, pemanasan global adalah masalah yang sangat serius dan kita semua harus bertindak sekarang.\"],\n",
    "            [\"Pemanasan global merupakan fenomena peningkatan suhu rata-rata permukaan bumi yang disebabkan oleh berbagai faktor, terutama aktivitas manusia yang menghasilkan gas rumah kaca. Dampaknya sangat signifikan terhadap ekosistem global.\"]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Jalankan aplikasi\n",
    "    iface.launch(share=True)  # share=True untuk membuat link publik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk Inferensi Batch (untuk file CSV/Excel)\n",
    "def batch_inference(file_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Melakukan inferensi batch pada file CSV atau Excel.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path ke file CSV atau Excel.\n",
    "        output_path (str, optional): Path untuk menyimpan hasil. Default None.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame hasil deteksi.\n",
    "    \"\"\"\n",
    "    if not models_loaded:\n",
    "        print(\"Models not loaded. Please load models first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load file\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xlsx', '.xls')):\n",
    "        df = pd.read_excel(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use CSV or Excel.\")\n",
    "        return None\n",
    "    \n",
    "    # Cek kolom teks\n",
    "    text_column = None\n",
    "    possible_columns = ['text', 'content', 'essay', 'teks', 'konten', 'esai']\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None and len(df.columns) > 0:\n",
    "        # Gunakan kolom pertama jika tidak ada kolom yang cocok\n",
    "        text_column = df.columns[0]\n",
    "    \n",
    "    if text_column is None:\n",
    "        print(\"No suitable text column found in the file.\")\n",
    "        return None\n",
    "    \n",
    "    # Tambahkan kolom hasil\n",
    "    results = []\n",
    "    \n",
    "    # Proses setiap baris\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        text = row[text_column]\n",
    "        result = detect_text_source(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            bi_encoder_student_chatgpt, \n",
    "            bi_encoder_only_chatgpt, \n",
    "            classifier, \n",
    "            scaler,\n",
    "            reference_embeddings\n",
    "        )\n",
    "        \n",
    "        # Tambahkan hasil ke list\n",
    "        results.append({\n",
    "            'source': result.get('source', 'Error'),\n",
    "            'confidence': result.get('confidence', 0.0),\n",
    "            'avg_word_length': result.get('stylometric_features', {}).get('avg_word_length', 0),\n",
    "            'unique_word_ratio': result.get('stylometric_features', {}).get('unique_word_ratio', 0),\n",
    "            'punctuation_ratio': result.get('stylometric_features', {}).get('punctuation_ratio', 0),\n",
    "            'sentence_length': result.get('stylometric_features', {}).get('sentence_length', 0),\n",
    "            'student_similarity': result.get('embedding_similarity', {}).get('student_similarity', 0),\n",
    "            'chatgpt_similarity': result.get('embedding_similarity', {}).get('chatgpt_similarity', 0),\n",
    "            'knowledge_similarity': result.get('embedding_similarity', {}).get('knowledge_similarity', 0)\n",
    "        })\n",
    "    \n",
    "    # Buat DataFrame hasil\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Gabungkan dengan DataFrame asli\n",
    "    output_df = pd.concat([df, results_df], axis=1)\n",
    "    \n",
    "    # Simpan hasil jika output_path diberikan\n",
    "    if output_path:\n",
    "        if output_path.endswith('.csv'):\n",
    "            output_df.to_csv(output_path, index=False)\n",
    "        elif output_path.endswith(('.xlsx', '.xls')):\n",
    "            output_df.to_excel(output_path, index=False)\n",
    "        else:\n",
    "            output_df.to_csv(output_path + '.csv', index=False)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# %%\n",
    "# Demo Batch Inference (uncomment untuk menggunakan)\n",
    "\"\"\"\n",
    "if models_loaded:\n",
    "    # Contoh penggunaan batch inference\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Path ke file input\n",
    "    input_file = \"sample_essays.csv\"\n",
    "    \n",
    "    # Path untuk menyimpan hasil\n",
    "    output_file = \"detection_results.csv\"\n",
    "    \n",
    "    # Jalankan batch inference\n",
    "    results = batch_inference(input_file, output_file)\n",
    "    \n",
    "    # Tampilkan ringkasan hasil\n",
    "    if results is not None:\n",
    "        print(\"\\nSummary of Results:\")\n",
    "        print(f\"Total texts processed: {len(results)}\")\n",
    "        print(f\"Detected as Human: {sum(results['source'] == 'Human (Student)')}\")\n",
    "        print(f\"Detected as AI: {sum(results['source'] == 'AI (ChatGPT)')}\")\n",
    "        print(f\"Average confidence: {results['confidence'].mean():.4f}\")\n",
    "        \n",
    "        # Visualisasi distribusi confidence\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(results['confidence'], bins=20, kde=True)\n",
    "        plt.title('Distribution of Confidence Scores')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Fungsi untuk Inferensi Interaktif di Notebook\n",
    "def interactive_inference():\n",
    "    \"\"\"\n",
    "    Fungsi untuk inferensi interaktif di notebook.\n",
    "    \"\"\"\n",
    "    if not models_loaded:\n",
    "        print(\"Models not loaded. Please load models first.\")\n",
    "        return\n",
    "    \n",
    "    # Input teks dari pengguna\n",
    "    text = input(\"Masukkan teks untuk dideteksi (ketik 'exit' untuk keluar):\\n\")\n",
    "    \n",
    "    while text.lower() != 'exit':\n",
    "        # Deteksi sumber teks\n",
    "        result = detect_text_source(\n",
    "            text, \n",
    "            tokenizer, \n",
    "            bi_encoder_student_chatgpt, \n",
    "            bi_encoder_only_chatgpt, \n",
    "            classifier, \n",
    "            scaler,\n",
    "            reference_embeddings\n",
    "        )\n",
    "        \n",
    "        # Tampilkan hasil\n",
    "        print(\"\\nHasil Deteksi:\")\n",
    "        print(f\"Sumber: {result.get('source', 'Error')}\")\n",
    "        print(f\"Confidence: {result.get('confidence', 0):.4f}\")\n",
    "        \n",
    "        # Tampilkan fitur stylometric\n",
    "        print(\"\\nFitur Stylometric:\")\n",
    "        for feature, value in result.get('stylometric_features', {}).items():\n",
    "            print(f\"  - {feature}: {value:.4f}\")\n",
    "        \n",
    "        # Tampilkan similarity scores\n",
    "        print(\"\\nSimilarity Scores:\")\n",
    "        for sim_type, value in result.get('embedding_similarity', {}).items():\n",
    "            print(f\"  - {sim_type}: {value:.4f}\")\n",
    "        \n",
    "        # Input teks berikutnya\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        text = input(\"Masukkan teks untuk dideteksi (ketik 'exit' untuk keluar):\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_loaded:\n",
    "    interactive_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Script started\")\n",
    "print(f\"Models loaded successfully: {models_loaded}\")\n",
    "\n",
    "# At the end of the script\n",
    "if models_loaded:\n",
    "    print(\"Running interactive inference...\")\n",
    "    interactive_inference()\n",
    "else:\n",
    "    print(\"Models failed to load. Check the 'saved_models' directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
