{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe276a1",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c088b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfd2a4",
   "metadata": {},
   "source": [
    "# Load Essay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a23b8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdset = pd.read_csv(\"datasets_ta/esai_siswa.csv\")\n",
    "gptset = pd.read_csv(\"datasets_ta/esai_gpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52788db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 135 entries, 0 to 134\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Essay   135 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Selama berlibur dan bulan suci Ramadhan saya pulang kampung, setelah di kampung, saya sering ke masjid tarawih dan tadarusan bersama teman kampung saya, setelah tadarus kami melanjutkan dengan membangunkan sahur orang' di kampung, saya dan teman-teman sering bermain free fire sambil menunggu waktu berbuka,selain itu saya juga membantu bapak saya di kebun, kemudian di malam idul Fitri saya dan orang kampung saya melakukan takbiran keliling kampung.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>selama bulan Ramadhan saya membantu orang tua saya, saya juga mengikuti solat tarawih, buka bersama, saat lebaran saya bertemu teman dan keluarga saya, kami meminta maaf satu sama lainnya lalu kami berbincang\"sebelumnya saya menunggu waktu berbuka sambil nnton tiktok dan rells ig.saya juga ikut melakukan kegiatan berbagi takjil bersama anggota OSIS di sekolah, kami melakukannya bersama\" kamu membuat es timun serut, lalu kami membagikannya di simpang tiga dekat dengan sekolah kami,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assalamualaikum, selama bulan suci Ramadhan saya sering pergi teraweh sama teman teman dan setelah teraweh saya melakukan tadarus, dan selama bulan suci ramadhan saya tinggal jauh sama orang tua tetapi orang tua saya datang saat Idul Fitri sudah dekat untuk melakukan Idul Fitri di sumbawa bersama saya setelah melakukan Idul Fitri saya mudik ke lombok bersama keluarga sekalian menikmati libur saat di di lombok saya juga pergi liburan ke Mandalika dan setelah mau masuk sekolah saya kembali ke sumbawa untuk sekolah sekian dari saya terimakasih assalamualaikum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selama libur idul Fitri sayaa hanya tidur tetapi terkadang saya juga membantu mama saya membereskan rumah,lalu malamnyaa saya trawehh,sehabis trawe saya jalan' dngn teman' saya sambil mnunggu jam sahur,stelah jam sahur kami membangunkan orang sahur dengn menggunakan speaker.\\n-3 hari lebaran saya pulang ke kampung mama saya,setelah sampai kampung saya membantu kaka saya untuk membuat ketupat,soto,dan kue kue,saat hari lebaran saya sholat Ied,sehabis sholat Ied saya ke kuburan,shbis dri kuburan saya lalu plng ke rmh,lalu saya ganti baju dn make up,lalu stlh itu saya makan soto lombokkk bikinan kakaa sayaa,sehabis makann lalu saya foto foto dengan kaka saya dn kami pun juga membuat tiktok (TikTok adalah platform media sosial yang memungkinkan pengguna untuk membuat, menonton, dan berbagi video pendek. Video yang dibuat di TikTok dapat berdurasi mulai dari 3 detik hingga 10 menit). mengikuti trend Velocity,lalu stelah itu saya pergi ke rumah kluarga saya,saat di sana kami salam'an,bermaaf'an,stlh itu saya dn kaka saya hanya scroll tiktok smpai Cass hp kami habiss,dn barulah kitaa plng ke rmh dan tidur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assalamu'alaikum warahmatullahi waabarakatuh \\nSaya akan menceritakan kegiata saya di bulan suci ramadhan \\nSalama ramadhan di hari pertama saya melakukan puasa dan sholat \\n5 waktu sehari semalam dan taraweh, dan di pertegahan puasa \\nSaya pulang kampung bersama teman yang bernama RIDHO ARDIANSYAH pada tanggal 20 Maret 2025 setelah di kampung saya membantu Abang saya Garut sambil menunggu berbukah puasa di sawah.dan di malam hari salesai taraweh saya bermain geme bersama teman \\nTerimakasih</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Essay\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Selama berlibur dan bulan suci Ramadhan saya pulang kampung, setelah di kampung, saya sering ke masjid tarawih dan tadarusan bersama teman kampung saya, setelah tadarus kami melanjutkan dengan membangunkan sahur orang' di kampung, saya dan teman-teman sering bermain free fire sambil menunggu waktu berbuka,selain itu saya juga membantu bapak saya di kebun, kemudian di malam idul Fitri saya dan orang kampung saya melakukan takbiran keliling kampung.\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        selama bulan Ramadhan saya membantu orang tua saya, saya juga mengikuti solat tarawih, buka bersama, saat lebaran saya bertemu teman dan keluarga saya, kami meminta maaf satu sama lainnya lalu kami berbincang\"sebelumnya saya menunggu waktu berbuka sambil nnton tiktok dan rells ig.saya juga ikut melakukan kegiatan berbagi takjil bersama anggota OSIS di sekolah, kami melakukannya bersama\" kamu membuat es timun serut, lalu kami membagikannya di simpang tiga dekat dengan sekolah kami,\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Assalamualaikum, selama bulan suci Ramadhan saya sering pergi teraweh sama teman teman dan setelah teraweh saya melakukan tadarus, dan selama bulan suci ramadhan saya tinggal jauh sama orang tua tetapi orang tua saya datang saat Idul Fitri sudah dekat untuk melakukan Idul Fitri di sumbawa bersama saya setelah melakukan Idul Fitri saya mudik ke lombok bersama keluarga sekalian menikmati libur saat di di lombok saya juga pergi liburan ke Mandalika dan setelah mau masuk sekolah saya kembali ke sumbawa untuk sekolah sekian dari saya terimakasih assalamualaikum\n",
       "3  Selama libur idul Fitri sayaa hanya tidur tetapi terkadang saya juga membantu mama saya membereskan rumah,lalu malamnyaa saya trawehh,sehabis trawe saya jalan' dngn teman' saya sambil mnunggu jam sahur,stelah jam sahur kami membangunkan orang sahur dengn menggunakan speaker.\\n-3 hari lebaran saya pulang ke kampung mama saya,setelah sampai kampung saya membantu kaka saya untuk membuat ketupat,soto,dan kue kue,saat hari lebaran saya sholat Ied,sehabis sholat Ied saya ke kuburan,shbis dri kuburan saya lalu plng ke rmh,lalu saya ganti baju dn make up,lalu stlh itu saya makan soto lombokkk bikinan kakaa sayaa,sehabis makann lalu saya foto foto dengan kaka saya dn kami pun juga membuat tiktok (TikTok adalah platform media sosial yang memungkinkan pengguna untuk membuat, menonton, dan berbagi video pendek. Video yang dibuat di TikTok dapat berdurasi mulai dari 3 detik hingga 10 menit). mengikuti trend Velocity,lalu stelah itu saya pergi ke rumah kluarga saya,saat di sana kami salam'an,bermaaf'an,stlh itu saya dn kaka saya hanya scroll tiktok smpai Cass hp kami habiss,dn barulah kitaa plng ke rmh dan tidur\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Assalamu'alaikum warahmatullahi waabarakatuh \\nSaya akan menceritakan kegiata saya di bulan suci ramadhan \\nSalama ramadhan di hari pertama saya melakukan puasa dan sholat \\n5 waktu sehari semalam dan taraweh, dan di pertegahan puasa \\nSaya pulang kampung bersama teman yang bernama RIDHO ARDIANSYAH pada tanggal 20 Maret 2025 setelah di kampung saya membantu Abang saya Garut sambil menunggu berbukah puasa di sawah.dan di malam hari salesai taraweh saya bermain geme bersama teman \\nTerimakasih"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdset.info()\n",
    "stdset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b97e9f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 539 entries, 0 to 538\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Response  539 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 4.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>libur idul fitri tahun ini menjadi momen yang sangat dinantikan oleh semua orang, termasuk aku. setelah melewati masa ujian dan kesibukan sekolah, kini aku bisa merayakan hari raya bersama keluarga. teknologi memainkan peran penting selama masa libur ini, terutama dalam menghubungkan aku dengan kerabat yang tidak bisa pulang ke kampung halaman. salah satu aplikasi yang sangat berguna adalah panggilan video, yang kami gunakan untuk bersilaturahmi secara virtual. meskipun tidak bisa bertemu secara langsung, setidaknya kami masih bisa berbincang dan saling mendoakan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>selain itu, teknologi juga membantuku dalam mengabadikan momen-momen kebersamaan dengan keluarga. kamera ponsel menjadi alat yang andal untuk mengambil foto dan video selama kegiatan berkumpul. hasil jepretan ini kemudian disimpan dan dibagikan melalui media sosial, menjadi kenangan yang dapat diingat sepanjang tahun. platform berbagi foto juga menjadi tempat bertukar cerita dengan teman-teman, melihat bagaimana mereka merayakan idul fitri di tempat mereka masing-masing. ini memberikan rasa kebersamaan dan kegembiraan, meskipun berada di tempat yang berbeda.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tak lupa, teknologi juga memudahkan dalam mencari resep dan panduan memasak untuk persiapan hidangan lebaran. berbagai situs dan aplikasi menyediakan ide-ide baru serta tips memasak yang sangat membantu, terutama ketika mencoba menu baru yang berbeda dari biasa. dengan bantuan teknologi, aku dan ibu bisa menciptakan hidangan lezat yang menggugah selera, membuat suasana lebaran menjadi lebih berkesan. video tutorial juga memberikan wawasan baru bagi kami, menambah ketrampilan memasak kami selama liburan ini.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>secara keseluruhan, libur idul fitri ini menjadi lebih kaya dan bermakna berkat teknologi. dari silaturahmi virtual hingga berbagi momen melalui media sosial, teknologi memastikan kami tetap terhubung dan mendapatkan pengalaman baru. momen-momen indah ini akan selalu diingat, didukung oleh kemajuan teknologi yang ada di era digital saat ini. harapan ke depannya, meskipun tantangan mungkin berbeda, teknologi akan terus memfasilitasi kita untuk mempererat hubungan dan berbagi kebahagiaan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selama libur idul fitri kali ini, teknologi memainkan peran penting dalam berkomunikasi dan menghibur diri. karena pandemi masih berlangsung dan pembatasan mobilitas sering diterapkan, kami sekeluarga memutuskan untuk merayakan idul fitri secara virtual. kami menggunakan aplikasi video call seperti zoom dan whatsapp untuk tetap terhubung dengan keluarga besar yang tersebar di berbagai kota bahkan negara. meskipun terasa berbeda karena tidak bisa bertatap muka dan berpelukan secara langsung, teknologi berhasil menjembatani jarak di antara kami dan tetap menciptakan suasana hangat silaturahmi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Response\n",
       "0                              libur idul fitri tahun ini menjadi momen yang sangat dinantikan oleh semua orang, termasuk aku. setelah melewati masa ujian dan kesibukan sekolah, kini aku bisa merayakan hari raya bersama keluarga. teknologi memainkan peran penting selama masa libur ini, terutama dalam menghubungkan aku dengan kerabat yang tidak bisa pulang ke kampung halaman. salah satu aplikasi yang sangat berguna adalah panggilan video, yang kami gunakan untuk bersilaturahmi secara virtual. meskipun tidak bisa bertemu secara langsung, setidaknya kami masih bisa berbincang dan saling mendoakan.\n",
       "1                                    selain itu, teknologi juga membantuku dalam mengabadikan momen-momen kebersamaan dengan keluarga. kamera ponsel menjadi alat yang andal untuk mengambil foto dan video selama kegiatan berkumpul. hasil jepretan ini kemudian disimpan dan dibagikan melalui media sosial, menjadi kenangan yang dapat diingat sepanjang tahun. platform berbagi foto juga menjadi tempat bertukar cerita dengan teman-teman, melihat bagaimana mereka merayakan idul fitri di tempat mereka masing-masing. ini memberikan rasa kebersamaan dan kegembiraan, meskipun berada di tempat yang berbeda.\n",
       "2                                                                                        tak lupa, teknologi juga memudahkan dalam mencari resep dan panduan memasak untuk persiapan hidangan lebaran. berbagai situs dan aplikasi menyediakan ide-ide baru serta tips memasak yang sangat membantu, terutama ketika mencoba menu baru yang berbeda dari biasa. dengan bantuan teknologi, aku dan ibu bisa menciptakan hidangan lezat yang menggugah selera, membuat suasana lebaran menjadi lebih berkesan. video tutorial juga memberikan wawasan baru bagi kami, menambah ketrampilan memasak kami selama liburan ini.\n",
       "3                                                                                                             secara keseluruhan, libur idul fitri ini menjadi lebih kaya dan bermakna berkat teknologi. dari silaturahmi virtual hingga berbagi momen melalui media sosial, teknologi memastikan kami tetap terhubung dan mendapatkan pengalaman baru. momen-momen indah ini akan selalu diingat, didukung oleh kemajuan teknologi yang ada di era digital saat ini. harapan ke depannya, meskipun tantangan mungkin berbeda, teknologi akan terus memfasilitasi kita untuk mempererat hubungan dan berbagi kebahagiaan.\n",
       "4  selama libur idul fitri kali ini, teknologi memainkan peran penting dalam berkomunikasi dan menghibur diri. karena pandemi masih berlangsung dan pembatasan mobilitas sering diterapkan, kami sekeluarga memutuskan untuk merayakan idul fitri secara virtual. kami menggunakan aplikasi video call seperti zoom dan whatsapp untuk tetap terhubung dengan keluarga besar yang tersebar di berbagai kota bahkan negara. meskipun terasa berbeda karena tidak bisa bertatap muka dan berpelukan secara langsung, teknologi berhasil menjembatani jarak di antara kami dan tetap menciptakan suasana hangat silaturahmi."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptset.info()\n",
    "gptset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bfe94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Text preprocessing:\n",
    "    - Convert text to lowercase\n",
    "    - Split text into sentences using regex\n",
    "    - Keep final punctuation marks (., ?, !)\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of processed sentences.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return []\n",
    "\n",
    "    text = text.lower().strip()\n",
    "    sentences = re.findall(r'[^.!?]+[.!?]?', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2b71b",
   "metadata": {},
   "source": [
    "# Load Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ba72dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total student sentences: 1023 (label=0)\n",
      "Total ChatGPT sentences: 2115 (label=1)\n",
      "Total combined data: 3138\n",
      "\n",
      "First few rows of combined data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selama berlibur dan bulan suci ramadhan saya pulang kampung, setelah di kampung, saya sering ke masjid tarawih dan tadarusan bersama teman kampung saya, setelah tadarus kami melanjutkan dengan membangunkan sahur orang' di kampung, saya dan teman-teman sering bermain free fire sambil menunggu waktu berbuka,selain itu saya juga membantu bapak saya di kebun, kemudian di malam idul fitri saya dan orang kampung saya melakukan takbiran keliling kampung.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>selama bulan ramadhan saya membantu orang tua saya, saya juga mengikuti solat tarawih, buka bersama, saat lebaran saya bertemu teman dan keluarga saya, kami meminta maaf satu sama lainnya lalu kami berbincang\"sebelumnya saya menunggu waktu berbuka sambil nnton tiktok dan rells ig.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saya juga ikut melakukan kegiatan berbagi takjil bersama anggota osis di sekolah, kami melakukannya bersama\" kamu membuat es timun serut, lalu kami membagikannya di simpang tiga dekat dengan sekolah kami,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>assalamualaikum, selama bulan suci ramadhan saya sering pergi teraweh sama teman teman dan setelah teraweh saya melakukan tadarus, dan selama bulan suci ramadhan saya tinggal jauh sama orang tua tetapi orang tua saya datang saat idul fitri sudah dekat untuk melakukan idul fitri di sumbawa bersama saya setelah melakukan idul fitri saya mudik ke lombok bersama keluarga sekalian menikmati libur saat di di lombok saya juga pergi liburan ke mandalika dan setelah mau masuk sekolah saya kembali ke sumbawa untuk sekolah sekian dari saya terimakasih assalamualaikum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selama libur idul fitri sayaa hanya tidur tetapi terkadang saya juga membantu mama saya membereskan rumah,lalu malamnyaa saya trawehh,sehabis trawe saya jalan' dngn teman' saya sambil mnunggu jam sahur,stelah jam sahur kami membangunkan orang sahur dengn menggunakan speaker.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                 selama berlibur dan bulan suci ramadhan saya pulang kampung, setelah di kampung, saya sering ke masjid tarawih dan tadarusan bersama teman kampung saya, setelah tadarus kami melanjutkan dengan membangunkan sahur orang' di kampung, saya dan teman-teman sering bermain free fire sambil menunggu waktu berbuka,selain itu saya juga membantu bapak saya di kebun, kemudian di malam idul fitri saya dan orang kampung saya melakukan takbiran keliling kampung.   \n",
       "1                                                                                                                                                                                                                                                                                           selama bulan ramadhan saya membantu orang tua saya, saya juga mengikuti solat tarawih, buka bersama, saat lebaran saya bertemu teman dan keluarga saya, kami meminta maaf satu sama lainnya lalu kami berbincang\"sebelumnya saya menunggu waktu berbuka sambil nnton tiktok dan rells ig.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                        saya juga ikut melakukan kegiatan berbagi takjil bersama anggota osis di sekolah, kami melakukannya bersama\" kamu membuat es timun serut, lalu kami membagikannya di simpang tiga dekat dengan sekolah kami,   \n",
       "3  assalamualaikum, selama bulan suci ramadhan saya sering pergi teraweh sama teman teman dan setelah teraweh saya melakukan tadarus, dan selama bulan suci ramadhan saya tinggal jauh sama orang tua tetapi orang tua saya datang saat idul fitri sudah dekat untuk melakukan idul fitri di sumbawa bersama saya setelah melakukan idul fitri saya mudik ke lombok bersama keluarga sekalian menikmati libur saat di di lombok saya juga pergi liburan ke mandalika dan setelah mau masuk sekolah saya kembali ke sumbawa untuk sekolah sekian dari saya terimakasih assalamualaikum   \n",
       "4                                                                                                                                                                                                                                                                                                 selama libur idul fitri sayaa hanya tidur tetapi terkadang saya juga membantu mama saya membereskan rumah,lalu malamnyaa saya trawehh,sehabis trawe saya jalan' dngn teman' saya sambil mnunggu jam sahur,stelah jam sahur kami membangunkan orang sahur dengn menggunakan speaker.   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "std_sen = []\n",
    "gpt_sen = []  \n",
    "\n",
    "for text in stdset['Essay']:\n",
    "    std_sen.extend(preprocess_text(text))\n",
    "\n",
    "for text in gptset['Response']:\n",
    "    gpt_sen.extend(preprocess_text(text))\n",
    "\n",
    "# Create DataFrames with text and labels\n",
    "std_df = pd.DataFrame({'text': std_sen, 'label': 0})\n",
    "gpt1_df = pd.DataFrame({'text': gpt_sen, 'label': 1})\n",
    "\n",
    "# Combine all data\n",
    "data = pd.concat([std_df, gpt1_df], ignore_index=True)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total student sentences: {len(std_sen)} (label=0)\")\n",
    "print(f\"Total ChatGPT sentences: {len(gpt_sen)} (label=1)\")\n",
    "print(f\"Total combined data: {len(data)}\")\n",
    "\n",
    "# Display first few rows of combined data\n",
    "print(\"\\nFirst few rows of combined data:\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57714453",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "- Training 75%\n",
    "- Validation 5%\n",
    "- Testing 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c3b9387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training set: 2510 samples\n",
      "Initial test set: 628 samples\n",
      "Initial training distribution: Student=818, ChatGPT=1692\n",
      "Initial test distribution: Student=205, ChatGPT=423\n",
      "\n",
      "After undersampling:\n",
      "Training set: 1533 samples (Student=766, ChatGPT=767)\n",
      "Validation set: 103 samples (Student=52, ChatGPT=51)\n",
      "Test set: 1502 samples (Student=205, ChatGPT=1297)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.2, random_state=42, stratify=data['label']\n",
    ")\n",
    "\n",
    "print(f\"Initial training set: {len(train_data)} samples\")\n",
    "print(f\"Initial test set: {len(test_data)} samples\")\n",
    "print(f\"Initial training distribution: Student={sum(train_data['label']==0)}, ChatGPT={sum(train_data['label']==1)}\")\n",
    "print(f\"Initial test distribution: Student={sum(test_data['label']==0)}, ChatGPT={sum(test_data['label']==1)}\")\n",
    "\n",
    "\n",
    "X_train = train_data[['text']]\n",
    "y_train = train_data['label']\n",
    "\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_indices = pd.DataFrame({'index': range(len(X_train))})\n",
    "X_resampled_indices, y_resampled = undersampler.fit_resample(X_train_indices, y_train)\n",
    "\n",
    "selected_indices = X_resampled_indices['index'].values\n",
    "\n",
    "balanced_train_data = train_data.iloc[selected_indices].reset_index(drop=True)\n",
    "\n",
    "removed_indices = set(range(len(train_data))) - set(selected_indices)\n",
    "removed_samples = train_data.iloc[list(removed_indices)]\n",
    "\n",
    "test_set = pd.concat([test_data, removed_samples]).reset_index(drop=True)\n",
    "test_set = test_set.sort_values(by='label').reset_index(drop=True)\n",
    "\n",
    "train_set, val_set = train_test_split(\n",
    "    balanced_train_data, test_size=5/80, random_state=42, stratify=balanced_train_data['label']\n",
    ")\n",
    "\n",
    "train_set = train_set.sort_values(by='label').reset_index(drop=True)\n",
    "val_set = val_set.sort_values(by='label').reset_index(drop=True)\n",
    "\n",
    "# Print final dataset statistics\n",
    "print(\"\\nAfter undersampling:\")\n",
    "print(f\"Training set: {len(train_set)} samples (Student={sum(train_set['label']==0)}, ChatGPT={sum(train_set['label']==1)})\")\n",
    "print(f\"Validation set: {len(val_set)} samples (Student={sum(val_set['label']==0)}, ChatGPT={sum(val_set['label']==1)})\")\n",
    "print(f\"Test set: {len(test_set)} samples (Student={sum(test_set['label']==0)}, ChatGPT={sum(test_set['label']==1)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bfcf3c",
   "metadata": {},
   "source": [
    "# Initialize BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IndoBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_text(texts, max_length=128):\n",
    "    \"\"\"\n",
    "    Text tokenization using IndoBERT tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to be tokenized.\n",
    "        max_length (int): Maximum token length.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tokenized result, including input_ids and attention_mask.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8dcdc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize student essay...\n",
      "Tokenize ChatGPT essay...\n",
      "Tokenize student essay (Training Set)...\n",
      "Tokenize ChatGPT essay (Training Set)...\n",
      "Tokenize student essay (Validation Set)...\n",
      "Tokenize ChatGPT essay (Validation Set)...\n",
      "Tokenize student essay (Test Set)...\n",
      "Tokenize ChatGPT essay (Test Set)...\n",
      "\n",
      "Example of tokenization results:\n",
      "tf.Tensor(\n",
      "[[    2  1580   305   209   722  1614  1614   209  1821  3605    43   515\n",
      "     34  3627    98  1258  1313 30470     3     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    2   209  3614   486 14002    90   929   269 30469   269   440   709\n",
      "    628  1339  1476   663 30470     3     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    2   304   419   166  9562    41   521 24025  4674  1137    26  1351\n",
      "      1   469   211  1429  1369  1121  2693   421    34  2999  6319   633\n",
      "     92   472   216  5159    41 15540 30470     3     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]], shape=(3, 256), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]], shape=(3, 256), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize student and ChatGPT essays\n",
    "print(\"Tokenize student essay...\")\n",
    "std_tokens = tokenize_text(std_sen)\n",
    "print(\"Tokenize ChatGPT essay...\")\n",
    "gpt_tokens = tokenize_text(gpt_sen)\n",
    "\n",
    "print(\"Tokenize student essay (Training Set)...\")\n",
    "std_tr_tokens = tokenize_text(train_set[train_set['label'] == 0]['text'].tolist())\n",
    "print(\"Tokenize ChatGPT essay (Training Set)...\")\n",
    "gpt_tr_tokens = tokenize_text(train_set[train_set['label'] == 1]['text'].tolist())\n",
    "\n",
    "print(\"Tokenize student essay (Validation Set)...\")\n",
    "std_va_tokens = tokenize_text(val_set[val_set['label'] == 0]['text'].tolist())\n",
    "print(\"Tokenize ChatGPT essay (Validation Set)...\")\n",
    "gpt_va_tokens = tokenize_text(val_set[val_set['label'] == 1]['text'].tolist())\n",
    "\n",
    "print(\"Tokenize student essay (Test Set)...\")\n",
    "std_te_tokens = tokenize_text(test_set[test_set['label'] == 0]['text'].tolist())\n",
    "print(\"Tokenize ChatGPT essay (Test Set)...\")\n",
    "gpt_te_tokens = tokenize_text(test_set[test_set['label'] == 1]['text'].tolist())\n",
    "\n",
    "# Display tokenization results (example: Student)\n",
    "print(\"\\nExample of tokenization results:\")\n",
    "print(std_tr_tokens['input_ids'][:3])  # Input token ID\n",
    "print(std_tr_tokens['attention_mask'][:3])  # Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2da4bfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length for Student: 20.78\n",
      "Percentage truncated for Student: 0.00%\n",
      "Average token length for ChatGPT: 20.79\n",
      "Percentage truncated for ChatGPT: 0.00%\n"
     ]
    }
   ],
   "source": [
    "std_tr_lengths = [sum(mask) for mask in std_tr_tokens['attention_mask'].numpy()]\n",
    "gpt_tr_lengths = [sum(mask) for mask in gpt_tr_tokens['attention_mask'].numpy()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(std_tr_lengths, bins=30, alpha=0.5, label='Student', color='purple')\n",
    "plt.hist(gpt_tr_lengths, bins=30, alpha=0.5, label='ChatGPT', color='orange')\n",
    "plt.title('Distribution of Token Lengths')\n",
    "plt.xlabel('Active Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "print(f\"Average token length for Student: {np.mean(std_tr_lengths):.2f}\")\n",
    "print(f\"Percentage truncated for Student: {sum(l == 128 for l in std_tr_lengths) / len(std_tr_lengths) * 100:.2f}%\")\n",
    "print(f\"Average token length for ChatGPT: {np.mean(gpt_tr_lengths):.2f}\")\n",
    "print(f\"Percentage truncated for ChatGPT: {sum(l == 128 for l in gpt_tr_lengths) / len(gpt_tr_lengths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0896a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: libur idul fitri tahun ini menjadi momen yang sangat dinantikan oleh semua orang, termasuk aku.\n",
      "Token ID: [2, 7676, 8616, 8109, 262, 92, 234, 7585, 34, 310, 22766, 828, 213, 366, 232, 30468, 1087, 304, 30470, 3]\n",
      "Token dekode: [CLS] libur idul fitri tahun ini menjadi momen yang sangat dinantikan oleh semua orang, termasuk aku. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Decoding tokens for ensuring correctness\n",
    "sample_text = gpt_sen[0]\n",
    "sample_tokens = tokenizer.encode(sample_text)\n",
    "print(f\"Sample Text: {sample_text}\")\n",
    "print(f\"Token ID: {sample_tokens}\")\n",
    "print(f\"Token dekode: {tokenizer.decode(sample_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee363614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized data\n",
    "tokenized_data = {\n",
    "    'student': std_tokens,\n",
    "    'chatgpt': gpt_tokens\n",
    "}\n",
    "\n",
    "# Save input_ids and attention_mask as numpy arrays\n",
    "tokenized_numpy = {\n",
    "    'student': {\n",
    "        'input_ids': std_tokens['input_ids'].numpy(),\n",
    "        'attention_mask': std_tokens['attention_mask'].numpy()\n",
    "    },\n",
    "    'chatgpt ': {\n",
    "        'input_ids': gpt_tokens ['input_ids'].numpy(),\n",
    "        'attention_mask': gpt_tokens ['attention_mask'].numpy()\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(\"ta_sentence\", exist_ok=True)\n",
    "\n",
    "with open('ta_sentence/tokenized_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_numpy, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26faf74",
   "metadata": {},
   "source": [
    "# Build IndoBERT Semantic Similarity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f19b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IndoBERT model\n",
    "bert_model = TFBertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# Freeze BERT layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define Bi-Encoder model\n",
    "def model(bert_model):\n",
    "    \"\"\"\n",
    "    Create a Bi-Encoder model with IndoBERT.\n",
    "    \n",
    "    Args:\n",
    "        bert_model (TFBertModel): Base model of IndoBERT.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Bi-Encoder model.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Extract CLS token embeddings from IndoBERT\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0][:, 0, :]  # [CLS] token\n",
    "    \n",
    "    # Dense layer for fine-tuning\n",
    "    dense1 = tf.keras.layers.Dense(128, activation=\"relu\")(bert_output)\n",
    "    dropout1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "    dense2 = tf.keras.layers.Dense(128, activation=\"relu\")(dropout1)\n",
    "    dropout2 = tf.keras.layers.Dropout(0.1)(dense2)\n",
    "    dense3 = tf.keras.layers.Dense(128)(dropout2)\n",
    "    \n",
    "    # Output normalization (L2 normalization)\n",
    "    normalized_output = tf.nn.l2_normalize(dense3, axis=1)\n",
    "    \n",
    "    # Semantic model\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=normalized_output)\n",
    "\n",
    "# Build model\n",
    "semantic_model = model(bert_model)\n",
    "\n",
    "# Show model summary\n",
    "print(\"Model Summary:\")\n",
    "semantic_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbf4d6",
   "metadata": {},
   "source": [
    "# Create Contrastive Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrastive_pairs(student_tokens, chatgpt_tokens, max_pairs=None):\n",
    "    \"\"\"\n",
    "    Creates data pairs for contrastive learning with dataset-appropriate quantities.\n",
    "\n",
    "    Args:\n",
    "        student_tokens: Tokenized student text\n",
    "        chatgpt_tokens_1: First set of tokenized ChatGPT text\n",
    "        chatgpt_tokens_2: Second set of tokenized ChatGPT text\n",
    "        max_pairs: Maximum number of pairs (optional). If None, uses all possible combinations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dictionary of anchor, positive, negative pairs and labels, and the total number of pairs.\n",
    "    \"\"\"\n",
    "    # Get dataset sizes\n",
    "    n_student = student_tokens['input_ids'].shape[0]\n",
    "    n_chatgpt = chatgpt_tokens['input_ids'].shape[0]\n",
    "    \n",
    "    # Calculate maximum possible combinations\n",
    "    max_student_pairs = (n_student * (n_student - 1)) // 2  # student-student combinations\n",
    "    max_chatgpt_pairs = (n_chatgpt * (n_chatgpt - 1)) // 2  # chatgpt-chatgpt combinations\n",
    "    max_negative_pairs = n_student * n_chatgpt  # student-chatgpt combinations\n",
    "    \n",
    "    # Determine number of pairs to create\n",
    "    if max_pairs is None:\n",
    "        # Use minimum number of positive pairs for balance\n",
    "        n_pos_student = min(max_student_pairs, max_chatgpt_pairs) // 2\n",
    "        n_pos_chatgpt = n_pos_student\n",
    "        # Limit negative pairs to balance with positives\n",
    "        n_neg_pairs = min(max_negative_pairs, 2 * n_pos_student)\n",
    "    else:\n",
    "        # If max_pairs is specified, use that with equal proportions\n",
    "        n_pos_student = max_pairs // 4\n",
    "        n_pos_chatgpt = max_pairs // 4\n",
    "        n_neg_pairs = max_pairs // 2\n",
    "    \n",
    "    # Ensure we don't exceed the maximum possible combinations\n",
    "    n_pos_student = min(n_pos_student, max_student_pairs)\n",
    "    n_pos_chatgpt = min(n_pos_chatgpt, max_chatgpt_pairs)\n",
    "    n_neg_pairs = min(n_neg_pairs, max_negative_pairs)\n",
    "    \n",
    "    # Initialize arrays for data pairs\n",
    "    anchor_input_ids = []\n",
    "    anchor_attention_mask = []\n",
    "    positive_input_ids = []\n",
    "    positive_attention_mask = []\n",
    "    negative_input_ids = []\n",
    "    negative_attention_mask = []\n",
    "    labels = []\n",
    "    \n",
    "    # Generate positive student-student pairs\n",
    "    if n_pos_student > 0:\n",
    "        # Create all possible student-student pairs\n",
    "        student_pairs = [(i, j) for i in range(n_student) for j in range(i+1, n_student)]\n",
    "        # Randomly select pairs\n",
    "        selected_pairs = random.sample(student_pairs, n_pos_student)\n",
    "        \n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            # Anchor (student)\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][idx1])\n",
    "            \n",
    "            # Positive (another student)\n",
    "            positive_input_ids.append(student_tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(student_tokens['attention_mask'][idx2])\n",
    "            \n",
    "            # Negative (from ChatGPT)\n",
    "            neg_idx = np.random.choice(n_chatgpt)\n",
    "            negative_input_ids.append(chatgpt_tokens['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens['attention_mask'][neg_idx])\n",
    "            \n",
    "            # Label (1 for positive pair)\n",
    "            labels.append(1)\n",
    "    \n",
    "    # Generate positive chatgpt-chatgpt pairs\n",
    "    if n_pos_chatgpt > 0:\n",
    "        # Create all possible chatgpt-chatgpt pairs\n",
    "        chatgpt_pairs = [(i, j) for i in range(n_chatgpt) for j in range(i+1, n_chatgpt)]\n",
    "        # Randomly select pairs\n",
    "        selected_pairs = random.sample(chatgpt_pairs, n_pos_chatgpt)\n",
    "        \n",
    "        for idx1, idx2 in selected_pairs:\n",
    "            # Anchor\n",
    "            anchor_input_ids.append(chatgpt_tokens['input_ids'][idx1])\n",
    "            anchor_attention_mask.append(chatgpt_tokens['attention_mask'][idx1])\n",
    "            \n",
    "            # Positive\n",
    "            positive_input_ids.append(chatgpt_tokens['input_ids'][idx2])\n",
    "            positive_attention_mask.append(chatgpt_tokens['attention_mask'][idx2])\n",
    "            \n",
    "            # Negative (from Student)\n",
    "            neg_idx = np.random.choice(n_student)\n",
    "            negative_input_ids.append(student_tokens['input_ids'][neg_idx])\n",
    "            negative_attention_mask.append(student_tokens['attention_mask'][neg_idx])\n",
    "            \n",
    "            # Label (1 for positive pair)\n",
    "            labels.append(1)\n",
    "    \n",
    "    # Generate negative student-chatgpt pairs\n",
    "    if n_neg_pairs > 0:\n",
    "        # Create all possible student-chatgpt pairs\n",
    "        negative_pairs = [(i, j) for i in range(n_student) for j in range(n_chatgpt)]\n",
    "        # Randomly select pairs\n",
    "        selected_pairs = random.sample(negative_pairs, n_neg_pairs)\n",
    "        \n",
    "        for student_idx, chatgpt_idx in selected_pairs:\n",
    "            # Anchor (Student)\n",
    "            anchor_input_ids.append(student_tokens['input_ids'][student_idx])\n",
    "            anchor_attention_mask.append(student_tokens['attention_mask'][student_idx])\n",
    "            \n",
    "            # Negative (ChatGPT)\n",
    "            negative_input_ids.append(chatgpt_tokens['input_ids'][chatgpt_idx])\n",
    "            negative_attention_mask.append(chatgpt_tokens['attention_mask'][chatgpt_idx])\n",
    "            \n",
    "            # Positive (another Student different from anchor)\n",
    "            available_pos = [i for i in range(n_student) if i != student_idx]\n",
    "            if available_pos:  # Ensure there are available indices\n",
    "                pos_idx = np.random.choice(available_pos)\n",
    "                positive_input_ids.append(student_tokens['input_ids'][pos_idx])\n",
    "                positive_attention_mask.append(student_tokens['attention_mask'][pos_idx])\n",
    "                \n",
    "                # Label (0 for negative pair)\n",
    "                labels.append(0)\n",
    "    \n",
    "    # Count actual pairs created\n",
    "    actual_pairs = len(labels)\n",
    "    \n",
    "    # Convert to tensors and return\n",
    "    return {\n",
    "        'anchor': {\n",
    "            'input_ids': tf.convert_to_tensor(anchor_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(anchor_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'positive': {\n",
    "            'input_ids': tf.convert_to_tensor(positive_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(positive_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'negative': {\n",
    "            'input_ids': tf.convert_to_tensor(negative_input_ids, dtype=tf.int32),\n",
    "            'attention_mask': tf.convert_to_tensor(negative_attention_mask, dtype=tf.int32)\n",
    "        },\n",
    "        'labels': tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    }, actual_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrastive pairs\n",
    "std_gpt_tr_pairs, total_pairs_tr = create_contrastive_pairs(std_tr_tokens, gpt_tr_tokens, max_pairs=99999)\n",
    "std_gpt_va_pairs, total_pairs_va = create_contrastive_pairs(std_va_tokens, gpt_va_tokens, max_pairs=99999)\n",
    "\n",
    "# Show the number of pairs created\n",
    "print(f\"Total contrastive pairs (training) created: {total_pairs_tr}\")\n",
    "print(f\"- Positive pairs student-student: {sum(1 for label in std_gpt_tr_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Positive pairs chatgpt-chatgpt: {sum(1 for label in std_gpt_tr_pairs['labels'].numpy() if label == 1)//2}\")\n",
    "print(f\"- Negative pairs student-chatgpt: {sum(1 for label in std_gpt_tr_pairs['labels'].numpy() if label == 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7571a6",
   "metadata": {},
   "source": [
    "# Build Triplet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc30b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for training with triplet loss\n",
    "def triplet_model(semantic_model):\n",
    "    \"\"\"\n",
    "    Build a model for training with triplet loss.\n",
    "    \n",
    "    Args:\n",
    "        semantic_model: The semantic similarity model to be trained.\n",
    "        \n",
    "    Returns:\n",
    "         tf.keras.Model: Model for training with triplet loss.\n",
    "    \"\"\"\n",
    "    # Input for anchor, positive, and negative\n",
    "    anchor_input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"anchor_input_ids\")\n",
    "    anchor_attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"anchor_attention_mask\")\n",
    "    \n",
    "    positive_input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"positive_input_ids\")\n",
    "    positive_attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"positive_attention_mask\")\n",
    "    \n",
    "    negative_input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"negative_input_ids\")\n",
    "    negative_attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"negative_attention_mask\")\n",
    "    \n",
    "    # Embedding for anchor, positive, and negative\n",
    "    anchor_embedding = semantic_model([anchor_input_ids, anchor_attention_mask])\n",
    "    positive_embedding = semantic_model([positive_input_ids, positive_attention_mask])\n",
    "    negative_embedding = semantic_model([negative_input_ids, negative_attention_mask])\n",
    "    \n",
    "    # measure cosine similarity\n",
    "    pos_similarity = tf.reduce_sum(anchor_embedding * positive_embedding, axis=1)\n",
    "    neg_similarity = tf.reduce_sum(anchor_embedding * negative_embedding, axis=1)\n",
    "    \n",
    "    # Model output is the similarity score\n",
    "    output = tf.stack([pos_similarity, neg_similarity], axis=1)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[\n",
    "            anchor_input_ids, anchor_attention_mask,\n",
    "            positive_input_ids, positive_attention_mask,\n",
    "            negative_input_ids, negative_attention_mask\n",
    "        ],\n",
    "        outputs=output\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659fd85",
   "metadata": {},
   "source": [
    "# Triplet Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss function\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Triplet loss: tunes the network such that\n",
    "the distance between a and p is smaller than the\n",
    "distance between a and n.\n",
    "    \n",
    "    Args:\n",
    "        y_true: not used triplet loss.\n",
    "        y_pred: stack of [positive_similarity, negative_similarity].\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: loss value.\n",
    "    \"\"\"\n",
    "    pos_sim = y_pred[:, 0]\n",
    "    neg_sim = y_pred[:, 1]\n",
    "    margin = 0.5\n",
    "    \n",
    "    # Triplet loss: max(0, margin - (pos_sim - neg_sim))\n",
    "    loss = tf.maximum(0., margin - (pos_sim - neg_sim))\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb09fe5",
   "metadata": {},
   "source": [
    "# Train IndoBERT Semantic Similarity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build triplet model for student vs ChatGPT essay\n",
    "build_triplet = triplet_model(semantic_model)\n",
    "\n",
    "# Compile model\n",
    "build_triplet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=triplet_loss\n",
    ")\n",
    "\n",
    "# Training model Student_ChatGPT\n",
    "print(\"Training model...\")\n",
    "history = build_triplet.fit(\n",
    "    x=[\n",
    "        std_gpt_tr_pairs['anchor']['input_ids'],\n",
    "        std_gpt_tr_pairs['anchor']['attention_mask'],\n",
    "        std_gpt_tr_pairs['positive']['input_ids'],\n",
    "        std_gpt_tr_pairs['positive']['attention_mask'],\n",
    "        std_gpt_tr_pairs['negative']['input_ids'],\n",
    "        std_gpt_tr_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    y=std_gpt_tr_pairs['labels'], \n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_data=(\n",
    "    [\n",
    "        std_gpt_va_pairs['anchor']['input_ids'],\n",
    "        std_gpt_va_pairs['anchor']['attention_mask'],\n",
    "        std_gpt_va_pairs['positive']['input_ids'],\n",
    "        std_gpt_va_pairs['positive']['attention_mask'],\n",
    "        std_gpt_va_pairs['negative']['input_ids'],\n",
    "        std_gpt_va_pairs['negative']['attention_mask']\n",
    "    ],\n",
    "    std_gpt_va_pairs['labels']\n",
    "    ),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a46e7b",
   "metadata": {},
   "source": [
    "# Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history untuk model Student_ChatGPT\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Student_ChatGPT: Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265bd25",
   "metadata": {},
   "source": [
    "# Generate Embeddings Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ae85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_emb(tokens, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate embeddings using IndoBERT in batches.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Token from text.\n",
    "        model: IndoBERT Semantic Similarity.\n",
    "        batch_size: Number of samples per batch.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    num_batches = len(tokens['input_ids']) // batch_size + (len(tokens['input_ids']) % batch_size > 0)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(tokens['input_ids']))\n",
    "\n",
    "        batch_input_ids = tokens['input_ids'][start_idx:end_idx]\n",
    "        batch_attention_mask = tokens['attention_mask'][start_idx:end_idx]\n",
    "        \n",
    "        batch_embeddings = model([batch_input_ids, batch_attention_mask])\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.concatenate(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bee85e",
   "metadata": {},
   "source": [
    "# Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Calculate similarity score using standard cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: First embedding (input text)\n",
    "        embedding2: Second embedding (reference)\n",
    "        \n",
    "    Returns:\n",
    "        float: Average cosine similarity score\n",
    "    \"\"\"\n",
    "    embedding1_norm = tf.nn.l2_normalize(embedding1, axis=-1)\n",
    "    embedding2_norm = tf.nn.l2_normalize(embedding2, axis=-1)\n",
    "    \n",
    "    similarities = tf.matmul(embedding1_norm, tf.transpose(embedding2_norm))\n",
    "    similarities = tf.reshape(similarities, [-1])\n",
    "\n",
    "    avg_similarity = tf.reduce_mean(similarities).numpy()\n",
    "    return avg_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2da6b",
   "metadata": {},
   "source": [
    "# Generate Embeddings (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70124035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all text\n",
    "print(\"Generating embeddings for Student (all text)...\")\n",
    "std_emb = gen_emb(std_tokens, semantic_model, batch_size=32)\n",
    "print(\"Generating embeddings for ChatGPT (all text)...\")\n",
    "gpt_emb = gen_emb(gpt_tokens, semantic_model, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3a0ce",
   "metadata": {},
   "source": [
    "# Measure Similarity Score (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_std_sim_scores = []\n",
    "std_gpt_sim_scores = []\n",
    "for emb in std_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    std_std_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    std_gpt_sim_scores.append(avg_similarity)\n",
    "\n",
    "gpt_std_sim_scores = []\n",
    "gpt_gpt_sim_scores = []\n",
    "for emb in gpt_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    gpt_std_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    gpt_gpt_sim_scores.append(avg_similarity)\n",
    "\n",
    "std_sim_scores = np.array([\n",
    "    std_std_sim_scores, \n",
    "    std_gpt_sim_scores\n",
    "])\n",
    "\n",
    "gpt_sim_scores = np.array([\n",
    "    gpt_std_sim_scores, \n",
    "    gpt_gpt_sim_scores\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d747f",
   "metadata": {},
   "source": [
    "# Visualization of Similarity Score (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43555acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "student_student_color = \"#9747FF\"  \n",
    "student_chatgpt_color = \"#FCD19C\"  \n",
    "chatgpt_chatgpt_color = \"#FFA629\"  \n",
    "chatgpt_student_color = \"#E4CCFF\"  \n",
    "\n",
    "# Subplot 1: Student\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(range(len(std_std_sim_scores)), std_std_sim_scores, \n",
    "            label='Student-Student', color=student_student_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.scatter(range(len(std_gpt_sim_scores)), std_gpt_sim_scores, \n",
    "            label='Student-ChatGPT', color=student_chatgpt_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.title('Student Essay', fontsize=14, fontweight='bold')\n",
    "plt.ylim(-1.05, 1.05)  \n",
    "plt.yticks(np.arange(-1, 1.1, 0.1))\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Subplot 2: ChatGPT\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(range(len(gpt_std_sim_scores)), gpt_std_sim_scores, \n",
    "            label='ChatGPT-Student', color=chatgpt_student_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.scatter(range(len(gpt_gpt_sim_scores)), gpt_gpt_sim_scores, \n",
    "            label='ChatGPT-ChatGPT', color=chatgpt_chatgpt_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.title('ChatGPT Essay', fontsize=14, fontweight='bold')\n",
    "plt.ylim(-1.05, 1.05)  \n",
    "plt.yticks(np.arange(-1, 1.1, 0.1))\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Similarity Scores Comparison All Text', fontsize=16, fontweight='bold', y=0.98, ha='center')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  \n",
    "\n",
    "os.makedirs('ta_sentence/images', exist_ok=True)\n",
    "plt.savefig('ta_sentence/images/similarity_scores_comparison(all).png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ca615",
   "metadata": {},
   "source": [
    "# Generate Embeddings (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234dea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for data test\n",
    "print(\"Generating embeddings for Student (test set)...\")\n",
    "std_te_emb = gen_emb(std_te_tokens, semantic_model, batch_size=32)\n",
    "print(\"Generating embeddings for ChatGPT (test set)...\")\n",
    "gpt_te_emb = gen_emb(gpt_te_tokens, semantic_model, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a78f7e",
   "metadata": {},
   "source": [
    "# Measure Similarity Score (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_std_te_sim_scores = []\n",
    "std_gpt_te_sim_scores = []\n",
    "for emb in std_te_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    std_std_te_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    std_gpt_te_sim_scores.append(avg_similarity)\n",
    "\n",
    "gpt_std_te_sim_scores = []\n",
    "gpt_gpt_te_sim_scores = []\n",
    "for emb in gpt_te_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    gpt_std_te_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    gpt_gpt_te_sim_scores.append(avg_similarity)\n",
    "\n",
    "std_te_sim_scores = np.array([\n",
    "    std_std_te_sim_scores, \n",
    "    std_gpt_te_sim_scores\n",
    "])\n",
    "\n",
    "gpt_te_sim_scores = np.array([\n",
    "    gpt_std_te_sim_scores, \n",
    "    gpt_gpt_te_sim_scores\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a3a6b",
   "metadata": {},
   "source": [
    "# Similarity Text Breakdown (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea813fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = []\n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "pd.set_option('display.width', 1000)        \n",
    "pd.set_option('display.max_rows', None)      \n",
    "\n",
    "for i, student_text in enumerate(std_sen):\n",
    "    if i >= len(std_te_emb):\n",
    "        continue\n",
    "        \n",
    "    student_embedding = tf.expand_dims(std_te_emb[i], 0)\n",
    "\n",
    "    for j in range(len(gpt_sen)):\n",
    "        if j >= len(gpt_te_emb):\n",
    "            continue\n",
    "\n",
    "        chatgpt_embedding = tf.expand_dims(gpt_te_emb[j], 0)\n",
    "        similarity = cos_sim(student_embedding, chatgpt_embedding)\n",
    "        \n",
    "        test_pairs.append({\n",
    "            'student_idx': i,\n",
    "            'chatgpt_idx': j,\n",
    "            'student_text': student_text,\n",
    "            'chatgpt_text': gpt_sen[j],\n",
    "            'similarity_score': similarity\n",
    "        })\n",
    "\n",
    "sorted_pairs = sorted(test_pairs, key=lambda x: x['similarity_score'], reverse=True)\n",
    "\n",
    "result_v1 = sorted_pairs[:5]\n",
    "\n",
    "df_v1 = pd.DataFrame(result_v1)[['student_text', 'chatgpt_text', 'similarity_score']]\n",
    "df_v1.columns = ['Student Essay', 'ChatGPT Essay', 'Similarity Score']\n",
    "\n",
    "print(\"=== 1: Standard ===\")\n",
    "display(df_v1)\n",
    "\n",
    "used_student_indices_v2 = set()\n",
    "used_chatgpt_indices_v2 = set()\n",
    "result_v2 = []\n",
    "\n",
    "for pair in sorted_pairs:\n",
    "    if (pair['student_idx'] in used_student_indices_v2 or \n",
    "        pair['chatgpt_idx'] in used_chatgpt_indices_v2):\n",
    "        continue\n",
    "    result_v2.append(pair)\n",
    "    used_student_indices_v2.add(pair['student_idx'])\n",
    "    used_chatgpt_indices_v2.add(pair['chatgpt_idx'])\n",
    "    if len(result_v2) >= 5:\n",
    "        break\n",
    "\n",
    "df_v2 = pd.DataFrame(result_v2)[['student_text', 'chatgpt_text', 'similarity_score']]\n",
    "df_v2.columns = ['Student Essay', 'ChatGPT Essay', 'Similarity Score']\n",
    "\n",
    "print(\"=== 2: Unique ===\")\n",
    "display(df_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d3c91",
   "metadata": {},
   "source": [
    "# Visualization of Similarity Score (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe10ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "student_student_color = \"#9747FF\"  \n",
    "student_chatgpt_color = \"#FCD19C\"  \n",
    "chatgpt_chatgpt_color = \"#FFA629\"  \n",
    "chatgpt_student_color = \"#E4CCFF\"  \n",
    "\n",
    "# Subplot 1: Student\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(range(len(std_std_te_sim_scores)), std_std_te_sim_scores, \n",
    "            label='Student-Student', color=student_student_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.scatter(range(len(std_gpt_te_sim_scores)), std_gpt_te_sim_scores, \n",
    "            label='Student-ChatGPT', color=student_chatgpt_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.title('Student Essay', fontsize=14, fontweight='bold')\n",
    "plt.ylim(-1.05, 1.05)  \n",
    "plt.yticks(np.arange(-1, 1.1, 0.1))\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Subplot 2: ChatGPT\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(range(len(gpt_std_te_sim_scores)), gpt_std_te_sim_scores, \n",
    "            label='ChatGPT-Student', color=chatgpt_student_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.scatter(range(len(gpt_gpt_te_sim_scores)), gpt_gpt_te_sim_scores, \n",
    "            label='ChatGPT-ChatGPT', color=chatgpt_chatgpt_color, s=70, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.title('ChatGPT Essay', fontsize=14, fontweight='bold')\n",
    "plt.ylim(-1.05, 1.05)  \n",
    "plt.yticks(np.arange(-1, 1.1, 0.1))\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Similarity Scores Comparison (Test Set)', fontsize=16, fontweight='bold', y=0.98, ha='center')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)  \n",
    "\n",
    "plt.savefig('ta_sentence/images/similarity_scores_comparison(test).png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69aad25",
   "metadata": {},
   "source": [
    "# Generate Embeddings (Training & Validation Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ecd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for training and validation sets\n",
    "print(\"Generating embeddings for Student (training set)...\")\n",
    "std_tr_emb = gen_emb(std_tr_tokens, semantic_model, batch_size=32)\n",
    "print(\"Generating embeddings for ChatGPT (training set)...\")\n",
    "gpt_tr_emb = gen_emb(gpt_tr_tokens, semantic_model, batch_size=32)\n",
    "\n",
    "print(\"Generating embeddings for Student (validation set)...\")\n",
    "std_va_emb = gen_emb(std_va_tokens, semantic_model, batch_size=32)\n",
    "print(\"Generating embeddings for ChatGPT (validation set)...\")\n",
    "gpt_va_emb = gen_emb(gpt_va_tokens, semantic_model, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1cec6",
   "metadata": {},
   "source": [
    "# Measure Similarity Score (Training & Validation Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc390814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "std_std_tr_sim_scores = []\n",
    "std_gpt_tr_sim_scores = []\n",
    "for emb in std_tr_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    std_std_tr_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    std_gpt_tr_sim_scores.append(avg_similarity)\n",
    "\n",
    "gpt_std_tr_sim_scores = []\n",
    "gpt_gpt_tr_sim_scores = []\n",
    "for emb in gpt_tr_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    gpt_std_tr_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    gpt_gpt_tr_sim_scores.append(avg_similarity)\n",
    "\n",
    "std_tr_sim_scores = np.array([\n",
    "    std_std_tr_sim_scores, \n",
    "    std_gpt_tr_sim_scores\n",
    "])\n",
    "\n",
    "gpt_tr_sim_scores = np.array([\n",
    "    gpt_std_tr_sim_scores, \n",
    "    gpt_gpt_tr_sim_scores\n",
    "])\n",
    "\n",
    "# Validation set\n",
    "std_std_va_sim_scores = []\n",
    "std_gpt_va_sim_scores = []\n",
    "for emb in std_va_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    std_std_va_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    std_gpt_va_sim_scores.append(avg_similarity)\n",
    "\n",
    "gpt_std_va_sim_scores = []\n",
    "gpt_gpt_va_sim_scores = []\n",
    "for emb in gpt_va_emb:\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), std_emb)\n",
    "    gpt_std_va_sim_scores.append(avg_similarity)\n",
    "\n",
    "    avg_similarity = cos_sim(tf.expand_dims(emb, 0), gpt_emb)\n",
    "    gpt_gpt_va_sim_scores.append(avg_similarity)\n",
    "\n",
    "std_va_sim_scores = np.array([\n",
    "    std_std_va_sim_scores, \n",
    "    std_gpt_va_sim_scores\n",
    "])\n",
    "\n",
    "gpt_va_sim_scores = np.array([\n",
    "    gpt_std_va_sim_scores, \n",
    "    gpt_gpt_va_sim_scores\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ab90c",
   "metadata": {},
   "source": [
    "# Define Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_features(text):\n",
    "    \"\"\"\n",
    "    Features extraction from text:\n",
    "    1.\tLexical Diversity\n",
    "    2.\tTotal words in the essay\n",
    "    3.\tTotal unique words*\n",
    "    4.\tModals\n",
    "    5.\tStopwords ratio*\n",
    "    6.\tAverage sentence length*\n",
    "    7.\tSentence length variation*\n",
    "    8.\tPunctuation Ratio*\n",
    "\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Linguistic features.\n",
    "    \"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    word_count = len(words)\n",
    "    unique_count = len(set(words))\n",
    "    \n",
    "    ld = (unique_count / word_count * 100) if word_count > 0 else 0\n",
    "    \n",
    "    # Load modals from corpus file\n",
    "    modals = set()\n",
    "    if os.path.exists('corpus/Indonesian_Manually_Tagged_Corpus_ID.tsv'):\n",
    "        with open('corpus/Indonesian_Manually_Tagged_Corpus_ID.tsv', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2 and parts[1] == 'MD':\n",
    "                        modals.add(parts[0].lower())\n",
    "    \n",
    "    # Count modals in text\n",
    "    modal_count = sum(1 for word in words if word.lower() in modals)\n",
    "    \n",
    "    # Load stopwords from file\n",
    "    stopwords = set()\n",
    "    if os.path.exists('corpus/stopwords.txt'):\n",
    "        with open('corpus/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                stopwords.add(line.strip())\n",
    "    \n",
    "    # Calculate stopword ratio\n",
    "    stopword_count = sum(1 for word in words if word.lower() in stopwords)\n",
    "    stopword_ratio = (stopword_count / word_count * 100) if word_count > 0 else 0\n",
    "    \n",
    "    # Calculate sentence length statistics\n",
    "    sentence_lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n",
    "    avg_sent_len = np.mean(sentence_lengths) if sentence_lengths else 0\n",
    "    sent_len_var = np.std(sentence_lengths) if len(sentence_lengths) > 1 else 0\n",
    "    \n",
    "    # Calculate punctuation ratio\n",
    "    punct_count = len(re.findall(r'[.!?]', text))\n",
    "    punct_ratio = (punct_count / word_count) * 100 if word_count > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'lexical_diversity': ld,\n",
    "        'total_words': word_count,\n",
    "        'total_unique_words': unique_count,\n",
    "        'modals': modal_count,\n",
    "        'stopwords_ratio': stopword_ratio,\n",
    "        'avg_sentence_length': avg_sent_len,\n",
    "        'sentence_length_variation': sent_len_var,\n",
    "        'punctuation_ratio': punct_ratio\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb7a06",
   "metadata": {},
   "source": [
    "# Features Extraction (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for Student and ChatGPT essays\n",
    "print(\"Features extraction for Student...\")\n",
    "std_features = [linguistic_features(text) for text in std_sen]\n",
    "\n",
    "print(\"Features extraction for ChatGPT...\")\n",
    "gpt_features = [linguistic_features(text) for text in gpt_sen]\n",
    "\n",
    "\n",
    "# convert\n",
    "std_features_df = pd.DataFrame(std_features)\n",
    "gpt_features_df = pd.DataFrame(gpt_features)\n",
    "\n",
    "\n",
    "# Show the first few rows of the features DataFrames\n",
    "print(\"\\nStudent Linguistic Features:\")\n",
    "display(std_features_df.head())\n",
    "\n",
    "print(\"\\nChatGPT Linguistic Features:\")\n",
    "display(gpt_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2ccff",
   "metadata": {},
   "source": [
    "# Normalize Linguistic Features (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features into a single DataFrame\n",
    "all_features = pd.concat([std_features_df, gpt_features_df], axis=0)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(all_features)\n",
    "\n",
    "# Separate normalized features back into student and ChatGPT\n",
    "n_student = len(std_features_df)\n",
    "n_chatgpt = len(gpt_features_df)\n",
    "\n",
    "std_features_norm = normalized_features[:n_student]\n",
    "gpt_features_norm = normalized_features[n_student:n_student + n_chatgpt]\n",
    "\n",
    "print(\"Student features after normalization:\")\n",
    "print(std_features_norm[:5])\n",
    "\n",
    "print(\"ChatGPT features after normalization:\")\n",
    "print(gpt_features_norm[:5])\n",
    "\n",
    "# Save scaler for later inference\n",
    "try:\n",
    "    with open('ta_sentence/scaler_stylometric.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(\"Scaler saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving scaler: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87746ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to features\n",
    "std_features_df['label'] = 'Student Essay'\n",
    "gpt_features_df['label'] = 'ChatGPT Essay'\n",
    "\n",
    "# Combine datasets\n",
    "combined_features = pd.concat([std_features_df, gpt_features_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e7238",
   "metadata": {},
   "source": [
    "# Visualize Linguistic Features (All Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "for i, feature in enumerate([\n",
    "    'lexical_diversity',\n",
    "    'total_words',\n",
    "    'total_unique_words',\n",
    "    'modals',\n",
    "    'stopwords_ratio',\n",
    "    'avg_sentence_length',\n",
    "    'sentence_length_variation',\n",
    "    'punctuation_ratio'\n",
    "]):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    sns.violinplot(x='label', y=feature, data=combined_features, inner=None, alpha=0.6, linewidth=1.5)\n",
    "    sns.boxplot(x='label', y=feature, data=combined_features, width=0.4, \n",
    "                saturation=1, showfliers=True, color='white', linewidth=1.5)\n",
    "    sns.stripplot(x='label', y=feature, data=combined_features, color='red', alpha=0.2, size=6, jitter=True, dodge=True)\n",
    "    plt.title(f'Distribution of {feature.replace(\"_\", \" \").title()}', fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "plt.suptitle('Linguistic Features All Text', fontsize=32, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(pad=3.0)\n",
    "\n",
    "plt.savefig('ta_sentence/images/linguistic_features_comparison(all).png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b34c0f",
   "metadata": {},
   "source": [
    "# Feature Extraction (Training, Validation, & Test Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for Student and ChatGPT essays\n",
    "print(\"Features extraction for Student (training set)...\")\n",
    "std_tr_features = [linguistic_features(text) for text in train_set[train_set['label'] == 0]['text'].tolist()]\n",
    "print(\"Features extraction for ChatGPT (training set)...\")\n",
    "gpt_tr_features = [linguistic_features(text) for text in train_set[train_set['label'] == 1]['text'].tolist()]\n",
    "\n",
    "print(\"Features extraction for Student (validation set)...\")\n",
    "std_va_features = [linguistic_features(text) for text in val_set[val_set['label'] == 0]['text'].tolist()]\n",
    "print(\"Features extraction for ChatGPT (validation set)...\")\n",
    "gpt_va_features = [linguistic_features(text) for text in val_set[val_set['label'] == 1]['text'].tolist()]\n",
    "\n",
    "print(\"Features extraction for Student (test set)...\")\n",
    "std_te_features = [linguistic_features(text) for text in test_set[test_set['label'] == 0]['text'].tolist()]\n",
    "print(\"Features extraction for ChatGPT (test set)...\")\n",
    "gpt_te_features = [linguistic_features(text) for text in test_set[test_set['label'] == 1]['text'].tolist()]\n",
    "\n",
    "# convert\n",
    "std_tr_features_df = pd.DataFrame(std_tr_features)\n",
    "gpt_tr_features_df = pd.DataFrame(gpt_tr_features)\n",
    "\n",
    "std_va_features_df = pd.DataFrame(std_va_features)\n",
    "gpt_va_features_df = pd.DataFrame(gpt_va_features)\n",
    "\n",
    "std_te_features_df = pd.DataFrame(std_te_features)\n",
    "gpt_te_features_df = pd.DataFrame(gpt_te_features)\n",
    "\n",
    "#Show the first few rows of the features DataFrames\n",
    "print(\"\\nStudent Linguistic Features (Test Set):\")\n",
    "display(std_te_features_df.head())\n",
    "\n",
    "print(\"\\nChatGPT Linguistic Features (Test Set):\")\n",
    "display(gpt_te_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_tr_features_df.shape, gpt_tr_features_df.shape, std_va_features_df.shape, gpt_va_features_df.shape, std_te_features_df.shape, gpt_te_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa34444",
   "metadata": {},
   "source": [
    "# Normalize Linguistic Features (Training, Validation, & Test Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features into a single DataFrame\n",
    "all_tr_features = pd.concat([std_tr_features_df, gpt_tr_features_df], axis=0)\n",
    "all_va_features = pd.concat([std_va_features_df, gpt_va_features_df], axis=0)\n",
    "all_te_features = pd.concat([std_te_features_df, gpt_te_features_df], axis=0)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "normalized_tr_features = scaler.fit_transform(all_tr_features)\n",
    "normalized_va_features = scaler.fit_transform(all_va_features)\n",
    "normalized_te_features = scaler.fit_transform(all_te_features)\n",
    "\n",
    "# Separate normalized features back into student and ChatGPT\n",
    "n_student_tr = len(std_tr_features_df)\n",
    "n_chatgpt_tr = len(gpt_tr_features_df)\n",
    "\n",
    "n_student_va = len(std_va_features_df)\n",
    "n_chatgpt_va = len(gpt_va_features_df)\n",
    "\n",
    "n_student_te = len(std_te_features_df)\n",
    "n_chatgpt_te = len(gpt_te_features_df)\n",
    "\n",
    "std_tr_features_norm = normalized_tr_features[:n_student_tr]\n",
    "gpt_tr_features_norm = normalized_tr_features[n_student_tr:n_student_tr + n_chatgpt_tr]\n",
    "\n",
    "std_va_features_norm = normalized_va_features[:n_student_va]\n",
    "gpt_va_features_norm = normalized_va_features[n_student_va:n_student_va + n_chatgpt_va]\n",
    "\n",
    "std_te_features_norm = normalized_te_features[:n_student_te]\n",
    "gpt_te_features_norm = normalized_te_features[n_student_te:n_student_te + n_chatgpt_te]\n",
    "\n",
    "print(\"Student features after normalization (training):\")\n",
    "print(std_tr_features_norm[:5])\n",
    "\n",
    "print(\"ChatGPT features after normalization (training):\")\n",
    "print(gpt_tr_features_norm[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a768f1",
   "metadata": {},
   "source": [
    "# Visualize Linguistic Features (Data Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to features\n",
    "std_tr_features_df['label'] = 'Student Essay'\n",
    "gpt_tr_features_df['label'] = 'ChatGPT Essay'\n",
    "\n",
    "# Combine datasets\n",
    "combined_tr_features = pd.concat([std_tr_features_df, gpt_tr_features_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_func():\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    for i, feature in enumerate([\n",
    "    'lexical_diversity',\n",
    "    'total_words',\n",
    "    'total_unique_words',\n",
    "    'modals',\n",
    "    'stopwords_ratio',\n",
    "    'avg_sentence_length',\n",
    "    'sentence_length_variation',\n",
    "    'punctuation_ratio'\n",
    "]):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        sns.violinplot(x='label', y=feature, data=combined_tr_features, inner=None, alpha=0.6, linewidth=1.5)\n",
    "        sns.boxplot(x='label', y=feature, data=combined_tr_features, width=0.4, \n",
    "                saturation=1, showfliers=True, color='white', linewidth=1.5)\n",
    "        sns.stripplot(x='label', y=feature, data=combined_tr_features, color='red', alpha=0.2, size=6, jitter=True, dodge=True)\n",
    "        plt.title(f'Distribution of {feature.replace(\"_\", \" \").title()}', fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "\n",
    "new_func()\n",
    "\n",
    "plt.suptitle('Linguistic Features Data Test', fontsize=32, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(pad=3.0)\n",
    "\n",
    "plt.savefig('ta_sentence/images/linguistic_features_comparison(test).png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6801f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions of your embedding arrays\n",
    "print(\"std_te_emb shape:\", std_te_emb.shape)\n",
    "print(\"gpt_te_emb shape:\", gpt_te_emb.shape)\n",
    "\n",
    "\n",
    "# Print shapes of the resulting similarity scores\n",
    "print(\"std_std_te_sim_scores shape:\", np.array(std_std_te_sim_scores).shape)\n",
    "print(\"std_gpt_te_sim_scores shape:\", np.array(std_gpt_te_sim_scores).shape)\n",
    "print(\"gpt_std_te_sim_scores shape:\", np.array(gpt_std_te_sim_scores).shape)\n",
    "print(\"gpt_gpt_te_sim_scores shape:\", np.array(gpt_gpt_te_sim_scores).shape)\n",
    "\n",
    "# Check shapes of the combined arrays\n",
    "print(\"std_te_sim_scores shape:\", std_te_sim_scores.shape)\n",
    "print(\"gpt_te_sim_scores shape:\", gpt_te_sim_scores.shape)\n",
    "\n",
    "print(\"std_te_features_norm shape:\", std_te_features_norm.shape)\n",
    "print(\"gpt_te_features_norm shape:\", gpt_te_features_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ccbca",
   "metadata": {},
   "source": [
    "# Data Preparation for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90390e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine embeddings for model 1 (already correct)\n",
    "emb_tr_features = np.vstack([std_tr_emb, gpt_tr_emb])\n",
    "emb_va_features = np.vstack([std_va_emb, gpt_va_emb])\n",
    "emb_te_features = np.vstack([std_te_emb, gpt_te_emb])\n",
    "\n",
    "std_tr_features_selected = std_tr_features_norm\n",
    "gpt_tr_features_selected = gpt_tr_features_norm\n",
    "\n",
    "std_va_features_selected = std_va_features_norm\n",
    "gpt_va_features_selected = gpt_va_features_norm\n",
    "\n",
    "std_te_features_selected = std_te_features_norm\n",
    "gpt_te_features_selected = gpt_te_features_norm\n",
    "\n",
    "linguistic_tr_features = np.vstack([\n",
    "    std_tr_features_selected,\n",
    "    gpt_tr_features_selected\n",
    "])\n",
    "linguistic_va_features = np.vstack([\n",
    "    std_va_features_selected,\n",
    "    gpt_va_features_selected\n",
    "])\n",
    "linguistic_te_features = np.vstack([\n",
    "    std_te_features_selected,\n",
    "    gpt_te_features_selected\n",
    "])\n",
    "\n",
    "std_tr_sim_scores_transposed = std_tr_sim_scores.T \n",
    "gpt_tr_sim_scores_transposed = gpt_tr_sim_scores.T\n",
    "similarity_tr_scores = np.vstack([\n",
    "    std_tr_sim_scores_transposed,\n",
    "    gpt_tr_sim_scores_transposed,\n",
    "])\n",
    "\n",
    "std_va_sim_scores_transposed = std_va_sim_scores.T \n",
    "gpt_va_sim_scores_transposed = gpt_va_sim_scores.T\n",
    "similarity_va_scores = np.vstack([\n",
    "    std_va_sim_scores_transposed,\n",
    "    gpt_va_sim_scores_transposed,\n",
    "])\n",
    "std_te_sim_scores_transposed = std_te_sim_scores.T \n",
    "gpt_te_sim_scores_transposed = gpt_te_sim_scores.T\n",
    "similarity_te_scores = np.vstack([\n",
    "    std_te_sim_scores_transposed,\n",
    "    gpt_te_sim_scores_transposed,\n",
    "])\n",
    "\n",
    "# Create labels\n",
    "std_tr_labels = np.zeros(len(train_set[train_set['label'] == 0]))\n",
    "gpt_tr_labels = np.ones(len(train_set[train_set['label'] == 1]))\n",
    "tr_labels = np.hstack([std_tr_labels, gpt_tr_labels])\n",
    "\n",
    "# Validation set labels\n",
    "std_va_labels = np.zeros(len(val_set[val_set['label'] == 0]))\n",
    "gpt_va_labels = np.ones(len(val_set[val_set['label'] == 1]))\n",
    "va_labels = np.hstack([std_va_labels, gpt_va_labels])\n",
    "\n",
    "# Test set labels\n",
    "std_te_labels = np.zeros(len(test_set[test_set['label'] == 0]))\n",
    "gpt_te_labels = np.ones(len(test_set[test_set['label'] == 1]))\n",
    "te_labels = np.hstack([std_te_labels, gpt_te_labels])\n",
    "\n",
    "tr_labels = tr_labels.astype(int)\n",
    "va_labels = va_labels.astype(int)\n",
    "te_labels = te_labels.astype(int)\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"emb_te_features shape: {emb_te_features.shape}\")\n",
    "print(f\"linguistic_te_features shape: {linguistic_te_features.shape}\")\n",
    "print(f\"similarity_te_scores shape: {similarity_te_scores.shape}\")\n",
    "print(f\"te_labels shape: {te_labels.shape}\")\n",
    "\n",
    "print(f\"emb_te_features value: {emb_te_features[:5]}\")\n",
    "print(f\"linguistic_te_features value: {linguistic_te_features[:5]}\")\n",
    "print(f\"similarity_te_scores value: {similarity_te_scores[:5]}\")\n",
    "print(f\"te_labels value: {te_labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e31a6",
   "metadata": {},
   "source": [
    "# Build Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_input = tf.keras.layers.Input(\n",
    "    shape=(128,),\n",
    "    dtype=tf.float32, \n",
    "    name=\"embeddings\"\n",
    ")\n",
    "\n",
    "sim_score_input = tf.keras.layers.Input(\n",
    "    shape=(2,), \n",
    "    dtype=tf.float32, \n",
    "    name=\"similarity_score\"\n",
    ")\n",
    "\n",
    "linguistic_input = tf.keras.layers.Input(\n",
    "    shape=(8,),\n",
    "    dtype=tf.float32, \n",
    "    name=\"linguistic_features\"\n",
    ")\n",
    "\n",
    "emb_dense = tf.keras.layers.Dense(128, activation=\"relu\")(emb_input)\n",
    "sim_dense = tf.keras.layers.Dense(16, activation=\"relu\")(sim_score_input)\n",
    "lin_dense = tf.keras.layers.Dense(64, activation=\"relu\")(linguistic_input)\n",
    "\n",
    "combined = tf.keras.layers.Concatenate()([emb_dense, sim_dense, lin_dense])\n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(combined)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "classifier = tf.keras.Model(\n",
    "    inputs=[emb_input, sim_score_input, linguistic_input],\n",
    "    outputs=output,\n",
    "    name=\"text_classifier\"\n",
    ")\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde60e7",
   "metadata": {},
   "source": [
    "# Create Data Input for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c743699",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = {\n",
    "    \"embeddings\": emb_tr_features,\n",
    "    \"similarity_score\": similarity_tr_scores,\n",
    "    \"linguistic_features\": linguistic_tr_features\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    \"embeddings\": emb_va_features,\n",
    "    \"similarity_score\": similarity_va_scores,\n",
    "    \"linguistic_features\": linguistic_va_features\n",
    "}\n",
    "\n",
    "\n",
    "test_inputs = {\n",
    "    \"embeddings\": emb_te_features,\n",
    "    \"similarity_score\": similarity_te_scores,\n",
    "    \"linguistic_features\": linguistic_te_features\n",
    "}\n",
    "\n",
    "train_labels = tr_labels\n",
    "val_labels = va_labels\n",
    "test_labels = te_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a3990",
   "metadata": {},
   "source": [
    "# Train Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifier\n",
    "print(\"Training Classification Model...\")\n",
    "history_classifier = classifier.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_classifier.history['loss'], label='Training Loss')\n",
    "plt.plot(history_classifier.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_classifier.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_classifier.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd11e4",
   "metadata": {},
   "source": [
    "# Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine only training and validation inputs and labels (excluding test data)\n",
    "combined_inputs = {\n",
    "    \"embeddings\": np.concatenate([train_inputs[\"embeddings\"], val_inputs[\"embeddings\"]]),\n",
    "    \"similarity_score\": np.concatenate([train_inputs[\"similarity_score\"], val_inputs[\"similarity_score\"]]),\n",
    "    \"linguistic_features\": np.concatenate([train_inputs[\"linguistic_features\"], val_inputs[\"linguistic_features\"]])\n",
    "}\n",
    "combined_labels = np.concatenate([train_labels, val_labels])\n",
    "\n",
    "# Make predictions on combined training and validation data\n",
    "print(\"Making predictions on training and validation data...\")\n",
    "combined_predictions = classifier.predict(combined_inputs)\n",
    "\n",
    "# Calculate ROC curve and plot\n",
    "print(\"Generating ROC curve...\")\n",
    "fpr, tpr, _ = roc_curve(combined_labels, combined_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Training + Validation)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ta_sentence/images/roc_curve (train_val).png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate performance at different thresholds\n",
    "print(\"Analyzing threshold performance...\")\n",
    "percentiles = np.linspace(0, 100, num=101)\n",
    "sensitivity_data = []  \n",
    "specificity_data = [] \n",
    "intersection_points = []\n",
    "\n",
    "for p in percentiles:\n",
    "    threshold = np.percentile(combined_predictions, p)\n",
    "    predictions_binary = (combined_predictions >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(combined_labels, predictions_binary).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    sensitivity_data.append(sensitivity)\n",
    "    specificity_data.append(specificity)\n",
    "    \n",
    "    # Identify where sensitivity and specificity are approximately equal\n",
    "    if np.isclose(sensitivity, specificity, atol=1e-2):\n",
    "        intersection_points.append((p, threshold, sensitivity))\n",
    "\n",
    "# Plot sensitivity-specificity tradeoff\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(percentiles, sensitivity_data, label='Sensitivity (True Positive Rate)', color='blue', lw=2, linestyle='--')\n",
    "plt.plot(percentiles, specificity_data, label='Specificity (True Negative Rate)', color='green', lw=2)\n",
    "\n",
    "# Highlight balanced points\n",
    "for p, threshold, rate in intersection_points:\n",
    "    plt.scatter(p, rate, color='red')\n",
    "\n",
    "plt.xlabel('Percentile Score Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Sensitivity-Specificity Tradeoff (Training + Validation)')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('ta_sentence/images/tradeoff_with_intersections (train_val).png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Threshold points with equal sensitivity and specificity:\")\n",
    "for p, threshold, rate in intersection_points:\n",
    "    print(f\"Percentile: {p:.1f}, Threshold: {threshold:.4f}, Rate: {rate:.4f}\")\n",
    "\n",
    "if intersection_points:\n",
    "    _, optimal_threshold, _ = intersection_points[0]\n",
    "else:\n",
    "    optimal_threshold = np.percentile(combined_predictions, 50)\n",
    "\n",
    "print(f\"Selected optimal threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Apply optimal threshold for final predictions\n",
    "combined_predictions_binary = (combined_predictions >= optimal_threshold).astype(int)\n",
    "\n",
    "# Generate confusion matrix with custom styling\n",
    "cm_combined = confusion_matrix(combined_labels, combined_predictions_binary)\n",
    "\n",
    "# Define custom colors for confusion matrix\n",
    "colors = np.array([\n",
    "    [\"#9747FF\", \"#FCD19C\"],  \n",
    "    [\"#E4CCFF\", \"#FFA629\"]\n",
    "])\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "rows, cols = cm_combined.shape\n",
    "row_ind, col_ind = np.meshgrid(np.arange(rows), np.arange(cols), indexing='ij')\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        plt.fill_between([j, j+1], [rows-i-1, rows-i-1], [rows-i, rows-i], color=colors[i, j])\n",
    "        plt.text(j+0.5, rows-i-0.5, str(cm_combined[i, j]), ha='center', va='center', \n",
    "                 color='black', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.title('Confusion Matrix (Training + Validation)', fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize=14)\n",
    "plt.ylabel('Actual', fontsize=14)\n",
    "plt.xticks([0.5, 1.5], ['Student', 'ChatGPT'], fontsize=12)\n",
    "plt.yticks([0.5, 1.5], ['ChatGPT', 'Student'], fontsize=12)\n",
    "plt.xlim(0, 2)\n",
    "plt.ylim(0, 2)\n",
    "plt.savefig('ta_sentence/images/confusion_matrix(train_val).png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report (Training + Validation):\")\n",
    "print(classification_report(combined_labels, combined_predictions_binary, target_names=['Student', 'ChatGPT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data only\n",
    "print(\"Making predictions on test dataset...\")\n",
    "test_predictions = classifier.predict(test_inputs)\n",
    "\n",
    "test_predictions_binary = (test_predictions >= optimal_threshold).astype(int)\n",
    "\n",
    "cm_test = confusion_matrix(test_labels, test_predictions_binary)\n",
    "\n",
    "# Define custom colors for confusion matrix\n",
    "colors = np.array([\n",
    "    [\"#9747FF\", \"#FCD19C\"],  \n",
    "    [\"#E4CCFF\", \"#FFA629\"]\n",
    "])\n",
    "\n",
    "# Visualize confusion matrix for test data\n",
    "plt.figure(figsize=(10, 8))\n",
    "rows, cols = cm_test.shape\n",
    "row_ind, col_ind = np.meshgrid(np.arange(rows), np.arange(cols), indexing='ij')\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        plt.fill_between([j, j+1], [rows-i-1, rows-i-1], [rows-i, rows-i], color=colors[i, j])\n",
    "        plt.text(j+0.5, rows-i-0.5, str(cm_test[i, j]), ha='center', va='center', \n",
    "                 color='black', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.title('Confusion Matrix (Test Dataset)', fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize=14)\n",
    "plt.ylabel('Actual', fontsize=14)\n",
    "plt.xticks([0.5, 1.5], ['Student', 'ChatGPT'], fontsize=12)\n",
    "plt.yticks([0.5, 1.5], ['ChatGPT', 'Student'], fontsize=12)\n",
    "plt.xlim(0, 2)\n",
    "plt.ylim(0, 2)\n",
    "plt.savefig('ta_sentence/images/confusion_matrix (test set).png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report for test data\n",
    "print(\"\\nDetailed Classification Report (test set):\")\n",
    "print(classification_report(test_labels, test_predictions_binary, target_names=['Student', 'ChatGPT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb0f8a",
   "metadata": {},
   "source": [
    "# Misclassified Essay Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70450c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_students = []\n",
    "misclassified_chatgpt = []\n",
    "test_actual_texts = test_set['text'].values\n",
    "test_names = test_set['name'].values\n",
    "similarity_scores = test_inputs['similarity_score']\n",
    "\n",
    "for i, (actual, pred) in enumerate(zip(test_labels, test_predictions_binary)):\n",
    "    if actual == 0 and pred == 1:\n",
    "        student_idx = i\n",
    "        most_similar_chatgpt = None\n",
    "        highest_similarity = -1\n",
    "        avg_similarity_to_chatgpt = similarity_scores[i][1]\n",
    "        for j, gpt_text in enumerate(gpt_sen):\n",
    "            if j < len(gpt_te_emb):\n",
    "                student_embedding = tf.expand_dims(std_te_emb[student_idx], 0) if student_idx < len(std_te_emb) else None\n",
    "                chatgpt_embedding = tf.expand_dims(gpt_te_emb[j], 0)\n",
    "                \n",
    "                if student_embedding is not None:\n",
    "                    similarity = cos_sim(student_embedding, chatgpt_embedding)\n",
    "                    \n",
    "                    if similarity > highest_similarity:\n",
    "                        highest_similarity = similarity\n",
    "                        most_similar_chatgpt = gpt_sen[j]\n",
    "        \n",
    "        misclassified_students.append({\n",
    "            'Type': 'False Positive',\n",
    "            'Name': test_names[i],\n",
    "            'Text': test_actual_texts[i], \n",
    "            'Model Confidence': test_predictions[i][0],\n",
    "            'Avg. Similarity': avg_similarity_to_chatgpt,\n",
    "            'Most Similar Text': most_similar_chatgpt,\n",
    "            'Similarity Score': highest_similarity\n",
    "        })\n",
    "    elif actual == 1 and pred == 0:\n",
    "        chatgpt_idx = i\n",
    "        most_similar_student = None\n",
    "        highest_similarity = -1\n",
    "        avg_similarity_to_student = similarity_scores[i][0]\n",
    "        for j, std_text in enumerate(std_sen):\n",
    "            if j < len(std_te_emb):\n",
    "                chatgpt_embedding = tf.expand_dims(gpt_te_emb[chatgpt_idx-len(std_te_emb)], 0) if chatgpt_idx >= len(std_te_emb) else None\n",
    "                student_embedding = tf.expand_dims(std_te_emb[j], 0)\n",
    "                \n",
    "                if chatgpt_embedding is not None:\n",
    "                    similarity = cos_sim(chatgpt_embedding, student_embedding)\n",
    "                    \n",
    "                    if similarity > highest_similarity:\n",
    "                        highest_similarity = similarity\n",
    "                        most_similar_student = std_sen[j]\n",
    "        \n",
    "        misclassified_chatgpt.append({\n",
    "            'Type': 'False Negative', \n",
    "            'Name': test_names[i],\n",
    "            'Text': test_actual_texts[i], \n",
    "            'Model Confidence': 1 - test_predictions[i][0],\n",
    "            'Avg. Similarity': avg_similarity_to_student,\n",
    "            'Most Similar Text': most_similar_student,\n",
    "            'Similarity Score': highest_similarity\n",
    "        })\n",
    "\n",
    "sorted_misclassified_students = sorted(\n",
    "    misclassified_students, \n",
    "    key=lambda x: (x['Similarity Score'], x['Avg. Similarity'], x['Model Confidence']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "sorted_misclassified_chatgpt = sorted(\n",
    "    misclassified_chatgpt, \n",
    "    key=lambda x: (x['Similarity Score'], x['Avg. Similarity'], x['Model Confidence']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"Top 3 Student Essays Misclassified as ChatGPT (by Similarity):\")\n",
    "display(pd.DataFrame(sorted_misclassified_students).head(3))\n",
    "\n",
    "print(\"\\nTop 2 ChatGPT Essays Misclassified as Student (by Similarity):\")\n",
    "display(pd.DataFrame(sorted_misclassified_chatgpt).head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de9803",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_embeddings = {\n",
    "    'student': {\n",
    "        'embeddings': std_emb.numpy(),\n",
    "        'similarity_scores': std_sim_scores\n",
    "    },\n",
    "    'chatgpt': {\n",
    "        'embeddings': gpt_emb.numpy(),\n",
    "        'similarity_scores': gpt_sim_scores\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('ta_sentence'):\n",
    "    os.makedirs('ta_sentence')\n",
    "\n",
    "with open('ta_sentence/reference_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(reference_embeddings, f)\n",
    "\n",
    "semantic_model.save('ta_sentence/semantic_model.h5')\n",
    "classifier.save('ta_sentence/classification_model.h5')\n",
    "\n",
    "tokenizer.save_pretrained('ta_sentence/tokenizer')\n",
    "\n",
    "with open(\"ta_sentence/scaler_linguistic.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model and configuration successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark -iv --gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
